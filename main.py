import os
import cv2
import numpy as np
import mediapipe as mp
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

# ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
VIDEO_ROOT = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/0001~3000(á„‹á…§á†¼á„‰á…¡á†¼)"
MAX_SEQ_LENGTH = 100  # ìµœëŒ€ í”„ë ˆì„ ê¸¸ì´ (íŒ¨ë”©ì— ì‚¬ìš©)

label_dict = {
    # í™”ì¬
    "KETI_SL_0000000419.MOV": "í™”ì¬",
    "KETI_SL_0000000838.MTS": "í™”ì¬",
    "KETI_SL_0000001255.MTS": "í™”ì¬",
    "KETI_SL_0000001674.MTS": "í™”ì¬",
    "KETI_SL_0000002032.MOV": "í™”ì¬",
    "KETI_SL_0000002451.MP4": "í™”ì¬",
    "KETI_SL_0000002932.MOV": "í™”ì¬",
    # í™”ì¥ì‹¤
    "KETI_SL_0000000418.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000000837.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000001254.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000001673.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000002031.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000002450.MP4": "í™”ì¥ì‹¤",
    "KETI_SL_0000002931.MOV": "í™”ì¥ì‹¤"
}

def extract_landmarks(video_path):
    cap = cv2.VideoCapture(video_path)
    landmarks_list = []
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = holistic.process(rgb_frame)
        
        frame_data = {
            "pose": results.pose_landmarks,
            "left_hand": results.left_hand_landmarks,
            "right_hand": results.right_hand_landmarks,
            "face": results.face_landmarks
        }
        landmarks_list.append(frame_data)
    
    cap.release()
    return landmarks_list

def preprocess_landmarks(landmarks_list):
    processed_frames = []
    for frame in landmarks_list:
        combined = []
        for key in ["pose", "left_hand", "right_hand", "face"]:
            lm = frame[key]
            if lm:
                combined.extend([[l.x, l.y, l.z] for l in lm.landmark])
            else:
                num_points = {"pose": 33, "left_hand": 21, "right_hand": 21, "face": 468}[key]
                combined.extend([[0,0,0]] * num_points)
                
        arr = np.array(combined)
        root = arr[0].copy()
        arr -= root
        
        max_val = np.max(np.abs(arr))
        if max_val > 0:
            arr /= max_val
            
        processed_frames.append(arr.flatten())
        
    return np.array(processed_frames)

X = []
y = []

for filename, label in tqdm(label_dict.items()):
    file_id = filename.split(".")[0]
    actual_path = os.path.join(VIDEO_ROOT, f"{file_id}.avi")
    
    if not os.path.exists(actual_path):
        print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {actual_path}")
        continue
    
    landmarks = extract_landmarks(actual_path)
    if not landmarks:
        continue
        
    processed_sequence = preprocess_landmarks(landmarks)
    
    X.append(processed_sequence)
    y.append(1 if label == "í™”ì¬" else 0)

print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(X)}ê°œ ìƒ˜í”Œ")

if len(X) < 2:
    print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    # ë°ì´í„° íŒ¨ë”©
    X_padded = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post', dtype='float32')
    y_np = np.array(y)

    # ë°ì´í„°ì…‹ ì°¨ì› í™•ì¸
    print(f"Padded X shape: {X_padded.shape}")
    print(f"y shape: {y_np.shape}")
    
    if X_padded.shape[0] == 0:
        print("âš ï¸ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ì–´ ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X_padded, y_np, test_size=0.2, random_state=42, stratify=y_np
        )

        # LSTM ëª¨ë¸ ì •ì˜
        model = Sequential([
            LSTM(64, return_sequences=True, input_shape=(MAX_SEQ_LENGTH, X_padded.shape[2])),
            Dropout(0.5),
            LSTM(32),
            Dropout(0.5),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ])

        model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy'])

        print("\n--- ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
        model.summary()

        history = model.fit(X_train, y_train, epochs=30, batch_size=4, validation_split=0.2)
        
        print("\n--- ëª¨ë¸ í‰ê°€ ---")
        loss, accuracy = model.evaluate(X_test, y_test)
        print(f"ğŸš€ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")

        # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
        print("\n--- í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ---")
        y_pred_prob = model.predict(X_test)
        for i, (pred_prob, actual) in enumerate(zip(y_pred_prob, y_test)):
            pred_label = "í™”ì¬" if pred_prob[0] > 0.5 else "í™”ì¥ì‹¤"
            actual_label = "í™”ì¬" if actual == 1 else "í™”ì¥ì‹¤"
            result = "âœ…" if (pred_prob[0] > 0.5) == actual else "âŒ"
            print(f"ìƒ˜í”Œ {i+1}: ì˜ˆì¸¡={pred_label} (ì‹ ë¢°ë„: {pred_prob[0]:.2f}), ì‹¤ì œ={actual_label} {result}")