import os
import cv2
import numpy as np
import mediapipe as mp
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    LSTM,
    Dense,
    Dropout,
    Input,
    Bidirectional,
    Conv1D,
    MaxPooling1D,
    GlobalAveragePooling1D,
    LayerNormalization,
    MultiHeadAttention,
    Add,
    BatchNormalization,
    Lambda,
)
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from scipy.interpolate import interp1d
import sys
import json
import pandas as pd
import pickle
from datetime import datetime
import logging
from collections import defaultdict
from config import LABEL_MAX_SAMPLES_PER_CLASS, MIN_SAMPLES_PER_CLASS

# .env íŒŒì¼ ë¡œë“œ (s3_utilsë³´ë‹¤ ë¨¼ì € ë¡œë“œ)
try:
    from dotenv import load_dotenv
    # .env íŒŒì¼ì´ ìˆëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    env_path = os.path.join(os.path.dirname(__file__), '.env')
    if os.path.exists(env_path):
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œ: {env_path}")
    else:
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ .env íŒŒì¼ ì°¾ê¸°
        if os.path.exists('.env'):
            load_dotenv('.env')
            print("âœ… .env íŒŒì¼ ë¡œë“œ: ./.env")
        else:
            print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
except ImportError:
    print("âš ï¸ python-dotenvë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install python-dotenv")

# S3 í˜¸í™˜ ìºì‹œ ì‹œìŠ¤í…œ import
from s3_utils import (
    cache_join,
    cache_exists,
    cache_makedirs,
    cache_save_pickle,
    cache_load_pickle,
    cache_remove,
    is_s3_path
)

# MediaPipe ë° TensorFlow ë¡œê¹… ì™„ì „ ì–µì œ
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # ERRORë§Œ ì¶œë ¥
os.environ["CUDA_VISIBLE_DEVICES"] = ""  # GPU ë¹„í™œì„±í™” (CPUë§Œ ì‚¬ìš©)
logging.getLogger("mediapipe").setLevel(logging.CRITICAL)
logging.getLogger("tensorflow").setLevel(logging.ERROR)
logging.getLogger("absl").setLevel(logging.ERROR)

# ì„¤ì • íŒŒì¼ì—ì„œ íŒŒë¼ë¯¸í„° import
from config import *

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic


class MediaPipeManager:
    """MediaPipe ê°ì²´ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""

    _instance = None
    _holistic = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MediaPipeManager, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if self._holistic is None:
            self._holistic = mp_holistic.Holistic(
                static_image_mode=MEDIAPIPE_STATIC_IMAGE_MODE,
                model_complexity=MEDIAPIPE_MODEL_COMPLEXITY,
                smooth_landmarks=MEDIAPIPE_SMOOTH_LANDMARKS,
                enable_segmentation=MEDIAPIPE_ENABLE_SEGMENTATION,
                smooth_segmentation=MEDIAPIPE_SMOOTH_SEGMENTATION,
                min_detection_confidence=MEDIAPIPE_MIN_DETECTION_CONFIDENCE,
                min_tracking_confidence=MEDIAPIPE_MIN_TRACKING_CONFIDENCE,
            )

    def __enter__(self):
        return self._holistic

    def __exit__(self, exc_type, exc_val, exc_tb):
        # ì „ì—­ ê°ì²´ëŠ” ìœ ì§€í•˜ê³  ì •ë¦¬ë§Œ
        pass

    @classmethod
    def cleanup(cls):
        """ì „ì—­ MediaPipe ê°ì²´ ì •ë¦¬"""
        if cls._holistic:
            cls._holistic.close()
            cls._holistic = None


# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(INFO_DIR, exist_ok=True)

# ê³ ìœ í•œ ëª¨ë¸ ì´ë¦„ ìƒì„±
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
MODEL_SAVE_PATH = f"{MODELS_DIR}/sign_language_model_{timestamp}.keras"
MODEL_INFO_PATH = f"{INFO_DIR}/model-info-{timestamp}.json"

# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
cache_makedirs(CACHE_DIR, exist_ok=True)

DATA_CACHE_PATH = "fixed_preprocessed_data.npz"


# ë¼ë²¨ë³„ ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„± í•¨ìˆ˜
def get_label_cache_path(label):
    """ë¼ë²¨ë³„ ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì£¼ìš” íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì¼ëª…ì— í¬í•¨ì‹œì¼œ ìºì‹œ ë¬´íš¨í™”ê°€ ìë™ìœ¼ë¡œ ë˜ë„ë¡ í•©ë‹ˆë‹¤."""
    safe_label = label.replace(" ", "_").replace("/", "_")

    # ë°ì´í„° ê°œìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ì„ íŒŒì¼ëª…ì— í¬í•¨
    max_samples_str = (
        f"max{LABEL_MAX_SAMPLES_PER_CLASS}"
        if LABEL_MAX_SAMPLES_PER_CLASS
        else "maxNone"
    )
    min_samples_str = f"min{MIN_SAMPLES_PER_CLASS}"

    return cache_join(
        CACHE_DIR,
        f"{safe_label}_seq{TARGET_SEQ_LENGTH}_aug{AUGMENTATIONS_PER_VIDEO}_{max_samples_str}_{min_samples_str}.pkl",
    )


def save_label_cache(label, data):
    """ë¼ë²¨ë³„ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    cache_path = get_label_cache_path(label)

    # ìºì‹œì— ì €ì¥í•  ë°ì´í„°ì™€ íŒŒë¼ë¯¸í„° ì •ë³´
    cache_data = {
        "data": data,
        "parameters": {
            "TARGET_SEQ_LENGTH": TARGET_SEQ_LENGTH,
            "AUGMENTATIONS_PER_VIDEO": AUGMENTATIONS_PER_VIDEO,
            "AUGMENTATION_NOISE_LEVEL": AUGMENTATION_NOISE_LEVEL,
            "AUGMENTATION_SCALE_RANGE": AUGMENTATION_SCALE_RANGE,
            "AUGMENTATION_ROTATION_RANGE": AUGMENTATION_ROTATION_RANGE,
            "NONE_CLASS_NOISE_LEVEL": NONE_CLASS_NOISE_LEVEL,
            "NONE_CLASS_AUGMENTATIONS_PER_FRAME": NONE_CLASS_AUGMENTATIONS_PER_FRAME,
            # ë°ì´í„° ê°œìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
            "LABEL_MAX_SAMPLES_PER_CLASS": LABEL_MAX_SAMPLES_PER_CLASS,
            "MIN_SAMPLES_PER_CLASS": MIN_SAMPLES_PER_CLASS,
        },
    }

    # S3 í˜¸í™˜ ìºì‹œ ì €ì¥
    try:
        if is_s3_path(cache_path):
            # S3ì—ì„œëŠ” put_objectê°€ ì›ìì ì´ë¯€ë¡œ ì§ì ‘ ì €ì¥
            success = cache_save_pickle(cache_path, cache_data)
            if success:
                print(f"ğŸ’¾ {label} ë¼ë²¨ ë°ì´í„° ìºì‹œ ì €ì¥ (S3): {cache_path} ({len(data)}ê°œ ìƒ˜í”Œ)")
            else:
                raise Exception("S3 ìºì‹œ ì €ì¥ ì‹¤íŒ¨")
        else:
            # ë¡œì»¬ì—ì„œëŠ” ì„ì‹œ íŒŒì¼ ë°©ì‹ ì‚¬ìš© (ì›ìì  ì“°ê¸°)
            temp_path = cache_path + ".tmp"
            
            with open(temp_path, "wb") as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)

            # ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ë©´ ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™
            os.replace(temp_path, cache_path)
            print(f"ğŸ’¾ {label} ë¼ë²¨ ë°ì´í„° ìºì‹œ ì €ì¥ (ë¡œì»¬): {cache_path} ({len(data)}ê°œ ìƒ˜í”Œ)")

    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš°ë§Œ)
        if not is_s3_path(cache_path):
            temp_path = cache_path + ".tmp"
            if os.path.exists(temp_path):
                os.remove(temp_path)
        raise e


def load_label_cache(label):
    """ë¼ë²¨ë³„ ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤."""
    cache_path = get_label_cache_path(label)
    if cache_exists(cache_path):
        try:
            # S3 í˜¸í™˜ ìºì‹œ ë¡œë“œ
            cache_data = cache_load_pickle(cache_path)
            if cache_data is None:
                return None

            # ìºì‹œ í˜•ì‹ í™•ì¸ (êµ¬ë²„ì „ í˜¸í™˜ì„±)
            if (
                isinstance(cache_data, dict)
                and "data" in cache_data
                and "parameters" in cache_data
            ):
                # ìƒˆ í˜•ì‹: íŒŒë¼ë¯¸í„° ê²€ì¦
                cached_params = cache_data["parameters"]
                current_params = {
                    "TARGET_SEQ_LENGTH": TARGET_SEQ_LENGTH,
                    "AUGMENTATIONS_PER_VIDEO": AUGMENTATIONS_PER_VIDEO,
                    "AUGMENTATION_NOISE_LEVEL": AUGMENTATION_NOISE_LEVEL,
                    "AUGMENTATION_SCALE_RANGE": AUGMENTATION_SCALE_RANGE,
                    "AUGMENTATION_ROTATION_RANGE": AUGMENTATION_ROTATION_RANGE,
                    "NONE_CLASS_NOISE_LEVEL": NONE_CLASS_NOISE_LEVEL,
                    "NONE_CLASS_AUGMENTATIONS_PER_FRAME": NONE_CLASS_AUGMENTATIONS_PER_FRAME,
                    # ë°ì´í„° ê°œìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
                    "LABEL_MAX_SAMPLES_PER_CLASS": LABEL_MAX_SAMPLES_PER_CLASS,
                    "MIN_SAMPLES_PER_CLASS": MIN_SAMPLES_PER_CLASS,
                }

                # íŒŒë¼ë¯¸í„° ë¹„êµ
                if cached_params != current_params:
                    print(f"âš ï¸ {label} ìºì‹œ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ìºì‹œ ë¬´íš¨í™”.")
                    print(f"   ìºì‹œëœ íŒŒë¼ë¯¸í„°: {cached_params}")
                    print(f"   í˜„ì¬ íŒŒë¼ë¯¸í„°: {current_params}")
                    cache_remove(cache_path)
                    return None

                data = cache_data["data"]
            else:
                # êµ¬ë²„ì „: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ (íŒŒë¼ë¯¸í„° ê²€ì¦ ì—†ì´ ì‚¬ìš©)
                print(f"âš ï¸ {label} êµ¬ë²„ì „ ìºì‹œ í˜•ì‹ì…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                data = cache_data

            # ë°ì´í„° ê²€ì¦
            if isinstance(data, list) and len(data) > 0:
                cache_type = "S3" if is_s3_path(cache_path) else "ë¡œì»¬"
                print(
                    f"ğŸ“‚ {label} ë¼ë²¨ ë°ì´í„° ìºì‹œ ë¡œë“œ ({cache_type}): {cache_path} ({len(data)}ê°œ ìƒ˜í”Œ)"
                )
                return data
            else:
                print(f"âš ï¸ {label} ìºì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤.")
                return None

        except Exception as e:
            print(f"âš ï¸ {label} ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì†ìƒëœ ìºì‹œ íŒŒì¼ ì‚­ì œ
            try:
                cache_remove(cache_path)
                print(f"ğŸ—‘ï¸ ì†ìƒëœ ìºì‹œ íŒŒì¼ ì‚­ì œ: {cache_path}")
            except:
                pass
            return None
    return None


def process_data_in_batches(file_mapping, batch_size=100):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    all_files = list(file_mapping.items())
    total_files = len(all_files)

    print(f"ğŸ“Š ì´ {total_files}ê°œ íŒŒì¼ì„ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # ì§„í–‰ë¥  í‘œì‹œ ì„¤ì •ì— ë”°ë¼ tqdm ì‚¬ìš©
    if ENABLE_PROGRESS_BAR:
        iterator = tqdm(range(0, total_files, batch_size), desc="ë°°ì¹˜ ì²˜ë¦¬")
    else:
        iterator = range(0, total_files, batch_size)

    # MediaPipe ê°ì²´ ì¬ì‚¬ìš©
    try:
        with MediaPipeManager() as holistic:
            print("âœ… MediaPipe ê°ì²´ ì´ˆê¸°í™” ì™„ë£Œ")

            for i in iterator:
                batch_files = all_files[i : i + batch_size]
                batch_data = []

                print(
                    f"ğŸ”„ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch_files)}ê°œ íŒŒì¼)"
                )

                for filename, info in batch_files:
                    try:
                        print(f"  ğŸ“¹ {filename} ì²˜ë¦¬ ì¤‘...")
                        landmarks = extract_landmarks_with_holistic(
                            info["path"], holistic
                        )
                        if not landmarks:
                            print(f"    âš ï¸ ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {filename}")
                            continue

                        processed_sequence = improved_preprocess_landmarks(landmarks)
                        if processed_sequence.shape != (TARGET_SEQ_LENGTH, 675):
                            print(
                                f"    âš ï¸ ì‹œí€€ìŠ¤ í˜•íƒœ ë¶ˆì¼ì¹˜: {filename} - {processed_sequence.shape}"
                            )
                            continue

                        batch_data.append(
                            {
                                "sequence": processed_sequence,
                                "label": info["label"],
                                "filename": filename,
                            }
                        )
                        print(f"    âœ… ì„±ê³µ: {filename}")

                    except Exception as e:
                        print(f"    âŒ ì˜¤ë¥˜: {filename} - {e}")
                        continue

                print(f"âœ… ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ: {len(batch_data)}ê°œ ì„±ê³µ")
                yield batch_data

    except Exception as e:
        print(f"âŒ MediaPipe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        yield []


def extract_and_cache_label_data_optimized(file_mapping, label):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¼ë²¨ë³„ ë°ì´í„° ì¶”ì¶œ ë° ìºì‹±"""
    print(f"\nğŸ”„ {label} ë¼ë²¨ ë°ì´í„° ì¶”ì¶œ ì¤‘...")

    # ìºì‹œ í™•ì¸
    cached_data = load_label_cache(label)
    if cached_data:
        print(f"âœ… {label} ë¼ë²¨ ìºì‹œ ë°ì´í„° ì‚¬ìš©: {len(cached_data)}ê°œ ìƒ˜í”Œ")
        return cached_data

    # í•´ë‹¹ ë¼ë²¨ì˜ íŒŒì¼ë“¤ë§Œ í•„í„°ë§
    label_files = {
        filename: info
        for filename, info in file_mapping.items()
        if info["label"] == label
    }

    if not label_files:
        print(f"âš ï¸ {label} ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    label_data = []

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for batch in process_data_in_batches(
        label_files, batch_size=BATCH_SIZE_FOR_PROCESSING
    ):
        for item in batch:
            if item["label"] == label:
                # ì›ë³¸ ë°ì´í„° ì¶”ê°€
                label_data.append(item["sequence"])

                # ì¦ê°• ë°ì´í„° ì¶”ê°€
                for _ in range(AUGMENTATIONS_PER_VIDEO):
                    try:
                        augmented = augment_sequence_improved(item["sequence"])
                        if augmented.shape == (TARGET_SEQ_LENGTH, 675):
                            label_data.append(augmented)
                    except Exception as e:
                        print(f"âš ï¸ ì¦ê°• ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

    print(f"âœ… {label} ë¼ë²¨ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(label_data)}ê°œ ìƒ˜í”Œ")

    # ìºì‹œì— ì €ì¥
    save_label_cache(label, label_data)

    return label_data


def generate_balanced_none_class_data(file_mapping, none_class, target_count=None):
    """ë‹¤ë¥¸ í´ë˜ìŠ¤ì™€ ê· í˜•ìˆëŠ” None í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    print(f"\nâœ¨ '{none_class}' í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")

    # ê¸°ì¡´ ìºì‹œ í™•ì¸ (target_count ì •ë³´ í¬í•¨)
    if target_count is not None:
        cached_none_data = load_none_class_cache(none_class, target_count)
    else:
        cached_none_data = load_label_cache(none_class)  # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±

    if cached_none_data:
        print(
            f"âœ… {none_class} í´ë˜ìŠ¤ ìºì‹œ ë°ì´í„° ì‚¬ìš©: {len(cached_none_data)}ê°œ ìƒ˜í”Œ"
        )
        return cached_none_data

    # ëª©í‘œ ê°œìˆ˜ ê³„ì‚° (ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ í‰ê·  ê°œìˆ˜)
    if target_count is None:
        # ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ì˜ ì›ë³¸ íŒŒì¼ ê°œìˆ˜ ê³„ì‚°
        other_class_counts = []
        for filename, info in file_mapping.items():
            if info["label"] != none_class:
                other_class_counts.append(info["label"])

        # ë¼ë²¨ë³„ ê°œìˆ˜ ì§‘ê³„
        from collections import Counter

        label_counts = Counter(other_class_counts)

        if label_counts:
            # ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ì˜ í‰ê·  ê°œìˆ˜ ê³„ì‚° (ì¦ê°• í›„ ì˜ˆìƒ ê°œìˆ˜)
            avg_original_count = sum(label_counts.values()) / len(label_counts)
            target_count = int(avg_original_count * (1 + AUGMENTATIONS_PER_VIDEO))
            print(
                f"ğŸ“Š ë‹¤ë¥¸ í´ë˜ìŠ¤ í‰ê· : {avg_original_count:.1f}ê°œ â†’ ëª©í‘œ None í´ë˜ìŠ¤: {target_count}ê°œ"
            )
        else:
            target_count = 100  # ê¸°ë³¸ê°’
            print(f"ğŸ“Š ê¸°ë³¸ ëª©í‘œ None í´ë˜ìŠ¤: {target_count}ê°œ")

    none_samples = []
    source_videos = list(file_mapping.keys())

    # ëª©í‘œ ê°œìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
    video_index = 0
    while len(none_samples) < target_count and video_index < len(source_videos):
        filename = source_videos[video_index % len(source_videos)]  # ìˆœí™˜ ì‚¬ìš©
        file_path = file_mapping[filename]["path"]

        try:
            # MediaPipe ê°ì²´ ì¬ì‚¬ìš© (í•œ ë²ˆì— í•˜ë‚˜ì”© ì²˜ë¦¬)
            with MediaPipeManager() as holistic:
                landmarks = extract_landmarks_with_holistic(file_path, holistic)

                if landmarks and len(landmarks) > 10:
                    # ì˜ìƒì˜ ì‹œì‘, 1/4, 1/2, 3/4, ë ì§€ì ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
                    frame_indices = [
                        0,
                        len(landmarks) // 4,
                        len(landmarks) // 2,
                        3 * len(landmarks) // 4,
                        -1,
                    ]

                    for idx in frame_indices:
                        if len(none_samples) >= target_count:
                            break

                        static_landmarks = [landmarks[idx]] * TARGET_SEQ_LENGTH
                        static_sequence = improved_preprocess_landmarks(
                            static_landmarks
                        )

                        if static_sequence.shape != (TARGET_SEQ_LENGTH, 675):
                            continue

                        # ì •ì  ì‹œí€€ìŠ¤ ì¶”ê°€
                        none_samples.append(static_sequence)

                        # ë¯¸ì„¸í•œ ì›€ì§ì„ ì¶”ê°€ (ë…¸ì´ì¦ˆ) - ëª©í‘œ ê°œìˆ˜ ì œí•œ
                        for _ in range(
                            min(
                                NONE_CLASS_AUGMENTATIONS_PER_FRAME,
                                target_count - len(none_samples),
                            )
                        ):
                            if len(none_samples) >= target_count:
                                break
                            augmented = augment_sequence_improved(
                                static_sequence, noise_level=NONE_CLASS_NOISE_LEVEL
                            )
                            if augmented.shape == (TARGET_SEQ_LENGTH, 675):
                                none_samples.append(augmented)

                    # ëŠë¦° ì „í™˜ ë°ì´í„° ìƒì„± (ëª©í‘œ ê°œìˆ˜ ì œí•œ)
                    if len(none_samples) < target_count:
                        start_frame_lm = landmarks[0]
                        middle_frame_lm = landmarks[len(landmarks) // 2]

                        transition_landmarks = []
                        for i in range(TARGET_SEQ_LENGTH):
                            alpha = i / (TARGET_SEQ_LENGTH - 1)
                            interp_frame = {}
                            for key in ["pose", "left_hand", "right_hand"]:
                                if start_frame_lm.get(key) and middle_frame_lm.get(key):
                                    interp_lm = []
                                    start_lms = start_frame_lm[key].landmark
                                    mid_lms = middle_frame_lm[key].landmark
                                    for j in range(len(start_lms)):
                                        new_x = (
                                            start_lms[j].x * (1 - alpha)
                                            + mid_lms[j].x * alpha
                                        )
                                        new_y = (
                                            start_lms[j].y * (1 - alpha)
                                            + mid_lms[j].y * alpha
                                        )
                                        new_z = (
                                            start_lms[j].z * (1 - alpha)
                                            + mid_lms[j].z * alpha
                                        )
                                        interp_lm.append(
                                            type(
                                                "obj",
                                                (object,),
                                                {"x": new_x, "y": new_y, "z": new_z},
                                            )
                                        )
                                    interp_frame[key] = type(
                                        "obj", (object,), {"landmark": interp_lm}
                                    )
                                else:
                                    interp_frame[key] = None
                            transition_landmarks.append(interp_frame)

                        transition_sequence = improved_preprocess_landmarks(
                            transition_landmarks
                        )
                        if transition_sequence.shape == (TARGET_SEQ_LENGTH, 675):
                            none_samples.append(transition_sequence)

        except Exception as e:
            print(f"âš ï¸ None í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {filename}, ì˜¤ë¥˜: {e}")

        video_index += 1

    print(
        f"âœ… {none_class} í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(none_samples)}ê°œ ìƒ˜í”Œ (ëª©í‘œ: {target_count}ê°œ)"
    )

    # ìºì‹œì— ì €ì¥
    save_none_class_cache(none_class, none_samples, target_count)

    return none_samples


def validate_video_roots():
    """VIDEO_ROOTSì˜ ëª¨ë“  ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    print("ğŸ” ë¹„ë””ì˜¤ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²€ì¦ ì¤‘...")
    valid_roots = []

    for (range_start, range_end), root_path in VIDEO_ROOTS:
        if os.path.exists(root_path):
            valid_roots.append(((range_start, range_end), root_path))
            print(f"âœ… {range_start}~{range_end}: {root_path}")
        else:
            print(f"âŒ {range_start}~{range_end}: {root_path} (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)")

    return valid_roots


def find_file_in_directory(directory, filename_pattern):
    """ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    if not os.path.exists(directory):
        return None

    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
    base_name = filename_pattern.split(".")[0]

    # ê°€ëŠ¥í•œ í™•ì¥ìë“¤ (configì—ì„œ ê°€ì ¸ì˜´)
    for ext in VIDEO_EXTENSIONS:
        candidate = os.path.join(directory, base_name + ext)
        if os.path.exists(candidate):
            return candidate

    return None


def get_video_root_and_path(filename):
    """íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•´ ì˜¬ë°”ë¥¸ VIDEO_ROOT ê²½ë¡œì™€ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ í™•ì¥ì ì œê±°
        file_id = filename.split(".")[0]

        # KETI_SL_ í˜•ì‹ í™•ì¸
        if not file_id.startswith("KETI_SL_"):
            print(f"âš ï¸ KETI_SL_ í˜•ì‹ì´ ì•„ë‹Œ íŒŒì¼ëª…: {filename}")
            return None

        # ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ
        number_str = file_id.replace("KETI_SL_", "")
        if not number_str.isdigit():
            print(f"âš ï¸ ìˆ«ìê°€ ì•„ë‹Œ íŒŒì¼ëª…: {filename}")
            return None

        num = int(number_str)

        # ì ì ˆí•œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        target_root = None
        for (range_start, range_end), root_path in VIDEO_ROOTS:
            if range_start <= num <= range_end:
                target_root = root_path
                break

        if target_root is None:
            print(f"âš ï¸ ë²ˆí˜¸ {num}ì— í•´ë‹¹í•˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename}")
            return None

        # íŒŒì¼ ì°¾ê¸°
        file_path = find_file_in_directory(target_root, filename)
        if file_path:
            return file_path

        print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename} (ë””ë ‰í† ë¦¬: {target_root})")
        return None

    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ëª… íŒŒì‹± ì˜¤ë¥˜: {filename}, ì˜¤ë¥˜: {e}")
        return None


def normalize_sequence_length(sequence, target_length=30):
    """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    current_length = len(sequence)

    if current_length == target_length:
        return sequence

    x_old = np.linspace(0, 1, current_length)
    x_new = np.linspace(0, 1, target_length)

    normalized_sequence = []
    for i in range(sequence.shape[1]):
        f = interp1d(
            x_old,
            sequence[:, i],
            kind="linear",
            bounds_error=False,
            fill_value="extrapolate",
        )
        normalized_sequence.append(f(x_new))

    return np.array(normalized_sequence).T


def extract_dynamic_features(sequence):
    """ì†ë„ì™€ ê°€ì†ë„ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    velocity = np.diff(sequence, axis=0, prepend=sequence[0:1])
    acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])
    dynamic_features = np.concatenate([sequence, velocity, acceleration], axis=1)
    return dynamic_features


def convert_to_relative_coordinates(landmarks_list):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ì–´ê¹¨ ì¤‘ì‹¬ ìƒëŒ€ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    relative_landmarks = []

    for frame in landmarks_list:
        if not frame["pose"]:
            relative_landmarks.append(frame)
            continue

        pose_landmarks = frame["pose"].landmark

        left_shoulder = pose_landmarks[11]
        right_shoulder = pose_landmarks[12]
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2

        shoulder_width = abs(right_shoulder.x - left_shoulder.x)
        if shoulder_width == 0:
            shoulder_width = 1.0

        new_frame = {}

        if frame["pose"]:
            relative_pose = []
            for landmark in pose_landmarks:
                rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                relative_pose.append([rel_x, rel_y, rel_z])
            new_frame["pose"] = relative_pose

        for hand_key in ["left_hand", "right_hand"]:
            if frame[hand_key]:
                relative_hand = []
                for landmark in frame[hand_key].landmark:
                    rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                    rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                    rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                    relative_hand.append([rel_x, rel_y, rel_z])
                new_frame[hand_key] = relative_hand
            else:
                new_frame[hand_key] = None

        relative_landmarks.append(new_frame)

    return relative_landmarks


def improved_preprocess_landmarks(landmarks_list):
    """ê°œì„ ëœ ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ í•¨ìˆ˜."""
    if not landmarks_list:
        return np.zeros((TARGET_SEQ_LENGTH, 675))

    relative_landmarks = convert_to_relative_coordinates(landmarks_list)

    processed_frames = []
    for frame in relative_landmarks:
        combined = []
        for key in ["pose", "left_hand", "right_hand"]:
            if frame[key]:
                if isinstance(frame[key], list):
                    combined.extend(frame[key])
                else:
                    combined.extend([[l.x, l.y, l.z] for l in frame[key].landmark])
            else:
                num_points = {"pose": 33, "left_hand": 21, "right_hand": 21}[key]
                combined.extend([[0, 0, 0]] * num_points)

        if combined:
            processed_frames.append(np.array(combined).flatten())
        else:
            processed_frames.append(np.zeros(75 * 3))

    if not processed_frames:
        return np.zeros((TARGET_SEQ_LENGTH, 675))

    sequence = np.array(processed_frames)

    if len(sequence) > 0:
        try:
            sequence = normalize_sequence_length(sequence, TARGET_SEQ_LENGTH)
            sequence = extract_dynamic_features(sequence)

            # ì •ê·œí™” ê°œì„ : ë” ê°•í•œ ì •ê·œí™”
            sequence = (sequence - np.mean(sequence)) / (np.std(sequence) + 1e-8)

            return sequence
        except Exception as e:
            print(f"âš ï¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return np.zeros((TARGET_SEQ_LENGTH, 675))

    return np.zeros((TARGET_SEQ_LENGTH, 675))


def create_simple_model(input_shape, num_classes):
    """ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    inputs = Input(shape=input_shape)

    # 1D CNN
    x = Conv1D(64, kernel_size=3, activation="relu", padding="same")(inputs)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    x = Conv1D(128, kernel_size=3, activation="relu", padding="same")(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    # LSTM
    x = Bidirectional(LSTM(64, return_sequences=True))(x)
    x = Dropout(0.3)(x)
    x = Bidirectional(LSTM(32))(x)
    x = Dropout(0.3)(x)

    # Dense layers
    x = Dense(64, activation="relu")(x)
    x = Dropout(0.3)(x)
    x = Dense(32, activation="relu")(x)
    x = Dropout(0.3)(x)

    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


def load_model_info():
    """ê¸°ì¡´ ëª¨ë¸ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        if os.path.exists(MODEL_INFO_PATH):
            with open(MODEL_INFO_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ì •ë³´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    return None


def save_model_info(actions, model_path, info_path, training_stats):
    """ëª¨ë¸ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    model_info = {
        "model_path": model_path,
        "created_at": datetime.now().isoformat(),
        "labels": actions,
        "label_mapping": {label: idx for idx, label in enumerate(actions)},
        "num_classes": len(actions),
        "input_shape": [TARGET_SEQ_LENGTH, 675],  # ì‹œí€€ìŠ¤ ê¸¸ì´, íŠ¹ì§• ìˆ˜
        "training_stats": training_stats,
        "model_type": "LSTM",
        "description": "ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ - LSTM ê¸°ë°˜",
    }

    with open(info_path, "w", encoding="utf-8") as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ ëª¨ë¸ ì •ë³´ ì €ì¥: {info_path}")


def augment_sequence_improved(
    sequence,
    noise_level=AUGMENTATION_NOISE_LEVEL,
    scale_range=AUGMENTATION_SCALE_RANGE,
    rotation_range=AUGMENTATION_ROTATION_RANGE,
):
    """ê°œì„ ëœ ì‹œí€€ìŠ¤ ì¦ê°•."""
    augmented = sequence.copy()

    # ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.normal(0, noise_level, augmented.shape)
    augmented += noise

    # ìŠ¤ì¼€ì¼ë§
    scale_factor = np.random.uniform(1 - scale_range, 1 + scale_range)
    augmented *= scale_factor

    # ì‹œê°„ì¶•ì—ì„œì˜ íšŒì „ (ì‹œí”„íŠ¸)
    shift = np.random.randint(-3, 4)
    if shift > 0:
        augmented = np.roll(augmented, shift, axis=0)
    elif shift < 0:
        augmented = np.roll(augmented, shift, axis=0)

    return augmented


def extract_landmarks_with_holistic(video_path, holistic):
    """ì „ë‹¬ë°›ì€ MediaPipe ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŒ: {video_path}")
            return None

        # ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        print(f"    ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {total_frames}í”„ë ˆì„, {fps:.1f}fps")

        landmarks_list = []
        frame_count = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # í”„ë ˆì„ ì²˜ë¦¬
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = holistic.process(rgb_frame)

            frame_data = {
                "pose": results.pose_landmarks,
                "left_hand": results.left_hand_landmarks,
                "right_hand": results.right_hand_landmarks,
            }
            landmarks_list.append(frame_data)
            frame_count += 1

            # ì§„í–‰ìƒí™© í‘œì‹œ (10í”„ë ˆì„ë§ˆë‹¤)
            # if frame_count % 10 == 0:
            # print(f"      ğŸ“¹ í”„ë ˆì„ {frame_count}/{total_frames} ì²˜ë¦¬ ì¤‘...")

        cap.release()
        print(f"    âœ… ëœë“œë§ˆí¬ ì¶”ì¶œ ì™„ë£Œ: {len(landmarks_list)}í”„ë ˆì„")
        return landmarks_list

    except (cv2.error, OSError) as e:
        print(f"âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {video_path}, ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"âš ï¸ ëœë“œë§ˆí¬ ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {video_path}, ì˜¤ë¥˜: {e}")
        return None


def get_action_index(label, actions):
    """ë¼ë²¨ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return actions.index(label)


def get_all_video_paths():
    video_paths = []

    return video_paths


def cleanup_old_checkpoints(
    checkpoint_dir="checkpoints", keep_best=True, max_checkpoints=10
):
    """ê°œì„ ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ í•¨ìˆ˜ - ì™„ì „í•˜ê³  ì•ˆì „í•œ ì •ë¦¬"""
    if not os.path.exists(checkpoint_dir):
        return

    print(f"ğŸ§¹ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘: {checkpoint_dir}")

    try:
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        import shutil

        total, used, free = shutil.disk_usage(checkpoint_dir)
        free_gb = free / (1024**3)
        print(f"   ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„: {free_gb:.2f}GB")

        if free_gb < 0.5:  # 500MB ë¯¸ë§Œ
            print("   âš ï¸ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ì ê·¹ì ì¸ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            max_checkpoints = 5

        # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸° (ì—í­ ê¸°ë°˜ íŒŒì¼ëª…)
        checkpoint_files = []
        for file in os.listdir(checkpoint_dir):
            if file.startswith("model-epoch-") and file.endswith(".keras"):
                checkpoint_files.append(file)

        if not checkpoint_files:
            print("   ğŸ“ ì •ë¦¬í•  ì—í­ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"   ğŸ“Š ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸: {len(checkpoint_files)}ê°œ")

        # ì—í­ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ ìµœê·¼ì´ ë§ˆì§€ë§‰)
        def extract_epoch(filename):
            try:
                epoch_part = filename.split("-")[2].split(".")[0]  # "05"
                return int(epoch_part)
            except:
                return 0

        checkpoint_files.sort(key=extract_epoch)

        # ë³´ì¡´í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜ ê²°ì •
        files_to_keep = (
            checkpoint_files[-max_checkpoints:]
            if len(checkpoint_files) > max_checkpoints
            else []
        )
        files_to_delete = [f for f in checkpoint_files if f not in files_to_keep]

        print(f"   ğŸ¯ ë³´ì¡´í•  ì²´í¬í¬ì¸íŠ¸: {len(files_to_keep)}ê°œ")
        print(f"   ğŸ—‘ï¸ ì‚­ì œí•  ì²´í¬í¬ì¸íŠ¸: {len(files_to_delete)}ê°œ")

        # íŒŒì¼ ì‚­ì œ (ì²´í¬í¬ì¸íŠ¸ì™€ info íŒŒì¼ ëª¨ë‘)
        deleted_count = 0
        for file in files_to_delete:
            try:
                # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ
                checkpoint_path = os.path.join(checkpoint_dir, file)
                if os.path.exists(checkpoint_path):
                    os.remove(checkpoint_path)
                    deleted_count += 1

                # í•´ë‹¹í•˜ëŠ” info íŒŒì¼ë„ ì‚­ì œ
                info_path = checkpoint_path.replace(".keras", "_info.json")
                if os.path.exists(info_path):
                    os.remove(info_path)
                    deleted_count += 1

            except Exception as e:
                print(f"   âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file} - {e}")

        print(f"   âœ… {deleted_count}ê°œ íŒŒì¼ ì‚­ì œë¨")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì€ ìœ ì§€
        if keep_best and os.path.exists(
            os.path.join(checkpoint_dir, "best_model.keras")
        ):
            print("   ğŸ’ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìœ ì§€ë¨")

            # best_model_info.jsonë„ í™•ì¸
            best_info_path = os.path.join(checkpoint_dir, "best_model_info.json")
            if not os.path.exists(best_info_path):
                print("   âš ï¸ best_model_info.jsonì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì •ë¦¬ í›„ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        total_after, used_after, free_after = shutil.disk_usage(checkpoint_dir)
        freed_gb = (free_after - free) / (1024**3)
        if freed_gb > 0:
            print(f"   ğŸ’¾ ì •ë¦¬ë¡œ {freed_gb:.2f}GB ê³µê°„ í™•ë³´ë¨")

    except Exception as e:
        print(f"   âŒ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()


class ImprovedCheckpointInfoCallback(tf.keras.callbacks.Callback):
    """ê°œì„ ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì €ì¥ ì½œë°± - íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì²˜ë¦¬"""

    def __init__(self, actions, checkpoint_dir, training_stats):
        super().__init__()
        self.actions = actions
        self.checkpoint_dir = checkpoint_dir
        self.training_stats = training_stats
        self.saved_checkpoints = set()  # ì´ë¯¸ ì €ì¥í•œ ì²´í¬í¬ì¸íŠ¸ ì¶”ì 
        self.last_scan_time = 0  # ë§ˆì§€ë§‰ ìŠ¤ìº” ì‹œê°„
        self.scan_interval = 5  # ìŠ¤ìº” ê°„ê²© (ì´ˆ)

        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        self._check_disk_space()

    def _check_disk_space(self):
        """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
        try:
            import shutil

            total, used, free = shutil.disk_usage(self.checkpoint_dir)
            free_gb = free / (1024**3)
            if free_gb < 1.0:  # 1GB ë¯¸ë§Œ
                print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {free_gb:.2f}GB ë‚¨ìŒ")
        except Exception as e:
            print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")

    def _should_scan_directory(self):
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº”ì´ í•„ìš”í•œì§€ í™•ì¸"""
        import time

        current_time = time.time()
        if current_time - self.last_scan_time > self.scan_interval:
            self.last_scan_time = current_time
            return True
        return False

    def on_epoch_end(self, epoch, logs=None):
        # ìŠ¤ìº” ê°„ê²© ì œì–´ë¡œ ì„±ëŠ¥ ìµœì í™”
        if not self._should_scan_directory():
            return

        try:
            # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ í™•ì¸ (ì—í­ ê¸°ë°˜ íŒŒì¼ëª…)
            checkpoint_files = [
                f
                for f in os.listdir(self.checkpoint_dir)
                if f.startswith("model-epoch-") and f.endswith(".keras")
            ]

            for checkpoint_file in checkpoint_files:
                # ì´ë¯¸ ì²˜ë¦¬í•œ ì²´í¬í¬ì¸íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°
                if checkpoint_file in self.saved_checkpoints:
                    continue

                checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_file)

                # íŒŒì¼ëª…ì—ì„œ ì—í­ ì •ë³´ ì¶”ì¶œ
                try:
                    # "model-epoch-05.keras" -> epoch=5
                    parts = checkpoint_file.split("-")
                    epoch_part = parts[2].split(".")[0]  # "05"
                    epoch_num = int(epoch_part)

                    # ì„±ëŠ¥ ì •ë³´ëŠ” logsì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    val_accuracy = logs.get("val_accuracy", 0) if logs else 0

                except (IndexError, ValueError) as e:
                    print(f"âš ï¸ íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨: {checkpoint_file} - {e}")
                    epoch_num = epoch + 1
                    val_accuracy = logs.get("val_accuracy", 0) if logs else 0

                # ì²´í¬í¬ì¸íŠ¸ë³„ ëª¨ë¸ ì •ë³´ ìƒì„± (ìµœì¢… ê²°ê³¼ ì–‘ì‹ê³¼ ì¼ì¹˜)
                checkpoint_info = {
                    "model_path": checkpoint_path,
                    "created_at": datetime.now().isoformat(),
                    "labels": self.actions,
                    "label_mapping": {
                        label: idx for idx, label in enumerate(self.actions)
                    },
                    "num_classes": len(self.actions),
                    "input_shape": [TARGET_SEQ_LENGTH, 675],
                    "training_stats": {
                        **self.training_stats,
                        "checkpoint_epoch": epoch_num,
                        "checkpoint_accuracy": logs.get("accuracy", 0) if logs else 0,
                        "checkpoint_val_accuracy": val_accuracy,
                        "checkpoint_loss": logs.get("loss", 0) if logs else 0,
                        "checkpoint_val_loss": logs.get("val_loss", 0) if logs else 0,
                    },
                    "model_type": "LSTM",
                    "description": f"ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ - LSTM ê¸°ë°˜ (Epoch {epoch_num}, Val Acc: {val_accuracy:.4f})",
                }

                # ì²´í¬í¬ì¸íŠ¸ë³„ info íŒŒì¼ ì €ì¥
                checkpoint_info_path = checkpoint_path.replace(".keras", "_info.json")
                try:
                    with open(checkpoint_info_path, "w", encoding="utf-8") as f:
                        json.dump(checkpoint_info, f, ensure_ascii=False, indent=2)

                    # ì¶œë ¥ì„ ì¤„ì—¬ì„œ ì¤‘ì²© ë°©ì§€ (ì—í­ ëì—ë§Œ ì¶œë ¥)
                    if epoch_num % 5 == 0:  # 5 ì—í­ë§ˆë‹¤ë§Œ ì¶œë ¥
                        print(f"ğŸ“„ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì €ì¥: Epoch {epoch_num}")
                    self.saved_checkpoints.add(checkpoint_file)

                    # ë©”ëª¨ë¦¬ ìµœì í™”: ì„¸íŠ¸ í¬ê¸° ì œí•œ
                    if len(self.saved_checkpoints) > 100:
                        # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±°
                        oldest_items = list(self.saved_checkpoints)[:20]
                        for item in oldest_items:
                            self.saved_checkpoints.remove(item)

                except Exception as e:
                    print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {checkpoint_file} - {e}")

        except Exception as e:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def on_train_end(self, logs=None):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë³µì‚¬"""
        try:
            # ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì˜ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            checkpoint_files = [
                f
                for f in os.listdir(self.checkpoint_dir)
                if f.startswith("model-epoch-") and f.endswith(".keras")
            ]

            if checkpoint_files:
                # ì—í­ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ ìµœê·¼ì´ ë§ˆì§€ë§‰)
                def extract_epoch(filename):
                    try:
                        epoch_part = filename.split("-")[2].split(".")[0]  # "05"
                        return int(epoch_part)
                    except:
                        return 0

                # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ bestë¡œ ì„ íƒ
                best_checkpoint = max(checkpoint_files, key=extract_epoch)
                best_path = os.path.join(self.checkpoint_dir, best_checkpoint)
                best_final_path = os.path.join(self.checkpoint_dir, "best_model.keras")

                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ best_model.kerasë¡œ ë³µì‚¬
                import shutil

                shutil.copy2(best_path, best_final_path)

                # best_model_info.jsonë„ ë³µì‚¬
                best_info_path = best_path.replace(".keras", "_info.json")
                best_final_info_path = best_final_path.replace(".keras", "_info.json")
                if os.path.exists(best_info_path):
                    shutil.copy2(best_info_path, best_final_info_path)

                print(f"ğŸ† ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë³µì‚¬: {best_checkpoint} -> best_model.keras")

        except Exception as e:
            print(f"âš ï¸ ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë³µì‚¬ ì‹¤íŒ¨: {e}")


def load_latest_checkpoint(checkpoint_dir="checkpoints"):
    """ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(checkpoint_dir):
        return None, None, 0

    try:
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸°
        checkpoint_files = [
            f
            for f in os.listdir(checkpoint_dir)
            if f.startswith("model-epoch-") and f.endswith(".keras")
        ]

        if not checkpoint_files:
            return None, None, 0

        # ì—í­ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ ìµœê·¼ì´ ë§ˆì§€ë§‰)
        def extract_epoch(filename):
            try:
                epoch_part = filename.split("-")[2]  # "05"
                return int(epoch_part)
            except:
                return 0

        checkpoint_files.sort(key=extract_epoch)
        latest_checkpoint = checkpoint_files[-1]
        latest_path = os.path.join(checkpoint_dir, latest_checkpoint)

        # ì—í­ ë²ˆí˜¸ ì¶”ì¶œ
        latest_epoch = extract_epoch(latest_checkpoint)

        # í•´ë‹¹í•˜ëŠ” info íŒŒì¼ ë¡œë“œ
        info_path = latest_path.replace(".keras", "_info.json")
        checkpoint_info = None
        if os.path.exists(info_path):
            try:
                with open(info_path, "r", encoding="utf-8") as f:
                    checkpoint_info = json.load(f)
            except Exception as e:
                print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")

        print(f"ğŸ“‚ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {latest_checkpoint} (Epoch {latest_epoch})")
        return latest_path, checkpoint_info, latest_epoch

    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None, 0


def resume_training_from_checkpoint(
    model, checkpoint_path, checkpoint_info, latest_epoch
):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤."""
    try:
        print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ: {checkpoint_path}")

        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_weights(checkpoint_path)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ (Epoch {latest_epoch})")

        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥
        if checkpoint_info:
            print(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ ì„±ëŠ¥:")
            print(f"   - ê²€ì¦ ì •í™•ë„: {checkpoint_info.get('val_accuracy', 'N/A')}")
            print(
                f"   - ê²€ì¦ ì†ì‹¤: {checkpoint_info.get('training_stats', {}).get('checkpoint_val_loss', 'N/A')}"
            )

        return True

    except Exception as e:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def save_none_class_cache(none_class, data, target_count):
    """None í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤. target_count ì •ë³´ë„ í¬í•¨í•©ë‹ˆë‹¤."""
    cache_path = get_label_cache_path(none_class)

    # ìºì‹œì— ì €ì¥í•  ë°ì´í„°ì™€ íŒŒë¼ë¯¸í„° ì •ë³´
    cache_data = {
        "data": data,
        "parameters": {
            "TARGET_SEQ_LENGTH": TARGET_SEQ_LENGTH,
            "AUGMENTATIONS_PER_VIDEO": AUGMENTATIONS_PER_VIDEO,
            "AUGMENTATION_NOISE_LEVEL": AUGMENTATION_NOISE_LEVEL,
            "AUGMENTATION_SCALE_RANGE": AUGMENTATION_SCALE_RANGE,
            "AUGMENTATION_ROTATION_RANGE": AUGMENTATION_ROTATION_RANGE,
            "NONE_CLASS_NOISE_LEVEL": NONE_CLASS_NOISE_LEVEL,
            "NONE_CLASS_AUGMENTATIONS_PER_FRAME": NONE_CLASS_AUGMENTATIONS_PER_FRAME,
            # ë°ì´í„° ê°œìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
            "LABEL_MAX_SAMPLES_PER_CLASS": LABEL_MAX_SAMPLES_PER_CLASS,
            "MIN_SAMPLES_PER_CLASS": MIN_SAMPLES_PER_CLASS,
            # None í´ë˜ìŠ¤ íŠ¹ë³„ íŒŒë¼ë¯¸í„°
            "TARGET_NONE_COUNT": target_count,
        },
    }

    # S3 í˜¸í™˜ ìºì‹œ ì €ì¥
    try:
        if is_s3_path(cache_path):
            # S3ì—ì„œëŠ” put_objectê°€ ì›ìì ì´ë¯€ë¡œ ì§ì ‘ ì €ì¥
            success = cache_save_pickle(cache_path, cache_data)
            if success:
                print(
                    f"ğŸ’¾ {none_class} í´ë˜ìŠ¤ ë°ì´í„° ìºì‹œ ì €ì¥ (S3): {cache_path} ({len(data)}ê°œ ìƒ˜í”Œ, ëª©í‘œ: {target_count}ê°œ)"
                )
            else:
                raise Exception("S3 ìºì‹œ ì €ì¥ ì‹¤íŒ¨")
        else:
            # ë¡œì»¬ì—ì„œëŠ” ì„ì‹œ íŒŒì¼ ë°©ì‹ ì‚¬ìš© (ì›ìì  ì“°ê¸°)
            temp_path = cache_path + ".tmp"

            with open(temp_path, "wb") as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)

            # ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ë©´ ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™
            os.replace(temp_path, cache_path)
            print(
                f"ğŸ’¾ {none_class} í´ë˜ìŠ¤ ë°ì´í„° ìºì‹œ ì €ì¥ (ë¡œì»¬): {cache_path} ({len(data)}ê°œ ìƒ˜í”Œ, ëª©í‘œ: {target_count}ê°œ)"
            )

    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš°ë§Œ)
        if not is_s3_path(cache_path):
            temp_path = cache_path + ".tmp"
            if os.path.exists(temp_path):
                os.remove(temp_path)
        raise e


def load_none_class_cache(none_class, target_count):
    """None í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤. target_count ì •ë³´ë„ ê²€ì¦í•©ë‹ˆë‹¤."""
    cache_path = get_label_cache_path(none_class)
    if cache_exists(cache_path):
        try:
            # S3 í˜¸í™˜ ìºì‹œ ë¡œë“œ
            cache_data = cache_load_pickle(cache_path)
            if cache_data is None:
                return None

            # ìºì‹œ í˜•ì‹ í™•ì¸ (êµ¬ë²„ì „ í˜¸í™˜ì„±)
            if (
                isinstance(cache_data, dict)
                and "data" in cache_data
                and "parameters" in cache_data
            ):
                # ìƒˆ í˜•ì‹: íŒŒë¼ë¯¸í„° ê²€ì¦
                cached_params = cache_data["parameters"]
                current_params = {
                    "TARGET_SEQ_LENGTH": TARGET_SEQ_LENGTH,
                    "AUGMENTATIONS_PER_VIDEO": AUGMENTATIONS_PER_VIDEO,
                    "AUGMENTATION_NOISE_LEVEL": AUGMENTATION_NOISE_LEVEL,
                    "AUGMENTATION_SCALE_RANGE": AUGMENTATION_SCALE_RANGE,
                    "AUGMENTATION_ROTATION_RANGE": AUGMENTATION_ROTATION_RANGE,
                    "NONE_CLASS_NOISE_LEVEL": NONE_CLASS_NOISE_LEVEL,
                    "NONE_CLASS_AUGMENTATIONS_PER_FRAME": NONE_CLASS_AUGMENTATIONS_PER_FRAME,
                    # ë°ì´í„° ê°œìˆ˜ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
                    "LABEL_MAX_SAMPLES_PER_CLASS": LABEL_MAX_SAMPLES_PER_CLASS,
                    "MIN_SAMPLES_PER_CLASS": MIN_SAMPLES_PER_CLASS,
                    # None í´ë˜ìŠ¤ íŠ¹ë³„ íŒŒë¼ë¯¸í„°
                    "TARGET_NONE_COUNT": target_count,
                }

                # íŒŒë¼ë¯¸í„° ë¹„êµ
                if cached_params != current_params:
                    print(f"âš ï¸ {none_class} ìºì‹œ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ìºì‹œ ë¬´íš¨í™”.")
                    print(f"   ìºì‹œëœ íŒŒë¼ë¯¸í„°: {cached_params}")
                    print(f"   í˜„ì¬ íŒŒë¼ë¯¸í„°: {current_params}")
                    cache_remove(cache_path)
                    return None

                data = cache_data["data"]
            else:
                # êµ¬ë²„ì „: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ (íŒŒë¼ë¯¸í„° ê²€ì¦ ì—†ì´ ì‚¬ìš©)
                print(
                    f"âš ï¸ {none_class} êµ¬ë²„ì „ ìºì‹œ í˜•ì‹ì…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤."
                )
                data = cache_data

            # ë°ì´í„° ê²€ì¦
            if isinstance(data, list) and len(data) > 0:
                cache_type = "S3" if is_s3_path(cache_path) else "ë¡œì»¬"
                print(
                    f"ğŸ“‚ {none_class} í´ë˜ìŠ¤ ë°ì´í„° ìºì‹œ ë¡œë“œ ({cache_type}): {cache_path} ({len(data)}ê°œ ìƒ˜í”Œ, ëª©í‘œ: {target_count}ê°œ)"
                )
                return data
            else:
                print(f"âš ï¸ {none_class} ìºì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤.")
                return None

        except Exception as e:
            print(f"âš ï¸ {none_class} ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì†ìƒëœ ìºì‹œ íŒŒì¼ ì‚­ì œ
            try:
                cache_remove(cache_path)
                print(f"ğŸ—‘ï¸ ì†ìƒëœ ìºì‹œ íŒŒì¼ ì‚­ì œ: {cache_path}")
            except:
                pass
            return None
    return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    params = sys.argv[1]
    with open(params, "r") as f:
        params = json.load(f)
    label_dict = params["label_dict"]

    ACTIONS = list(label_dict.keys())
    NONE_CLASS = ACTIONS[-1]

    print(f"ğŸ”§ ë¼ë²¨ ëª©ë¡: {ACTIONS}")
    # 1. ë¹„ë””ì˜¤ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²€ì¦
    valid_roots = validate_video_roots()
    if not valid_roots:
        print("âŒ ìœ íš¨í•œ ë¹„ë””ì˜¤ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 2. labels.csv íŒŒì¼ ì½ê¸° ë° ê²€ì¦
    if not os.path.exists("labels.csv"):
        print("âŒ labels.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    labels_df = pd.read_csv("labels.csv")
    print(f"ğŸ“Š labels.csv ë¡œë“œ ì™„ë£Œ: {len(labels_df)}ê°œ í•­ëª©")
    print(labels_df.head())

    # 3. íŒŒì¼ëª…ì—ì„œ ë¹„ë””ì˜¤ ë£¨íŠ¸ ê²½ë¡œ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
    print("\nğŸ” íŒŒì¼ëª… ë¶„ì„ ë° ê²½ë¡œ ë§¤í•‘ ì¤‘...")
    file_mapping = {}
    found_files = 0
    missing_files = 0
    filtered_files = 0

    # ë¼ë²¨ë³„ë¡œ íŒŒì¼ì„ ëª¨ì•„ì„œ ìµœëŒ€ ê°œìˆ˜ë§Œí¼ë§Œ ìƒ˜í”Œë§
    label_to_files = defaultdict(list)
    for idx, row in labels_df.iterrows():
        filename = row["íŒŒì¼ëª…"]
        label = row["í•œêµ­ì–´"]
        if label not in ACTIONS:
            continue
        file_path = get_video_root_and_path(filename)
        if file_path:
            label_to_files[label].append((filename, file_path))
            found_files += 1
            filtered_files += 1
        else:
            missing_files += 1

    # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ë§Œ ìƒ˜í”Œë§
    for label in ACTIONS:
        files = label_to_files[label]
        if LABEL_MAX_SAMPLES_PER_CLASS is not None:
            files = files[:LABEL_MAX_SAMPLES_PER_CLASS]
        for filename, file_path in files:
            file_mapping[filename] = {"path": file_path, "label": label}

    # [ìˆ˜ì •] ë¼ë²¨ë³„ ì›ë³¸ ì˜ìƒ ê°œìˆ˜ ì²´í¬ ë° ìµœì†Œ ê°œìˆ˜ ë¯¸ë‹¬ ì‹œ í•™ìŠµ ì¤‘ë‹¨ (Noneì€ ì˜ˆì™¸)
    insufficient_labels = []
    for label in ACTIONS:
        if label == NONE_CLASS:
            continue  # None í´ë˜ìŠ¤ëŠ” ì˜ˆì™¸
        num_samples = len(label_to_files[label])
        if num_samples < MIN_SAMPLES_PER_CLASS:
            insufficient_labels.append((label, num_samples))
    if insufficient_labels:
        print("\nâŒ ìµœì†Œ ìƒ˜í”Œ ê°œìˆ˜ ë¯¸ë‹¬ ë¼ë²¨ ë°œê²¬! í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        for label, count in insufficient_labels:
            print(f"   - {label}: {count}ê°œ (ìµœì†Œ í•„ìš”: {MIN_SAMPLES_PER_CLASS}ê°œ)")
        sys.exit(1)

    print(f"\nğŸ“Š íŒŒì¼ ë§¤í•‘ ê²°ê³¼:")
    print(f"   âœ… ì°¾ì€ íŒŒì¼: {found_files}ê°œ")
    print(f"   âŒ ëˆ„ë½ëœ íŒŒì¼: {missing_files}ê°œ")
    print(f"   ğŸ¯ ACTIONS ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼: {filtered_files}ê°œ")
    print(f"   âš¡ ë¼ë²¨ë³„ ìµœëŒ€ {LABEL_MAX_SAMPLES_PER_CLASS}ê°œ íŒŒì¼ë§Œ ì‚¬ìš©")
    print(f"   âš¡ ë¼ë²¨ë³„ ìµœì†Œ {MIN_SAMPLES_PER_CLASS}ê°œ íŒŒì¼ í•„ìš”")

    if len(file_mapping) == 0:
        print("âŒ ì°¾ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 4. ë¼ë²¨ë³„ ë°ì´í„° ì¶”ì¶œ ë° ìºì‹± (ê°œë³„ ì²˜ë¦¬)
    print("\nğŸš€ ë¼ë²¨ë³„ ë°ì´í„° ì¶”ì¶œ ë° ìºì‹± ì‹œì‘...")

    # None í´ë˜ìŠ¤ ì œì™¸í•œ ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ì˜ í‰ê·  ê°œìˆ˜ ê³„ì‚°
    other_class_counts = {}
    for filename, info in file_mapping.items():
        if info["label"] != NONE_CLASS:
            label = info["label"]
            other_class_counts[label] = other_class_counts.get(label, 0) + 1

    if other_class_counts:
        avg_other_class_count = sum(other_class_counts.values()) / len(
            other_class_counts
        )
        target_none_count = int(avg_other_class_count * (1 + AUGMENTATIONS_PER_VIDEO))
        print(
            f"ğŸ“Š ë‹¤ë¥¸ í´ë˜ìŠ¤ í‰ê· : {avg_other_class_count:.1f}ê°œ â†’ None í´ë˜ìŠ¤ ëª©í‘œ: {target_none_count}ê°œ"
        )
    else:
        target_none_count = None
        print(f"ğŸ“Š ë‹¤ë¥¸ í´ë˜ìŠ¤ê°€ ì—†ìŒ â†’ None í´ë˜ìŠ¤ ê¸°ë³¸ê°’ ì‚¬ìš©")

    X = []
    y = []

    for label in ACTIONS:
        print(f"\n{'='*50}")
        print(f"ğŸ“‹ {label} ë¼ë²¨ ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*50}")

        if label == NONE_CLASS:
            label_data = generate_balanced_none_class_data(
                file_mapping, NONE_CLASS, target_none_count
            )
        else:
            label_data = extract_and_cache_label_data_optimized(file_mapping, label)

        if label_data:
            label_index = get_action_index(label, ACTIONS)
            X.extend(label_data)
            y.extend([label_index] * len(label_data))
            print(f"âœ… {label}: {len(label_data)}ê°œ ìƒ˜í”Œ ì¶”ê°€ë¨")
        else:
            print(f"âš ï¸ {label}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(f"\n{'='*50}")
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° í†µê³„:")
    print(f"{'='*50}")
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}")

    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸
    unique, counts = np.unique(y, return_counts=True)
    for class_idx, count in zip(unique, counts):
        if 0 <= class_idx < len(ACTIONS):
            print(f"í´ë˜ìŠ¤ {class_idx} ({ACTIONS[class_idx]}): {count}ê°œ")
        else:
            print(f"í´ë˜ìŠ¤ {class_idx} (Unknown): {count}ê°œ")

    X = np.array(X)
    y = np.array(y)

    # ëª¨ë¸ í•™ìŠµ
    print("\nğŸ‹ï¸â€â™€ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print(
        f"   ğŸ“Š Early Stopping: patience={EARLY_STOPPING_PATIENCE}, min_delta={EARLY_STOPPING_MIN_DELTA}"
    )
    print(f"   ğŸ“Š Learning Rate: patience={REDUCE_LR_PATIENCE}, min_lr={MIN_LR}")
    print(
        f"   ğŸ“Š ì •ê·œí™”: L2={USE_L2_REGULARIZATION}, BatchNorm={USE_BATCH_NORMALIZATION}"
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
    )

    model = create_simple_model(
        input_shape=(X.shape[1], X.shape[2]), num_classes=len(ACTIONS)
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=LEARNING_RATE, clipnorm=1.0  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì¶”ê°€
        ),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )

    print("\n--- ëª¨ë¸ êµ¬ì¡° ---")
    model.summary()

    # ì²´í¬í¬ì¸íŠ¸ìš© training_stats ë¯¸ë¦¬ ì •ì˜
    training_stats = {
        "total_samples": len(X),
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "augmentations_per_video": AUGMENTATIONS_PER_VIDEO,
        "target_sequence_length": TARGET_SEQ_LENGTH,
        "model_parameters": {
            "lstm_units_1": MODEL_LSTM_UNITS_1,
            "lstm_units_2": MODEL_LSTM_UNITS_2,
            "dense_units": MODEL_DENSE_UNITS,
            "dropout_rate": MODEL_DROPOUT_RATE,
            "l2_regularization": USE_L2_REGULARIZATION,
            "batch_normalization": USE_BATCH_NORMALIZATION,
        },
        "training_parameters": {
            "learning_rate": LEARNING_RATE,
            "batch_size": BATCH_SIZE,
            "early_stopping_patience": EARLY_STOPPING_PATIENCE,
            "early_stopping_min_delta": EARLY_STOPPING_MIN_DELTA,
            "reduce_lr_patience": REDUCE_LR_PATIENCE,
        },
    }

    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë° í•™ìŠµ ì¬ê°œ
    best_checkpoint_path, best_checkpoint_info, best_epoch = load_latest_checkpoint(
        CHECKPOINT_DIR
    )

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œí• ì§€ ê²°ì •
    resume_from_checkpoint = False
    if best_checkpoint_path:
        print(f"ğŸ“‚ ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸: {best_checkpoint_path} (Epoch {best_epoch})")

        # ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ìë™ ê²°ì • (ì—¬ê¸°ì„œëŠ” ìë™ìœ¼ë¡œ ì¬ê°œ)
        resume_from_checkpoint = True

        if resume_from_checkpoint:
            if resume_training_from_checkpoint(
                model, best_checkpoint_path, best_checkpoint_info, best_epoch
            ):
                print("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ ì¤€ë¹„ ì™„ë£Œ")
                initial_epoch = best_epoch
            else:
                print("âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ì²˜ìŒë¶€í„° ì‹œì‘")
                initial_epoch = 0
        else:
            print("ğŸ”„ ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘")
            initial_epoch = 0
    else:
        print("ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘")
        initial_epoch = 0

    # ì½œë°± ì„¤ì •
    callbacks = [
        # í†µí•©ëœ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì—í­ ê¸°ë°˜ íŒŒì¼ëª…)
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(CHECKPOINT_DIR, "model-epoch-{epoch:02d}.keras"),
            save_best_only=False,
            save_freq=5,  # 5 ì—í­ë§ˆë‹¤
            verbose=0,  # ì¶œë ¥ ì¤‘ì²© ë°©ì§€ë¥¼ ìœ„í•´ 0ìœ¼ë¡œ ë³€ê²½
        ),
        # ê°œì„ ëœ Early Stopping
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy",
            mode="max",
            patience=EARLY_STOPPING_PATIENCE,
            min_delta=EARLY_STOPPING_MIN_DELTA,
            restore_best_weights=True,
            verbose=1,
        ),
        # Learning Rate ê°ì†Œ
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_accuracy",
            mode="max",
            factor=0.5,
            patience=REDUCE_LR_PATIENCE,
            min_lr=MIN_LR,
            verbose=1,
        ),
        ImprovedCheckpointInfoCallback(ACTIONS, CHECKPOINT_DIR, training_stats),
    ]

    # ëª¨ë¸ í•™ìŠµ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ)
    history = model.fit(
        X_train,
        y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=2,  # ë” ê¹”ë”í•œ ì§„í–‰ë¥  í‘œì‹œ
        initial_epoch=initial_epoch,  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    )

    print(f"ğŸ§  ìˆ˜ì •ëœ ëª¨ë¸ ì €ì¥: {MODEL_SAVE_PATH}")
    model.save(MODEL_SAVE_PATH)

    # í‰ê°€
    print("\n--- ëª¨ë¸ í‰ê°€ ---")
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"ğŸš€ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")

    # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
    y_pred_prob = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred_prob, axis=1)
    y_true_classes = y_test

    print("\n--- í´ë˜ìŠ¤ë³„ ì •í™•ë„ ---")
    class_accuracies = {}
    for i in range(len(ACTIONS)):
        class_mask = y_true_classes == i
        if np.sum(class_mask) > 0:
            class_accuracy = np.mean(
                y_pred_classes[class_mask] == y_true_classes[class_mask]
            )
            class_accuracies[ACTIONS[i]] = class_accuracy
            print(f"{ACTIONS[i]}: {class_accuracy:.4f}")

    # ëª¨ë¸ ì •ë³´ ì €ì¥ (ìµœì¢… ê²°ê³¼ ì¶”ê°€)
    training_stats.update(
        {
            "test_loss": float(loss),
            "test_accuracy": float(accuracy),
            "class_accuracies": class_accuracies,
        }
    )
    save_model_info(ACTIONS, MODEL_SAVE_PATH, MODEL_INFO_PATH, training_stats)

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {MODEL_SAVE_PATH}")
    print(f"ğŸ“„ ëª¨ë¸ ì •ë³´ ìœ„ì¹˜: {MODEL_INFO_PATH}")

    # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)
    cleanup_old_checkpoints(
        checkpoint_dir=CHECKPOINT_DIR, keep_best=True, max_checkpoints=10
    )


if __name__ == "__main__":
    print("ğŸ”§ í•™ìŠµ ë°ì´í„° ë¬¸ì œ í•´ê²° ë° ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")

    try:
        # ê¸°ì¡´ ëª¨ë¸ ì •ë³´ ë¡œë“œ
        model_info = load_model_info()
        if model_info:
            print(f"ğŸ“‹ ê¸°ì¡´ ëª¨ë¸ ì •ë³´ ë¡œë“œë¨: {model_info['model_name']}")
            print(f"   - ì •í™•ë„: {model_info['test_accuracy']:.4f}")
            print(f"   - ì†ì‹¤: {model_info['test_loss']:.4f}")
            print(f"   - í›ˆë ¨ ì‹œê°„: {model_info['training_time']:.2f}ì´ˆ")

        # ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ ì¬í•™ìŠµ
        main()

    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        # MediaPipe ê°ì²´ ì •ë¦¬
        MediaPipeManager.cleanup()
        print("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
