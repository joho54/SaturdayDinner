#!/usr/bin/env python3
"""
S3Uploader - ë””ë ‰í† ë¦¬ë¥¼ S3ì— ì—…ë¡œë“œí•˜ê³  ê²€ì¦ í›„ ì‚­ì œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì•ˆì „í•œ ì—…ë¡œë“œë¥¼ ìœ„í•´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤:
1. ë¡œì»¬ íŒŒì¼ ëª©ë¡ ìƒì„±
2. S3ì— ì—…ë¡œë“œ
3. ì—…ë¡œë“œëœ íŒŒì¼ ê²€ì¦ (í¬ê¸°, í•´ì‹œ)
4. ê²€ì¦ ì™„ë£Œ í›„ ë¡œì»¬ íŒŒì¼ ì‚­ì œ

ì„¤ì • ë°©ë²•:
1. .env íŒŒì¼ ìƒì„±:
   AWS_ACCESS_KEY_ID=your-access-key
   AWS_SECRET_ACCESS_KEY=your-secret-key
   AWS_DEFAULT_REGION=ap-northeast-2
   S3_BUCKET_NAME=your-bucket-name
   AWS_PROFILE=default

2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
   pip install boto3 python-dotenv

ì‚¬ìš©ë²•:
   python S3Uploader.py <directory>
   python S3Uploader.py models/ --prefix "backups/2024-12-01"
"""

import os
import sys
import boto3
import hashlib
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from botocore.exceptions import ClientError, NoCredentialsError
import json

# .env íŒŒì¼ ì§€ì› ì¶”ê°€
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("Warning: python-dotenv not installed. Install with: pip install python-dotenv")
    print("Falling back to system environment variables.")


class S3Uploader:
    """S3ì— ì•ˆì „í•˜ê²Œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ê²€ì¦í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, bucket_name: str = None, aws_profile: str = None):
        """
        S3Uploader ì´ˆê¸°í™”
        
        Args:
            bucket_name: S3 ë²„í‚· ì´ë¦„ (.env íŒŒì¼ì˜ S3_BUCKET_NAME ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
            aws_profile: AWS í”„ë¡œí•„ ì´ë¦„ (.env íŒŒì¼ì˜ AWS_PROFILE ë˜ëŠ” ê¸°ë³¸ê°’: default)
        """
        # .env íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
        self.bucket_name = bucket_name or os.getenv('S3_BUCKET_NAME')
        if not self.bucket_name:
            raise ValueError("S3 bucket name must be provided via parameter or S3_BUCKET_NAME in .env file")
        
        self.aws_profile = aws_profile or os.getenv('AWS_PROFILE', 'default')
        
        # AWS ìê²©ì¦ëª… í™•ì¸
        self.check_aws_credentials()
        
        self.setup_logging()
        self.setup_s3_client()
        
        self.logger.info(f"S3Uploader initialized for bucket: {self.bucket_name}")
        self.logger.info(f"AWS Profile: {self.aws_profile}")
        self.logger.info(f"AWS Region: {os.getenv('AWS_DEFAULT_REGION', 'not set')}")

    def check_aws_credentials(self):
        """AWS ìê²©ì¦ëª… í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
        required_vars = ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']
        missing_vars = []
        
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars and self.aws_profile == 'default':
            print("Warning: Missing AWS credentials in environment variables:")
            for var in missing_vars:
                print(f"  - {var}")
            print("Make sure to set them in .env file or configure AWS CLI profile")
            print("\nExample .env file:")
            print("AWS_ACCESS_KEY_ID=your-access-key")
            print("AWS_SECRET_ACCESS_KEY=your-secret-key")
            print("AWS_DEFAULT_REGION=ap-northeast-2")
            print("S3_BUCKET_NAME=your-bucket-name")
            print("AWS_PROFILE=default")

    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = f"s3_upload_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger('S3Uploader')
        self.logger.setLevel(logging.INFO)
        
        # í•¸ë“¤ëŸ¬ê°€ ì´ë¯¸ ìˆë‹¤ë©´ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # í¬ë§¤í„°
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

    def setup_s3_client(self):
        """S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            # AWS ì„¸ì…˜ ìƒì„± (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” í”„ë¡œí•„ ì‚¬ìš©)
            if os.getenv('AWS_ACCESS_KEY_ID') and os.getenv('AWS_SECRET_ACCESS_KEY'):
                # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ ìê²©ì¦ëª… ì‚¬ìš©
                session = boto3.Session(
                    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                    aws_session_token=os.getenv('AWS_SESSION_TOKEN'),  # ì„ì‹œ ìê²©ì¦ëª… ì§€ì›
                    region_name=os.getenv('AWS_DEFAULT_REGION')
                )
                self.logger.info(f"Using AWS credentials from environment variables")
            else:
                # AWS í”„ë¡œí•„ ì‚¬ìš©
                session = boto3.Session(profile_name=self.aws_profile)
                self.logger.info(f"Using AWS profile: {self.aws_profile}")
            
            self.s3_client = session.client('s3')
            
            # ë²„í‚· ì ‘ê·¼ ê¶Œí•œ í™•ì¸
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            self.logger.info(f"Successfully connected to S3 bucket: {self.bucket_name}")
            
        except NoCredentialsError:
            raise RuntimeError(
                "AWS credentials not found. Please:\n"
                "1. Create .env file with AWS credentials, or\n"
                "2. Configure AWS CLI with 'aws configure', or\n"
                "3. Set AWS environment variables"
            )
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == '404':
                raise RuntimeError(f"Bucket '{self.bucket_name}' not found")
            elif error_code == '403':
                raise RuntimeError(f"Access denied to bucket '{self.bucket_name}'")
            else:
                raise RuntimeError(f"Error accessing S3 bucket: {e}")

    def calculate_file_hash(self, file_path: str, chunk_size: int = 8192) -> str:
        """íŒŒì¼ì˜ MD5 í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(chunk_size), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"Error calculating hash for {file_path}: {e}")
            return ""

    def get_local_files_info(self, directory: str) -> Dict[str, Dict]:
        """ë¡œì»¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ì •ë³´ ìˆ˜ì§‘"""
        files_info = {}
        directory_path = Path(directory)
        
        if not directory_path.exists():
            raise FileNotFoundError(f"Directory not found: {directory}")
        
        if not directory_path.is_dir():
            raise ValueError(f"Path is not a directory: {directory}")
        
        self.logger.info(f"Scanning directory: {directory}")
        
        for file_path in directory_path.rglob('*'):
            if file_path.is_file():
                relative_path = file_path.relative_to(directory_path)
                file_size = file_path.stat().st_size
                file_hash = self.calculate_file_hash(str(file_path))
                
                files_info[str(relative_path)] = {
                    'full_path': str(file_path),
                    'size': file_size,
                    'hash': file_hash
                }
        
        self.logger.info(f"Found {len(files_info)} files to upload")
        return files_info

    def upload_file_to_s3(self, local_file: str, s3_key: str) -> bool:
        """ë‹¨ì¼ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ"""
        try:
            self.s3_client.upload_file(local_file, self.bucket_name, s3_key)
            return True
        except Exception as e:
            self.logger.error(f"Failed to upload {local_file} to s3://{self.bucket_name}/{s3_key}: {e}")
            return False

    def verify_s3_file(self, s3_key: str, expected_size: int, expected_hash: str) -> bool:
        """S3ì— ì—…ë¡œë“œëœ íŒŒì¼ ê²€ì¦"""
        try:
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° í™•ì¸
            response = self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)
            s3_size = response['ContentLength']
            
            # í¬ê¸° ê²€ì¦
            if s3_size != expected_size:
                self.logger.error(f"Size mismatch for {s3_key}: expected {expected_size}, got {s3_size}")
                return False
            
            # ETagë¡œ ê°„ë‹¨ ê²€ì¦ (ë©€í‹°íŒŒíŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
            s3_etag = response['ETag'].strip('"')
            if '-' not in s3_etag and s3_etag != expected_hash:
                self.logger.warning(f"Hash mismatch for {s3_key}: expected {expected_hash}, got {s3_etag}")
                # í•´ì‹œ ë¶ˆì¼ì¹˜ëŠ” ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰ (ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ë“±ì˜ ê²½ìš°)
            
            return True
            
        except ClientError as e:
            self.logger.error(f"Failed to verify {s3_key}: {e}")
            return False

    def upload_directory(self, directory: str, s3_prefix: str = "") -> Tuple[bool, Dict]:
        """ë””ë ‰í† ë¦¬ë¥¼ S3ì— ì—…ë¡œë“œ"""
        self.logger.info(f"Starting upload of directory: {directory}")
        
        # ë¡œì»¬ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        local_files = self.get_local_files_info(directory)
        
        if not local_files:
            self.logger.warning("No files found to upload")
            return True, {}
        
        # S3 prefix ì„¤ì •
        if s3_prefix and not s3_prefix.endswith('/'):
            s3_prefix += '/'
        
        upload_results = {}
        successful_uploads = 0
        failed_uploads = 0
        
        # íŒŒì¼ë³„ ì—…ë¡œë“œ
        for relative_path, file_info in local_files.items():
            s3_key = s3_prefix + relative_path.replace('\\', '/')  # Windows ê²½ë¡œ ì²˜ë¦¬
            
            self.logger.info(f"Uploading: {relative_path} -> s3://{self.bucket_name}/{s3_key}")
            
            # ì—…ë¡œë“œ ì‹¤í–‰
            upload_success = self.upload_file_to_s3(file_info['full_path'], s3_key)
            
            if upload_success:
                # ì—…ë¡œë“œ ê²€ì¦
                verify_success = self.verify_s3_file(s3_key, file_info['size'], file_info['hash'])
                
                if verify_success:
                    upload_results[relative_path] = {
                        'status': 'success',
                        's3_key': s3_key,
                        'size': file_info['size']
                    }
                    successful_uploads += 1
                    self.logger.info(f"âœ… Successfully uploaded and verified: {relative_path}")
                else:
                    upload_results[relative_path] = {
                        'status': 'verification_failed',
                        's3_key': s3_key,
                        'error': 'Verification failed'
                    }
                    failed_uploads += 1
                    self.logger.error(f"âŒ Verification failed: {relative_path}")
            else:
                upload_results[relative_path] = {
                    'status': 'upload_failed',
                    'error': 'Upload failed'
                }
                failed_uploads += 1
                self.logger.error(f"âŒ Upload failed: {relative_path}")
        
        # ê²°ê³¼ ìš”ì•½
        success_rate = (successful_uploads / len(local_files)) * 100
        self.logger.info(f"\nUpload Summary:")
        self.logger.info(f"  Total files: {len(local_files)}")
        self.logger.info(f"  Successful: {successful_uploads}")
        self.logger.info(f"  Failed: {failed_uploads}")
        self.logger.info(f"  Success rate: {success_rate:.1f}%")
        
        return failed_uploads == 0, upload_results

    def delete_local_files(self, upload_results: Dict, directory: str) -> bool:
        """ì—…ë¡œë“œ ì„±ê³µí•œ íŒŒì¼ë“¤ì„ ë¡œì»¬ì—ì„œ ì‚­ì œ"""
        self.logger.info("Starting deletion of successfully uploaded files")
        
        deleted_count = 0
        failed_deletions = 0
        
        for relative_path, result in upload_results.items():
            if result['status'] == 'success':
                file_path = os.path.join(directory, relative_path)
                try:
                    os.remove(file_path)
                    deleted_count += 1
                    self.logger.info(f"ğŸ—‘ï¸  Deleted: {relative_path}")
                except Exception as e:
                    failed_deletions += 1
                    self.logger.error(f"Failed to delete {relative_path}: {e}")
        
        # ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬
        self.cleanup_empty_directories(directory)
        
        self.logger.info(f"\nDeletion Summary:")
        self.logger.info(f"  Files deleted: {deleted_count}")
        self.logger.info(f"  Failed deletions: {failed_deletions}")
        
        return failed_deletions == 0

    def cleanup_empty_directories(self, directory: str):
        """ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        directory_path = Path(directory)
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ë¶€í„° ìƒìœ„ë¡œ ì˜¬ë¼ê°€ë©´ì„œ ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ
        for dir_path in sorted(directory_path.rglob('*'), key=lambda p: len(p.parts), reverse=True):
            if dir_path.is_dir() and not any(dir_path.iterdir()):
                try:
                    dir_path.rmdir()
                    self.logger.info(f"ğŸ—‘ï¸  Removed empty directory: {dir_path.relative_to(directory_path)}")
                except Exception as e:
                    self.logger.warning(f"Failed to remove empty directory {dir_path}: {e}")

    def save_upload_report(self, upload_results: Dict, directory: str):
        """ì—…ë¡œë“œ ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥"""
        report_file = f"upload_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "directory": directory,
            "bucket": self.bucket_name,
            "total_files": len(upload_results),
            "successful_uploads": sum(1 for r in upload_results.values() if r['status'] == 'success'),
            "failed_uploads": sum(1 for r in upload_results.values() if r['status'] != 'success'),
            "files": upload_results
        }
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"ğŸ“‹ Upload report saved: {report_file}")
        except Exception as e:
            self.logger.error(f"Failed to save upload report: {e}")

    def upload_and_delete(self, directory: str, s3_prefix: str = "", 
                         confirm_delete: bool = True) -> bool:
        """ë””ë ‰í† ë¦¬ë¥¼ ì—…ë¡œë“œí•˜ê³  ê²€ì¦ í›„ ì‚­ì œí•˜ëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤"""
        try:
            self.logger.info(f"{'='*80}")
            self.logger.info(f"Starting S3 upload and delete process")
            self.logger.info(f"Directory: {directory}")
            self.logger.info(f"S3 Bucket: {self.bucket_name}")
            self.logger.info(f"S3 Prefix: {s3_prefix or '(root)'}")
            self.logger.info(f"{'='*80}")
            
            # 1. ì—…ë¡œë“œ ì‹¤í–‰
            upload_success, upload_results = self.upload_directory(directory, s3_prefix)
            
            # 2. ì—…ë¡œë“œ ë¦¬í¬íŠ¸ ì €ì¥
            self.save_upload_report(upload_results, directory)
            
            if not upload_success:
                self.logger.error("âŒ Upload process completed with failures. Files will NOT be deleted.")
                return False
            
            # 3. ì‚­ì œ í™•ì¸
            if confirm_delete:
                response = input(f"\nâš ï¸  All files uploaded successfully. Delete local files in '{directory}'? (yes/no): ")
                if response.lower() not in ['yes', 'y']:
                    self.logger.info("User cancelled file deletion. Local files preserved.")
                    return True
            
            # 4. ë¡œì»¬ íŒŒì¼ ì‚­ì œ
            delete_success = self.delete_local_files(upload_results, directory)
            
            if delete_success:
                self.logger.info("ğŸ‰ Upload and delete process completed successfully!")
                return True
            else:
                self.logger.warning("âš ï¸  Upload completed but some files could not be deleted.")
                return False
                
        except Exception as e:
            self.logger.error(f"Critical error in upload_and_delete: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='Upload directory to S3 and delete local files after verification',
        epilog="""
Examples:
  python S3Uploader.py models/
  python S3Uploader.py models/ --prefix "backups/2024-12-01"
  python S3Uploader.py models/ --bucket my-bucket --no-confirm

Environment variables (.env file):
  AWS_ACCESS_KEY_ID=your-access-key
  AWS_SECRET_ACCESS_KEY=your-secret-key
  AWS_DEFAULT_REGION=ap-northeast-2
  S3_BUCKET_NAME=your-bucket-name
  AWS_PROFILE=default
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('directory', help='Directory to upload')
    parser.add_argument('--bucket', help='S3 bucket name (or set S3_BUCKET_NAME in .env)')
    parser.add_argument('--prefix', default='', help='S3 prefix/folder (default: root)')
    parser.add_argument('--profile', help='AWS profile name (or set AWS_PROFILE in .env)')
    parser.add_argument('--no-confirm', action='store_true', help='Skip deletion confirmation')
    
    args = parser.parse_args()
    
    try:
        # S3Uploader ì´ˆê¸°í™”
        uploader = S3Uploader(bucket_name=args.bucket, aws_profile=args.profile)
        
        # ì—…ë¡œë“œ ë° ì‚­ì œ ì‹¤í–‰
        success = uploader.upload_and_delete(
            directory=args.directory,
            s3_prefix=args.prefix,
            confirm_delete=not args.no_confirm
        )
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Critical error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
