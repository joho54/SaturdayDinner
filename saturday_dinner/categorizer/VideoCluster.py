#!/usr/bin/env python3
"""
Motion Extraction and Clustering System
ìˆ˜ì–´ ì˜ìƒì—ì„œ MediaPipe í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë™ì‘ ìœ ì‚¬ì„±ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ë§í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import cv2
import numpy as np
import pandas as pd
import mediapipe as mp
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import json
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from saturday_dinner.utils.video_path_utils import get_video_root_and_path
import pickle
from datetime import datetime

@dataclass
class KeypointSequence:
    """í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤"""
    label: str
    filename: str
    sequence: np.ndarray  # Shape: (frames, landmarks, 3) - x, y, z or visibility
    pose_landmarks: np.ndarray
    left_hand_landmarks: np.ndarray
    right_hand_landmarks: np.ndarray
    face_landmarks: np.ndarray
    frame_count: int
    fps: float

class MotionExtractor:
    """MediaPipeë¥¼ ì‚¬ìš©í•œ ë™ì‘ ì¶”ì¶œê¸°"""
    
    def __init__(self, output_dir: str = "extracted-src"):
        self.output_dir = output_dir
        self.setup_mediapipe()
        os.makedirs(output_dir, exist_ok=True)
        
    def setup_mediapipe(self):
        """MediaPipe ëª¨ë¸ ì´ˆê¸°í™”"""
        self.mp_holistic = mp.solutions.holistic
        self.mp_drawing = mp.solutions.drawing_utils
        self.holistic = self.mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=2,
            enable_segmentation=False,
            refine_face_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
    
    def extract_unique_labels_with_first_files(self, labels_csv_path: str) -> Dict[str, str]:
        """ë¼ë²¨ë°ì´í„°ì—ì„œ ìœ ë‹ˆí¬í•œ ë¼ë²¨ê³¼ ì²«ë²ˆì§¸ íŒŒì¼ëª… ìŒ ì¶”ì¶œ"""
        print("ğŸ“‹ ë¼ë²¨ ë°ì´í„°ì—ì„œ ìœ ë‹ˆí¬í•œ ë¼ë²¨ ì¶”ì¶œ ì¤‘...")
        
        df = pd.read_csv(labels_csv_path)
        unique_labels = {}
        
        for _, row in df.iterrows():
            filename = row['íŒŒì¼ëª…']
            label = row['í•œêµ­ì–´']
            
            if label not in unique_labels:
                unique_labels[label] = filename
                
        print(f"âœ… {len(unique_labels)}ê°œì˜ ìœ ë‹ˆí¬í•œ ë¼ë²¨ ë°œê²¬:")
        for i, (label, filename) in enumerate(list(unique_labels.items())[:10]):
            print(f"   {i+1}. {filename} -> {label}")
        if len(unique_labels) > 10:
            print(f"   ... ì™¸ {len(unique_labels)-10}ê°œ")
            
        return unique_labels
    
    def extract_keypoints_from_video(self, video_path: str) -> Optional[KeypointSequence]:
        """ë¹„ë””ì˜¤ì—ì„œ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
            
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # í‚¤í¬ì¸íŠ¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        pose_landmarks_list = []
        left_hand_landmarks_list = []
        right_hand_landmarks_list = []
        face_landmarks_list = []
        
        frame_idx = 0
        error_count = 0
        max_errors = 10  # ìµœëŒ€ í—ˆìš© ì˜¤ë¥˜ ìˆ˜
        
        with tqdm(total=frame_count, desc=f"í”„ë ˆì„ ì²˜ë¦¬", leave=False) as pbar:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                    
                try:
                    # RGB ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # MediaPipe ì²˜ë¦¬
                    results = self.holistic.process(frame_rgb)
                    
                    # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                    pose_landmarks = self._extract_pose_landmarks(results.pose_landmarks)
                    left_hand = self._extract_hand_landmarks(results.left_hand_landmarks)
                    right_hand = self._extract_hand_landmarks(results.right_hand_landmarks)
                    face = self._extract_face_landmarks(results.face_landmarks)
                    
                    pose_landmarks_list.append(pose_landmarks)
                    left_hand_landmarks_list.append(left_hand)
                    right_hand_landmarks_list.append(right_hand)
                    face_landmarks_list.append(face)
                    
                except Exception as e:
                    error_count += 1
                    # ë„ˆë¬´ ë§ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨
                    if error_count > max_errors:
                        print(f"âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {video_path}")
                        print(f"   ì˜¤ë¥˜ ìˆ˜: {error_count}/{frame_idx + 1}")
                        cap.release()
                        return None
                    
                    # ì˜¤ë¥˜ í”„ë ˆì„ì€ ì˜ì  í‚¤í¬ì¸íŠ¸ë¡œ ëŒ€ì²´
                    pose_landmarks_list.append(np.zeros((33, 3)))
                    left_hand_landmarks_list.append(np.zeros((21, 3)))
                    right_hand_landmarks_list.append(np.zeros((21, 3)))
                    face_landmarks_list.append(np.zeros((468, 3)))
                
                frame_idx += 1
                pbar.update(1)
                
        cap.release()
        
        # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
        if error_count > 0:
            print(f"âš ï¸ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ {error_count}ê°œ ì˜¤ë¥˜ ë°œìƒ (ì´ {frame_idx}ê°œ í”„ë ˆì„)")
        
        if not pose_landmarks_list:
            print(f"âŒ í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
            
        # ì•ˆì „í•œ numpy ë°°ì—´ ë³€í™˜ (í˜•ìƒ ë¶ˆì¼ì¹˜ ì˜ˆì™¸ ì²˜ë¦¬)
        try:
            pose_landmarks = np.array(pose_landmarks_list)
            left_hand_landmarks = np.array(left_hand_landmarks_list)
            right_hand_landmarks = np.array(right_hand_landmarks_list)
            face_landmarks = np.array(face_landmarks_list)
            
            # ë°°ì—´ í˜•ìƒ ê²€ì¦
            if (pose_landmarks.ndim != 3 or left_hand_landmarks.ndim != 3 or 
                right_hand_landmarks.ndim != 3 or face_landmarks.ndim != 3):
                print(f"âŒ ë°°ì—´ í˜•ìƒ ì˜¤ë¥˜: {video_path}")
                print(f"   í¬ì¦ˆ: {pose_landmarks.shape}, ì™¼ì†: {left_hand_landmarks.shape}")
                print(f"   ì˜¤ë¥¸ì†: {right_hand_landmarks.shape}, ì–¼êµ´: {face_landmarks.shape}")
                return None
            
            # ì „ì²´ ì‹œí€¸ìŠ¤ ê²°í•© (pose + hands + face)
            full_sequence = np.concatenate([
                pose_landmarks, left_hand_landmarks, right_hand_landmarks, face_landmarks
            ], axis=1)
            
        except (ValueError, TypeError) as e:
            print(f"âŒ í‚¤í¬ì¸íŠ¸ ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨: {video_path}")
            print(f"   ì˜¤ë¥˜: {str(e)}")
            print(f"   í”„ë ˆì„ ìˆ˜: í¬ì¦ˆ={len(pose_landmarks_list)}, ì™¼ì†={len(left_hand_landmarks_list)}")
            print(f"             ì˜¤ë¥¸ì†={len(right_hand_landmarks_list)}, ì–¼êµ´={len(face_landmarks_list)}")
            return None
        
        filename = os.path.basename(video_path)
        
        return KeypointSequence(
            label="",  # ë‚˜ì¤‘ì— ì„¤ì •
            filename=filename,
            sequence=full_sequence,
            pose_landmarks=pose_landmarks,
            left_hand_landmarks=left_hand_landmarks,
            right_hand_landmarks=right_hand_landmarks,
            face_landmarks=face_landmarks,
            frame_count=frame_idx,
            fps=fps
        )
        
    def compose_extracted_sequences(self) -> List[Tuple[str, str]]:
        """ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ íŒŒì¼ë“¤ì„ ì¡°í•©í•˜ì—¬ ë°˜í™˜"""
        print(f"ğŸ“‚ {self.output_dir} ë””ë ‰í† ë¦¬ì—ì„œ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not os.path.exists(self.output_dir):
            print(f"âŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.output_dir}")
            return []
        
        extracted_sequences = []
        failed_loads = []
        
        # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ê²€ìƒ‰
        all_files = os.listdir(self.output_dir)
        pkl_files = [f for f in all_files if f.endswith('.pkl')]
        
        print(f"âœ… {len(pkl_files)}ê°œì˜ .pkl íŒŒì¼ ë°œê²¬")
        
        if not pkl_files:
            print("âš ï¸ .pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        for file in tqdm(pkl_files, desc="ì‹œí€¸ìŠ¤ íŒŒì¼ ë¡œë”©"):
            # ì „ì²´ ê²½ë¡œ ìƒì„±
            full_path = os.path.join(self.output_dir, file)
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(full_path):
                failed_loads.append((file, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"))
                continue
            
            try:
                # ì‹œí€¸ìŠ¤ ë¡œë”©
                with open(full_path, 'rb') as f:
                    sequence = pickle.load(f)
                    
                # ë¼ë²¨ í™•ì¸
                if hasattr(sequence, 'label') and sequence.label:
                    # ì „ì²´ ê²½ë¡œ ë°˜í™˜ (í´ëŸ¬ìŠ¤í„°ëŸ¬ê°€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
                    extracted_sequences.append((full_path, sequence.label))
                else:
                    failed_loads.append((file, "ë¼ë²¨ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ"))
                
            except Exception as e:
                failed_loads.append((file, f"ë¡œë”© ì‹¤íŒ¨: {str(e)}"))
            
        print(f"\nğŸ“Š ì‹œí€¸ìŠ¤ íŒŒì¼ ë¡œë”© ê²°ê³¼:")
        print(f"   âœ… ì„±ê³µ: {len(extracted_sequences)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_loads)}ê°œ")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ì •ë³´ ì¶œë ¥
        if failed_loads:
            print(f"\nì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
            for file, reason in failed_loads[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {file}: {reason}")
            if len(failed_loads) > 10:
                print(f"   ... ì™¸ {len(failed_loads) - 10}ê°œ")
        
        # ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ëœ ë¼ë²¨ë“¤ ìƒ˜í”Œ ì¶œë ¥
        if extracted_sequences:
            print(f"\në¡œë”©ëœ ë¼ë²¨ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
            for i, (path, label) in enumerate(extracted_sequences[:10]):
                filename = os.path.basename(path)
                print(f"   {i+1}. {filename} -> {label}")
            if len(extracted_sequences) > 10:
                print(f"   ... ì™¸ {len(extracted_sequences) - 10}ê°œ")
            
        return extracted_sequences
                    
    def _extract_pose_landmarks(self, landmarks) -> np.ndarray:
        """í¬ì¦ˆ ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (33ê°œ ì )"""
        if landmarks is None:
            return np.zeros((33, 3))
            
        landmarks_array = []
        for landmark in landmarks.landmark:
            landmarks_array.append([landmark.x, landmark.y, landmark.z])
            
        return np.array(landmarks_array)
    
    def _extract_hand_landmarks(self, landmarks) -> np.ndarray:
        """ì† ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (21ê°œ ì )"""
        if landmarks is None:
            return np.zeros((21, 3))
            
        landmarks_array = []
        for landmark in landmarks.landmark:
            landmarks_array.append([landmark.x, landmark.y, landmark.z])
            
        return np.array(landmarks_array)
    
    def _extract_face_landmarks(self, landmarks) -> np.ndarray:
        """ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (468ê°œ ì )"""
        if landmarks is None:
            return np.zeros((468, 3))
            
        landmarks_array = []
        for landmark in landmarks.landmark:
            landmarks_array.append([landmark.x, landmark.y, landmark.z])
            
        return np.array(landmarks_array)
    
    def generate_sequence_filename(self, filename: str, label: str) -> str:
        """ì‹œí€¸ìŠ¤ íŒŒì¼ëª… ìƒì„±"""
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        safe_label = "".join(c for c in label if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_filename = filename.replace('.', '_')
        
        output_filename = f"{safe_filename}_{safe_label}.pkl"
        output_path = os.path.join(self.output_dir, output_filename)
        
        return output_path
    
    def sequence_exists(self, filename: str, label: str) -> bool:
        """ì‹œí€¸ìŠ¤ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        sequence_path = self.generate_sequence_filename(filename, label)
        return os.path.exists(sequence_path)
    
    def save_sequence(self, sequence: KeypointSequence, label: str) -> str:
        """í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        sequence.label = label
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        output_path = self.generate_sequence_filename(sequence.filename, label)
        
        # ì‹œí€¸ìŠ¤ ì €ì¥
        with open(output_path, 'wb') as f:
            pickle.dump(sequence, f)
            
        return output_path
    
    def extract_all_sequences(self, labels_dict: Dict[str, str]) -> List[Tuple[str, str]]:
        """ëª¨ë“  ë¼ë²¨ì— ëŒ€í•´ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ"""
        print(f"\nğŸ¬ {len(labels_dict)}ê°œ ì˜ìƒì—ì„œ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ ì‹œì‘...")
        
        extracted_sequences = []
        failed_extractions = []
        skipped_sequences = []
        
        for label, filename in tqdm(labels_dict.items(), desc="ì˜ìƒ ì²˜ë¦¬"):
            print(f"\nì²˜ë¦¬ ì¤‘: {filename} -> {label}")
            
            # ë¹„ë””ì˜¤ ê²½ë¡œ ì°¾ê¸° (ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ ë¨¼ì € ì‹¤í–‰)
            video_path = get_video_root_and_path(filename, verbose=False)
            
            if video_path is None:
                print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                failed_extractions.append((filename, label, "íŒŒì¼ ì—†ìŒ"))
                continue
            
            # ğŸ” ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€: ì‹¤ì œ ë¹„ë””ì˜¤ íŒŒì¼ëª…ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            actual_filename = os.path.basename(video_path)  # ì‹¤ì œ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
            if self.sequence_exists(actual_filename, label):
                existing_path = self.generate_sequence_filename(actual_filename, label)
                extracted_sequences.append((existing_path, label))
                skipped_sequences.append((filename, label))
                print(f"â­ï¸ ì´ë¯¸ ì¡´ì¬í•¨: {existing_path}")
                continue
                
            # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            sequence = self.extract_keypoints_from_video(video_path)
            
            if sequence is None:
                print(f"âŒ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {filename}")
                failed_extractions.append((filename, label, "ì¶”ì¶œ ì‹¤íŒ¨"))
                continue
                
            # ì‹œí€¸ìŠ¤ ì €ì¥ (ì‹¤ì œ íŒŒì¼ëª… ì‚¬ìš©)
            try:
                sequence_path = self.save_sequence(sequence, label)
                extracted_sequences.append((sequence_path, label))
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {sequence_path}")
            except Exception as e:
                print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {filename} - {str(e)}")
                failed_extractions.append((filename, label, f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"))
                
        print(f"\nğŸ“Š ì¶”ì¶œ ì™„ë£Œ:")
        print(f"   âœ… ì„±ê³µ: {len(extracted_sequences)}ê°œ")
        print(f"   â­ï¸ ê±´ë„ˆëœ€: {len(skipped_sequences)}ê°œ (ì´ë¯¸ ì¡´ì¬)")
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_extractions)}ê°œ")
        
        if skipped_sequences:
            print(f"\nê±´ë„ˆë›´ íŒŒì¼ë“¤ (ì´ë¯¸ ì²˜ë¦¬ë¨):")
            for filename, label in skipped_sequences[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {filename} ({label})")
            if len(skipped_sequences) > 10:
                print(f"   ... ì™¸ {len(skipped_sequences) - 10}ê°œ")
        
        if failed_extractions:
            print("\nì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
            for filename, label, reason in failed_extractions:
                print(f"   â€¢ {filename} ({label}): {reason}")
                
        return extracted_sequences

class MotionEmbedder:
    """ë™ì‘ì˜ ë™ì  íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ì„ë² ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=0.95)  # 95% ë¶„ì‚° ìœ ì§€
        
    def extract_dynamic_features(self, sequence: KeypointSequence) -> np.ndarray:
        """ë™ì  íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        
        try:
            # 1. ì†ë„ (Velocity) ê³„ì‚°
            velocity = self._calculate_velocity(sequence.sequence)
            
            # 2. ê°€ì†ë„ (Acceleration) ê³„ì‚°  
            acceleration = self._calculate_acceleration(sequence.sequence)
            
            # 3. ê°ì†ë„ (Angular Velocity) ê³„ì‚°
            angular_velocity = self._calculate_angular_velocity(sequence.sequence)
            
            # 4. ì›€ì§ì„ ê¶¤ì ì˜ íŠ¹ì„±
            trajectory_features = self._extract_trajectory_features(sequence.sequence)
            
            # 5. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì„±
            frequency_features = self._extract_frequency_features(sequence.sequence)
            
            # 6. ì‹ ì²´ ë¶€ìœ„ë³„ ìƒëŒ€ì  ì›€ì§ì„
            relative_motion = self._calculate_relative_motion(sequence)
            
            # ëª¨ë“  íŠ¹ì„±ì„ ì•ˆì „í•˜ê²Œ ê²°í•©
            feature_list = []
            
            # ê° íŠ¹ì„±ì„ 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”í•˜ì—¬ ì¶”ê°€ (ê³ ì •ëœ í¬ê¸°ë¡œ)
            try:
                velocity_flat = velocity.flatten()
                # ë„ˆë¬´ í° íŠ¹ì„±ì€ í¬ê¸° ì œí•œ
                if len(velocity_flat) > 10000:
                    velocity_flat = velocity_flat[:10000]
                feature_list.append(velocity_flat)
            except:
                feature_list.append(np.zeros(100))  # ê¸°ë³¸ í¬ê¸°
            
            try:
                acceleration_flat = acceleration.flatten()
                if len(acceleration_flat) > 10000:
                    acceleration_flat = acceleration_flat[:10000]
                feature_list.append(acceleration_flat)
            except:
                feature_list.append(np.zeros(100))
            
            try:
                angular_flat = angular_velocity.flatten()
                if len(angular_flat) > 5000:
                    angular_flat = angular_flat[:5000]
                feature_list.append(angular_flat)
            except:
                feature_list.append(np.zeros(50))
            
            try:
                trajectory_flat = trajectory_features.flatten()
                if len(trajectory_flat) > 5000:
                    trajectory_flat = trajectory_flat[:5000]
                feature_list.append(trajectory_flat)
            except:
                feature_list.append(np.zeros(100))
            
            try:
                frequency_flat = frequency_features.flatten()
                if len(frequency_flat) > 1000:
                    frequency_flat = frequency_flat[:1000]
                feature_list.append(frequency_flat)
            except:
                feature_list.append(np.zeros(40))
            
            try:
                relative_flat = relative_motion.flatten()
                feature_list.append(relative_flat)
            except:
                feature_list.append(np.zeros(8))
            
            # ëª¨ë“  íŠ¹ì„± ê²°í•©
            all_features = np.concatenate(feature_list)
            
            # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
            all_features = np.nan_to_num(all_features, nan=0.0, posinf=0.0, neginf=0.0)
            
            return all_features
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {sequence.filename} - {str(e)}")
            # ê¸°ë³¸ íŠ¹ì„± ë²¡í„° ë°˜í™˜
            return np.zeros(1000)  # ê¸°ë³¸ í¬ê¸°ì˜ ì˜ë²¡í„°
    
    def _calculate_velocity(self, sequence: np.ndarray) -> np.ndarray:
        """í”„ë ˆì„ ê°„ ì†ë„ ê³„ì‚°"""
        if len(sequence) < 2:
            return np.zeros_like(sequence)
            
        velocity = np.diff(sequence, axis=0)
        # ì²« í”„ë ˆì„ì€ 0ìœ¼ë¡œ íŒ¨ë”©
        velocity = np.vstack([np.zeros((1,) + velocity.shape[1:]), velocity])
        
        return velocity
    
    def _calculate_acceleration(self, sequence: np.ndarray) -> np.ndarray:
        """ê°€ì†ë„ ê³„ì‚° (ì†ë„ì˜ ë³€í™”ìœ¨)"""
        velocity = self._calculate_velocity(sequence)
        
        if len(velocity) < 2:
            return np.zeros_like(velocity)
            
        acceleration = np.diff(velocity, axis=0)
        acceleration = np.vstack([np.zeros((1,) + acceleration.shape[1:]), acceleration])
        
        return acceleration
    
    def _calculate_angular_velocity(self, sequence: np.ndarray) -> np.ndarray:
        """ê°ì†ë„ ê³„ì‚° (ê´€ì ˆ ê°ë„ì˜ ë³€í™”ìœ¨)"""
        if len(sequence) < 2:
            return np.zeros((len(sequence), min(50, sequence.shape[1] - 1)))
            
        # ì¸ì ‘í•œ ëœë“œë§ˆí¬ ìŒë“¤ì˜ ê°ë„ ë³€í™” ê³„ì‚° (ê³ ì •ëœ ìˆ˜ì˜ íŠ¹ì„±)
        n_landmarks = sequence.shape[1]
        max_pairs = min(50, n_landmarks - 1)  # ìµœëŒ€ 50ìŒì˜ ê°ë„ ë³€í™” ê³„ì‚°
        
        angular_changes = []
        
        for i in range(len(sequence) - 1):
            frame_angular = []
            
            # ì¸ì ‘í•œ ëœë“œë§ˆí¬ ìŒë“¤ì˜ ë²¡í„° ë³€í™” ê³„ì‚°
            for j in range(max_pairs):
                if j + 1 < n_landmarks:
                    # 3D ì¢Œí‘œì—ì„œ ë²¡í„° ê³„ì‚°
                    v1_current = sequence[i, j, :]      # í˜„ì¬ í”„ë ˆì„ì˜ jë²ˆì§¸ ëœë“œë§ˆí¬
                    v2_current = sequence[i, j + 1, :]  # í˜„ì¬ í”„ë ˆì„ì˜ j+1ë²ˆì§¸ ëœë“œë§ˆí¬
                    v1_next = sequence[i + 1, j, :]     # ë‹¤ìŒ í”„ë ˆì„ì˜ jë²ˆì§¸ ëœë“œë§ˆí¬  
                    v2_next = sequence[i + 1, j + 1, :] # ë‹¤ìŒ í”„ë ˆì„ì˜ j+1ë²ˆì§¸ ëœë“œë§ˆí¬
                    
                    # ì•ˆì „í•œ ê°ë„ ë³€í™” ê³„ì‚°
                    try:
                        vec_current = v2_current - v1_current
                        vec_next = v2_next - v1_next
                        
                        # ë²¡í„°ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬
                        if np.linalg.norm(vec_current) < 1e-6 or np.linalg.norm(vec_next) < 1e-6:
                            angle_change = 0.0
                        else:
                            angle_change = self._angle_between_vectors(vec_current, vec_next)
                            if np.isnan(angle_change) or np.isinf(angle_change):
                                angle_change = 0.0
                    except:
                        angle_change = 0.0
                        
                    frame_angular.append(angle_change)
                else:
                    frame_angular.append(0.0)
                    
            angular_changes.append(frame_angular)
            
        # ì²« í”„ë ˆì„ì€ 0ìœ¼ë¡œ íŒ¨ë”©
        if angular_changes:
            first_frame = [0.0] * len(angular_changes[0])
            angular_changes.insert(0, first_frame)
        else:
            # ë¹ˆ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return np.zeros((len(sequence), max_pairs))
            
        return np.array(angular_changes)
    
    def _angle_between_vectors(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """ë‘ ë²¡í„° ê°„ì˜ ê°ë„ ê³„ì‚°"""
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        return np.arccos(cos_angle)
    
    def _extract_trajectory_features(self, sequence: np.ndarray) -> np.ndarray:
        """ì›€ì§ì„ ê¶¤ì ì˜ í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        # ê° ëœë“œë§ˆí¬ë³„ë¡œ ê¶¤ì  íŠ¹ì„± ê³„ì‚°
        for landmark_idx in range(sequence.shape[1]):
            landmark_traj = sequence[:, landmark_idx]
            
            # ê¸°ë³¸ í†µê³„ëŸ‰
            features.extend([
                np.mean(landmark_traj),
                np.std(landmark_traj),
                np.min(landmark_traj),
                np.max(landmark_traj),
                np.ptp(landmark_traj)  # peak-to-peak
            ])
            
        return np.array(features)
    
    def _extract_frequency_features(self, sequence: np.ndarray) -> np.ndarray:
        """ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì„± ì¶”ì¶œ (FFT ê¸°ë°˜)"""
        features = []
        
        # ê° ëœë“œë§ˆí¬ë³„ë¡œ ì£¼íŒŒìˆ˜ ë¶„ì„
        for landmark_idx in range(min(10, sequence.shape[1])):  # ì²˜ìŒ 10ê°œë§Œ ë¶„ì„ (ê³„ì‚° íš¨ìœ¨ì„±)
            landmark_traj = sequence[:, landmark_idx]
            
            # FFT ê³„ì‚°
            fft_result = np.fft.fft(landmark_traj)
            power_spectrum = np.abs(fft_result)
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„
            features.extend([
                np.mean(power_spectrum),
                np.std(power_spectrum),
                np.argmax(power_spectrum),  # ì£¼ìš” ì£¼íŒŒìˆ˜
                np.sum(power_spectrum[:len(power_spectrum)//4])  # ì €ì£¼íŒŒ ì—ë„ˆì§€
            ])
            
        return np.array(features)
    
    def _calculate_relative_motion(self, sequence: KeypointSequence) -> np.ndarray:
        """ì‹ ì²´ ë¶€ìœ„ë³„ ìƒëŒ€ì  ì›€ì§ì„ ê³„ì‚°"""
        features = []
        
        # ê° ì‹ ì²´ ë¶€ìœ„ì˜ ì›€ì§ì„ ì •ë„ ê³„ì‚°
        pose_motion = 0.0
        left_hand_motion = 0.0
        right_hand_motion = 0.0
        face_motion = 0.0
        
        if sequence.pose_landmarks.shape[1] > 0:
            pose_motion = np.mean(np.std(sequence.pose_landmarks, axis=0))
            
        if sequence.left_hand_landmarks.shape[1] > 0:
            left_hand_motion = np.mean(np.std(sequence.left_hand_landmarks, axis=0))
            
        if sequence.right_hand_landmarks.shape[1] > 0:
            right_hand_motion = np.mean(np.std(sequence.right_hand_landmarks, axis=0))
            
        if sequence.face_landmarks.shape[1] > 0:
            face_motion = np.mean(np.std(sequence.face_landmarks, axis=0))
        
        # ì† ëŒ€ë¹„ ëª¸í†µì˜ ì›€ì§ì„ ë¹„ìœ¨
        features.extend([
            left_hand_motion / (pose_motion + 1e-8),
            right_hand_motion / (pose_motion + 1e-8),
            (left_hand_motion + right_hand_motion) / (pose_motion + 1e-8)
        ])
            
        # ì–¼êµ´ê³¼ ëª¸í†µì˜ ìƒëŒ€ì  ì›€ì§ì„
        features.append(face_motion / (pose_motion + 1e-8))
        
        # ì¶”ê°€ì ì¸ ë¹„ìœ¨ íŠ¹ì„±ë“¤
        features.extend([
            left_hand_motion / (right_hand_motion + 1e-8),  # ì™¼ì† vs ì˜¤ë¥¸ì†
            face_motion / (left_hand_motion + right_hand_motion + 1e-8),  # ì–¼êµ´ vs ì†
            pose_motion,  # ì ˆëŒ€ì  ëª¸í†µ ì›€ì§ì„
            left_hand_motion + right_hand_motion,  # ì´ ì† ì›€ì§ì„
        ])
            
        return np.array(features)

class MotionClusterer:
    """ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, embedder: MotionEmbedder):
        self.embedder = embedder
        self.features = None
        self.labels = None
        self.cluster_model = None
        
    def load_sequences(self, sequence_paths: List[str]) -> List[KeypointSequence]:
        """ì €ì¥ëœ ì‹œí€¸ìŠ¤ë“¤ì„ ë¡œë“œ"""
        sequences = []
        
        print(f"ğŸ“‚ {len(sequence_paths)}ê°œ ì‹œí€¸ìŠ¤ ë¡œë”© ì¤‘...")
        
        for path in tqdm(sequence_paths, desc="ì‹œí€¸ìŠ¤ ë¡œë”©"):
            try:
                with open(path, 'rb') as f:
                    sequence = pickle.load(f)
                    sequences.append(sequence)
            except Exception as e:
                print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {path} - {str(e)}")
                
        print(f"âœ… {len(sequences)}ê°œ ì‹œí€¸ìŠ¤ ë¡œë”© ì™„ë£Œ")
        return sequences
    
    def extract_features_from_sequences(self, sequences: List[KeypointSequence]) -> Tuple[np.ndarray, List[str]]:
        """ì‹œí€¸ìŠ¤ë“¤ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        print("ğŸ” ë™ì  íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        
        all_features = []
        labels = []
        failed_extractions = []
        
        for i, sequence in enumerate(tqdm(sequences, desc="íŠ¹ì„± ì¶”ì¶œ")):
            try:
                features = self.embedder.extract_dynamic_features(sequence)
                
                # íŠ¹ì„±ì´ ìœ íš¨í•œì§€ í™•ì¸
                if features is not None and len(features) > 0:
                    # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
                    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
                    all_features.append(features)
                    labels.append(sequence.label)
                else:
                    failed_extractions.append((sequence.filename, "ë¹ˆ íŠ¹ì„± ë²¡í„°"))
                
            except Exception as e:
                failed_extractions.append((sequence.filename, f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"))
                
        if not all_features:
            print("âŒ ì¶”ì¶œëœ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return np.array([]), []
        
        # íŠ¹ì„± ë²¡í„° í¬ê¸° í™•ì¸ ë° ì •ê·œí™”
        feature_sizes = [len(f) for f in all_features]
        min_size = min(feature_sizes)
        max_size = max(feature_sizes)
        
        print(f"ğŸ“Š íŠ¹ì„± ë²¡í„° í¬ê¸° ë¶„ì„:")
        print(f"   ìµœì†Œ í¬ê¸°: {min_size}")
        print(f"   ìµœëŒ€ í¬ê¸°: {max_size}")
        print(f"   í‰ê·  í¬ê¸°: {np.mean(feature_sizes):.1f}")
        
        # í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš° ì²˜ë¦¬
        if min_size != max_size:
            print("âš ï¸ íŠ¹ì„± ë²¡í„° í¬ê¸°ê°€ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. í¬ê¸°ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤...")
            
            # ê°€ì¥ ì¼ë°˜ì ì¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
            from collections import Counter
            size_counts = Counter(feature_sizes)
            target_size = size_counts.most_common(1)[0][0]
            
            print(f"   ê¸°ì¤€ í¬ê¸°: {target_size} (ê°€ì¥ ë¹ˆë²ˆí•œ í¬ê¸°)")
            
            normalized_features = []
            for i, features in enumerate(all_features):
                if len(features) > target_size:
                    # í¬ê¸°ê°€ í° ê²½ìš° ìë¥´ê¸°
                    normalized_features.append(features[:target_size])
                elif len(features) < target_size:
                    # í¬ê¸°ê°€ ì‘ì€ ê²½ìš° 0ìœ¼ë¡œ íŒ¨ë”©
                    padded = np.zeros(target_size)
                    padded[:len(features)] = features
                    normalized_features.append(padded)
                else:
                    normalized_features.append(features)
                
            all_features = normalized_features
        
        try:
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            features_array = np.array(all_features)
            
            # ìµœì¢… ê²€ì¦
            if features_array.ndim != 2:
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ íŠ¹ì„± ë°°ì—´ í˜•íƒœ: {features_array.shape}")
                return np.array([]), []
            
            print(f"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {features_array.shape}")
            
            if failed_extractions:
                print(f"âš ï¸ ì‹¤íŒ¨í•œ ì¶”ì¶œ: {len(failed_extractions)}ê°œ")
                for filename, reason in failed_extractions[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    print(f"   â€¢ {filename}: {reason}")
                if len(failed_extractions) > 5:
                    print(f"   ... ì™¸ {len(failed_extractions) - 5}ê°œ")
            
            return features_array, labels
            
        except Exception as e:
            print(f"âŒ íŠ¹ì„± ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            print(f"   íŠ¹ì„± ê°œìˆ˜: {len(all_features)}")
            if all_features:
                print(f"   ì²« ë²ˆì§¸ íŠ¹ì„± í¬ê¸°: {len(all_features[0])}")
                print(f"   ë§ˆì§€ë§‰ íŠ¹ì„± í¬ê¸°: {len(all_features[-1])}")
            
            return np.array([]), []
    
    def find_optimal_clusters(self, features: np.ndarray, max_cluster_size: int = 20, max_k: int = 50) -> int:
        """ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ í¬í•¨)"""
        print(f"ğŸ¯ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì¤‘ (ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_cluster_size}ê°œ)...")
        
        n_samples = len(features)
        
        # ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° (ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ max_cluster_size ì´í•˜ê°€ ë˜ë„ë¡)
        min_k = max(2, (n_samples + max_cluster_size - 1) // max_cluster_size)  # ì˜¬ë¦¼ ê³„ì‚°
        
        print(f"   ë°ì´í„° ìˆ˜: {n_samples}ê°œ")
        print(f"   ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {min_k}ê°œ")
        
        best_k = min_k
        best_score = -1
        best_max_cluster_size = float('inf')
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ë²”ìœ„ ì„¤ì •
        k_range = range(min_k, min(max_k + 1, n_samples))
        
        for k in tqdm(k_range, desc="í´ëŸ¬ìŠ¤í„° ìˆ˜ í…ŒìŠ¤íŠ¸"):
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(features)
                
                # ê° í´ëŸ¬ìŠ¤í„°ì˜ í¬ê¸° ê³„ì‚°
                unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
                max_current_cluster_size = np.max(cluster_sizes)
                
                # í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ ì¡°ê±´ í™•ì¸
                if max_current_cluster_size <= max_cluster_size:
                    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
                    sil_score = silhouette_score(features, cluster_labels)
                    
                    # ë” ë‚˜ì€ ê²°ê³¼ì¸ì§€ í™•ì¸ (ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ìš°ì„ , í´ëŸ¬ìŠ¤í„° ê· í˜• ê³ ë ¤)
                    if (sil_score > best_score or 
                        (sil_score >= best_score - 0.05 and max_current_cluster_size < best_max_cluster_size)):
                        best_k = k
                        best_score = sil_score
                        best_max_cluster_size = max_current_cluster_size
                        
                else:
                    # í´ëŸ¬ìŠ¤í„° í¬ê¸°ê°€ ì œí•œì„ ë„˜ëŠ” ê²½ìš° ê³„ì† íƒìƒ‰
                    continue
                
            except Exception as e:
                print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ {k} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ê²°ê³¼ ê²€ì¦
        if best_k == min_k and best_score == -1:
            print("âš ï¸ í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            best_k = min_k
            
            # ìµœì¢… ê²€ì¦ì„ ìœ„í•´ í•œ ë²ˆ ë” í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(features)
            unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
            best_max_cluster_size = np.max(cluster_sizes)
            best_score = silhouette_score(features, cluster_labels)
        
        print(f"âœ… ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k}")
        print(f"   ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {best_score:.3f}")
        print(f"   ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {best_max_cluster_size}ê°œ")
        
        return best_k
    
    def cluster_motions(self, sequences: List[KeypointSequence], max_cluster_size: int = 4) -> Dict[str, Any]:
        """ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ í¬í•¨)"""
        print(f"\nğŸ¯ ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_cluster_size}ê°œ)...")
        
        # íŠ¹ì„± ì¶”ì¶œ
        features, labels = self.extract_features_from_sequences(sequences)
        
        if len(features) == 0:
            print("âŒ ì¶”ì¶œëœ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # íŠ¹ì„± ì •ê·œí™”
        features_normalized = self.embedder.scaler.fit_transform(features)
        
        # PCA ì°¨ì› ì¶•ì†Œ
        features_pca = self.embedder.pca.fit_transform(features_normalized)
        
        print(f"ğŸ“Š PCA í›„ ì°¨ì›: {features_pca.shape[1]} (ì›ë³¸: {features.shape[1]})")
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (í¬ê¸° ì œí•œ í¬í•¨)
        optimal_k = self.find_optimal_clusters(features_pca, max_cluster_size=max_cluster_size)
        
        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
        print("ğŸ¯ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
        self.cluster_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = self.cluster_model.fit_predict(features_pca)
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ê²€ì¦
        unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
        max_actual_size = np.max(cluster_sizes)
        
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„ì„:")
        print(f"   í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {np.mean(cluster_sizes):.1f}ê°œ")
        print(f"   ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_actual_size}ê°œ")
        print(f"   ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {np.min(cluster_sizes)}ê°œ")
        
        if max_actual_size > max_cluster_size:
            print(f"âš ï¸ ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ê°€ í¬ê¸° ì œí•œ({max_cluster_size}ê°œ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
            
            # ì¶”ê°€ ë¶„í•  ì‹œë„
            print("ğŸ”„ í° í´ëŸ¬ìŠ¤í„°ë¥¼ ì¶”ê°€ ë¶„í• í•©ë‹ˆë‹¤...")
            cluster_labels = self._split_large_clusters(features_pca, cluster_labels, max_cluster_size)
            
            # ì¬ê²€ì¦
            unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
            max_actual_size = np.max(cluster_sizes)
            print(f"   ë¶„í•  í›„ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_actual_size}ê°œ")
        
        # ê²°ê³¼ ì •ë¦¬
        clustering_results = {
            'cluster_labels': cluster_labels,
            'motion_labels': labels,
            'features': features_pca,
            'n_clusters': len(unique_labels),
            'sequences': sequences,
            'max_cluster_size': max_actual_size
        }
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë™ì‘ ë¶„ì„
        self._analyze_clusters(clustering_results)
        
        return clustering_results
    
    def _split_large_clusters(self, features: np.ndarray, cluster_labels: np.ndarray, max_size: int) -> np.ndarray:
        """í° í´ëŸ¬ìŠ¤í„°ë¥¼ ì¶”ê°€ë¡œ ë¶„í• """
        new_cluster_labels = cluster_labels.copy()
        next_cluster_id = np.max(cluster_labels) + 1
        
        unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
        
        for cluster_id, size in zip(unique_labels, cluster_sizes):
            if size > max_size:
                print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„í•  ì¤‘ ({size}ê°œ -> ëª©í‘œ: {max_size}ê°œ ì´í•˜)")
                
                # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„° ì¸ë±ìŠ¤
                cluster_indices = np.where(cluster_labels == cluster_id)[0]
                cluster_features = features[cluster_indices]
                
                # í•„ìš”í•œ ì„œë¸Œí´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
                n_subclusters = (size + max_size - 1) // max_size  # ì˜¬ë¦¼ ê³„ì‚°
                
                try:
                    # K-meansë¡œ ì„œë¸Œí´ëŸ¬ìŠ¤í„°ë§
                    subkmeans = KMeans(n_clusters=n_subclusters, random_state=42, n_init=10)
                    sub_labels = subkmeans.fit_predict(cluster_features)
                    
                    # ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ID í• ë‹¹
                    for i, sub_label in enumerate(sub_labels):
                        original_idx = cluster_indices[i]
                        if sub_label == 0:
                            # ì²« ë²ˆì§¸ ì„œë¸Œí´ëŸ¬ìŠ¤í„°ëŠ” ì›ë˜ ID ìœ ì§€
                            new_cluster_labels[original_idx] = cluster_id
                        else:
                            # ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œìš´ ID í• ë‹¹
                            new_cluster_labels[original_idx] = next_cluster_id + sub_label - 1
                    
                    next_cluster_id += n_subclusters - 1
                    
                except Exception as e:
                    print(f"     âš ï¸ í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„í•  ì‹¤íŒ¨: {str(e)}")
                
        return new_cluster_labels
    
    def _analyze_clusters(self, results: Dict[str, Any]):
        """í´ëŸ¬ìŠ¤í„°ë³„ ë™ì‘ ë¶„ì„ ë° ì¶œë ¥"""
        cluster_labels = results['cluster_labels']
        motion_labels = results['motion_labels']
        sequences = results['sequences']
        
        print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„:")
        
        for cluster_id in range(results['n_clusters']):
            cluster_indices = np.where(cluster_labels == cluster_id)[0]
            cluster_motions = [motion_labels[i] for i in cluster_indices]
            cluster_files = [sequences[i].filename for i in cluster_indices]
            
            print(f"\nğŸ·ï¸  í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_indices)}ê°œ ë™ì‘):")
            
            # ê°€ì¥ ë¹ˆë²ˆí•œ ë¼ë²¨ë“¤
            from collections import Counter
            motion_counts = Counter(cluster_motions)
            top_motions = motion_counts.most_common(5)
            
            for motion, count in top_motions:
                print(f"   â€¢ {motion}: {count}ê°œ")
                
            # ëŒ€í‘œ íŒŒì¼ë“¤
            if len(cluster_files) <= 3:
                print(f"   íŒŒì¼: {', '.join(cluster_files)}")
            else:
                print(f"   íŒŒì¼ ì˜ˆì‹œ: {', '.join(cluster_files[:3])} ...")

def save_clustering_results_to_csv(clustering_results: Dict[str, Any], output_path: str):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not clustering_results:
        print("âŒ ì €ì¥í•  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    cluster_labels = clustering_results['cluster_labels']
    motion_labels = clustering_results['motion_labels']
    sequences = clustering_results['sequences']
    
    # ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
    results_data = []
    
    for i, (cluster_id, motion_label, sequence) in enumerate(zip(cluster_labels, motion_labels, sequences)):
        results_data.append({
            'label_name': motion_label,
            'cluster_id': int(cluster_id),
            'filename': sequence.filename
        })
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df_results = pd.DataFrame(results_data)
    
    # í´ëŸ¬ìŠ¤í„° IDë¡œ ì •ë ¬
    df_results = df_results.sort_values(['cluster_id', 'label_name', 'filename'])
    
    # CSV ì €ì¥
    df_results.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV ì €ì¥: {output_path}")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ ì¶œë ¥
    cluster_stats = df_results.groupby('cluster_id').agg({
        'label_name': ['count', 'nunique'],
        'filename': 'count'
    }).round(2)
    
    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„:")
    print(f"   ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {df_results['cluster_id'].nunique()}ê°œ")
    print(f"   ì´ ì‹œí€¸ìŠ¤ ìˆ˜: {len(df_results)}ê°œ")
    print(f"   ì´ ë¼ë²¨ ìˆ˜: {df_results['label_name'].nunique()}ê°œ")
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” ë¼ë²¨ë“¤ ì¶œë ¥
    print(f"\nğŸ·ï¸  ê° í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” ë¼ë²¨ë“¤:")
    for cluster_id in sorted(df_results['cluster_id'].unique()):
        cluster_data = df_results[df_results['cluster_id'] == cluster_id]
        label_counts = cluster_data['label_name'].value_counts()
        top_labels = label_counts.head(3)
        
        print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_data)}ê°œ):")
        for label, count in top_labels.items():
            percentage = (count / len(cluster_data)) * 100
            print(f"     â€¢ {label}: {count}ê°œ ({percentage:.1f}%)")
        
        if len(label_counts) > 3:
            print(f"     ... ì™¸ {len(label_counts) - 3}ê°œ ë¼ë²¨")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Motion Extraction and Clustering System ì‹œì‘")
    print("=" * 60)
    
    # 1. ìœ ë‹ˆí¬í•œ ë¼ë²¨ ì¶”ì¶œ
    extractor = MotionExtractor()
    labels_dict = extractor.extract_unique_labels_with_first_files("labels.csv")
    
    # 2. í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ ë° ì €ì¥
    extracted_sequences = extractor.extract_all_sequences(labels_dict)
    
    if not extracted_sequences:
        print("âŒ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # compose extracted sequences from current files in the output directory
    # extracted_sequences = extractor.compose_extracted_sequences()
    
    if not extracted_sequences:
        print("âŒ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 3. extracted_labels.csv ìƒì„±
    df_extracted = pd.DataFrame(extracted_sequences, columns=['sequence_path', 'label'])
    df_extracted.to_csv('extracted_labels.csv', index=False, encoding='utf-8-sig')
    print(f"\nâœ… extracted_labels.csv ìƒì„± ì™„ë£Œ ({len(extracted_sequences)}ê°œ í•­ëª©)")
    
    # 4. ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§
    embedder = MotionEmbedder()
    clusterer = MotionClusterer(embedder)
    
    # ì‹œí€¸ìŠ¤ ë¡œë“œ
    sequence_paths = [path for path, _ in extracted_sequences]
    sequences = clusterer.load_sequences(sequence_paths)
    
    if sequences:
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        clustering_results = clusterer.cluster_motions(sequences)
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥
        if clustering_results:
            
            # CSV íŒŒì¼ë¡œ ì €ì¥
            csv_path = f"two-clusters/video_clusters.csv"
            save_clustering_results_to_csv(clustering_results, csv_path)
    
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
