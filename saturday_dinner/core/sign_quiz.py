import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
import random
import json
import sys
import os
from collections import deque
from PIL import ImageFont, ImageDraw, Image


def load_model_info(model_info_path):
    """ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(model_info_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì •ë³´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def validate_args():
    """ëª…ë ¹í–‰ ì¸ìë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."""
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python3 sign_quiz.py <model_info.json>")
        print("ì˜ˆì‹œ: python3 sign_quiz.py info/model-info-20250626_220849.json")
        sys.exit(1)

    model_info_path = sys.argv[1]
    if not os.path.exists(model_info_path):
        print(f"âŒ ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_info_path}")
        sys.exit(1)

    return model_info_path


# --- ëª¨ë¸ ì •ë³´ ë¡œë“œ ---
model_info_path = validate_args()
model_info = load_model_info(model_info_path)

if not model_info:
    print("âŒ ëª¨ë¸ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# --- ì„¤ì •ê°’ ---
MAX_SEQ_LENGTH = model_info["input_shape"][0]  # JSONì—ì„œ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¡œë“œ
MODEL_SAVE_PATH = model_info["model_path"]  # JSONì—ì„œ ëª¨ë¸ ê²½ë¡œ ë¡œë“œ
ACTIONS = model_info["labels"]  # JSONì—ì„œ ë¼ë²¨ ë¡œë“œ
QUIZ_LABELS = [a for a in ACTIONS if a != "None"]  # None ì œì™¸í•œ í€´ì¦ˆ ë¼ë²¨

print(f"ğŸ“‹ ë¡œë“œëœ ë¼ë²¨: {ACTIONS}")
print(f"ğŸ¯ í€´ì¦ˆ ë¼ë²¨: {QUIZ_LABELS}")
print(f"ğŸ“Š ëª¨ë¸ ê²½ë¡œ: {MODEL_SAVE_PATH}")
print(f"â±ï¸ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQ_LENGTH}")

# --- í•œê¸€ í°íŠ¸ ì„¤ì • ---
FONT_PATH = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"
try:
    font = ImageFont.truetype(FONT_PATH, 30)
except IOError:
    font = ImageFont.load_default()

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()
mp_drawing = mp.solutions.drawing_utils

# ëª¨ë¸ ë¡œë“œ
try:
    model = tf.keras.models.load_model(MODEL_SAVE_PATH)
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {MODEL_SAVE_PATH}")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    exit()


# --- ìœ í‹¸ í•¨ìˆ˜ ---
def draw_korean_text(img, text, pos, font, color=(0, 255, 0)):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


def cleanup_resources():
    """5ë¬¸ì œë§ˆë‹¤ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
    global sequence, hold_counter, feedback, feedback_timer

    # ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™”
    sequence.clear()

    # ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
    hold_counter = 0
    feedback = ""
    feedback_timer = 0

    print(f"ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ (í€´ì¦ˆ {quiz_number})")


# --- í€´ì¦ˆ ìƒíƒœ ë³€ìˆ˜ ---
quiz_index = 0
current_label = QUIZ_LABELS[quiz_index]
feedback = ""
feedback_timer = 0
FEEDBACK_DURATION = 30  # í”„ë ˆì„ ë‹¨ìœ„
CORRECT_HOLD_FRAMES = 10  # 0.5ì´ˆ(30fps ê¸°ì¤€)
hold_counter = 0
quiz_number = 1

# --- ì›¹ìº  ì‹¤í–‰ ---
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

sequence = deque(maxlen=MAX_SEQ_LENGTH)


# --- ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (fixed_realtime_demo.pyì—ì„œ ë³µì‚¬) ---
def normalize_sequence_length(sequence, target_length=30):
    current_length = len(sequence)
    if current_length == target_length:
        return sequence
    x_old = np.linspace(0, 1, current_length)
    x_new = np.linspace(0, 1, target_length)
    normalized_sequence = []
    for i in range(sequence.shape[1]):
        f = np.interp(x_new, x_old, sequence[:, i])
        normalized_sequence.append(f)
    return np.array(normalized_sequence).T


def extract_dynamic_features(sequence):
    velocity = np.diff(sequence, axis=0, prepend=sequence[0:1])
    acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])
    dynamic_features = np.concatenate([sequence, velocity, acceleration], axis=1)
    return dynamic_features


def convert_to_relative_coordinates(landmarks_list):
    relative_landmarks = []
    for frame in landmarks_list:
        if not frame["pose"]:
            relative_landmarks.append(frame)
            continue
        pose_landmarks = frame["pose"].landmark
        left_shoulder = pose_landmarks[11]
        right_shoulder = pose_landmarks[12]
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2
        shoulder_width = abs(right_shoulder.x - left_shoulder.x)
        if shoulder_width == 0:
            shoulder_width = 1.0
        new_frame = {}
        if frame["pose"]:
            relative_pose = []
            for landmark in pose_landmarks:
                rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                relative_pose.append([rel_x, rel_y, rel_z])
            new_frame["pose"] = relative_pose
        for hand_key in ["left_hand", "right_hand"]:
            if frame[hand_key]:
                relative_hand = []
                for landmark in frame[hand_key].landmark:
                    rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                    rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                    rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                    relative_hand.append([rel_x, rel_y, rel_z])
                new_frame[hand_key] = relative_hand
            else:
                new_frame[hand_key] = None
        relative_landmarks.append(new_frame)
    return relative_landmarks


def improved_preprocess_landmarks(landmarks_list):
    if not landmarks_list:
        return np.zeros((MAX_SEQ_LENGTH, 675))
    relative_landmarks = convert_to_relative_coordinates(landmarks_list)
    processed_frames = []
    for frame in relative_landmarks:
        combined = []
        for key in ["pose", "left_hand", "right_hand"]:
            if frame[key]:
                if isinstance(frame[key], list):
                    combined.extend(frame[key])
                else:
                    combined.extend([[l.x, l.y, l.z] for l in frame[key].landmark])
            else:
                num_points = {"pose": 33, "left_hand": 21, "right_hand": 21}[key]
                combined.extend([[0, 0, 0]] * num_points)
        if combined:
            processed_frames.append(np.array(combined).flatten())
        else:
            processed_frames.append(np.zeros(75 * 3))
    if not processed_frames:
        return np.zeros((MAX_SEQ_LENGTH, 675))
    sequence = np.array(processed_frames)
    if len(sequence) > 0:
        try:
            sequence = normalize_sequence_length(sequence, MAX_SEQ_LENGTH)
            sequence = extract_dynamic_features(sequence)
            sequence = (sequence - np.mean(sequence)) / (np.std(sequence) + 1e-8)
            return sequence
        except Exception as e:
            return np.zeros((MAX_SEQ_LENGTH, 675))
    return np.zeros((MAX_SEQ_LENGTH, 675))


while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # ëœë“œë§ˆí¬ ì¶”ì¶œ
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(rgb_frame)

    frame_data = {
        "pose": results.pose_landmarks,
        "left_hand": results.left_hand_landmarks,
        "right_hand": results.right_hand_landmarks,
        "face": results.face_landmarks,
    }
    sequence.append(frame_data)

    # ì‹œí€€ìŠ¤ê°€ ê½‰ ì°¼ì„ ë•Œë§Œ ì˜ˆì¸¡
    if len(sequence) == MAX_SEQ_LENGTH:
        processed_sequence = improved_preprocess_landmarks(list(sequence))
        input_data = np.expand_dims(processed_sequence, axis=0)
        pred_probs = model.predict(input_data, verbose=0)[0]
        pred_class_index = np.argmax(pred_probs)
        current_prediction = ACTIONS[pred_class_index]
        confidence = pred_probs[pred_class_index]

        # ì •ë‹µ íŒì •(1ì´ˆ ì—°ì† ìœ ì§€ í•„ìš”)
        if not feedback:  # í”¼ë“œë°± í‘œì‹œ ì¤‘ì´ ì•„ë‹ ë•Œë§Œ íŒì •
            if current_prediction == current_label and confidence > 0.7:
                hold_counter += 1
            else:
                hold_counter = 0

            if hold_counter >= CORRECT_HOLD_FRAMES:
                feedback = "ì •ë‹µ!"
                feedback_timer = FEEDBACK_DURATION
                hold_counter = 0

    # --- UI í‘œì‹œ ---
    # 1. í˜„ì¬ ë¬¸ì œ ë¼ë²¨ (ë¬¸ì œ ë²ˆí˜¸ í¬í•¨, ê²€ì€ìƒ‰)
    frame = draw_korean_text(
        frame, f"í€´ì¦ˆ {quiz_number}: {current_label}", (20, 30), font, (0, 0, 0)
    )
    # 2. í”¼ë“œë°±
    if feedback:
        frame = draw_korean_text(frame, feedback, (20, 70), font, (0, 0, 0))
        feedback_timer -= 1
        if feedback_timer <= 0:
            feedback = ""
            quiz_index = (quiz_index + 1) % len(QUIZ_LABELS)
            current_label = QUIZ_LABELS[quiz_index]
            quiz_number += 1

            # 5ë¬¸ì œë§ˆë‹¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if quiz_number % 5 == 0:
                cleanup_resources()

    # 3. ëª¨ë¸ íŒì • í™•ë¥  í‘œì‹œ (ê°œë°œìš©, ê²€ì •ìƒ‰)
    if "pred_probs" in locals():
        for i, prob in enumerate(pred_probs):
            label = ACTIONS[i]
            text = f"{label}: {prob*100:.1f}%"
            frame = draw_korean_text(frame, text, (20, 110 + i * 30), font, (0, 0, 0))

    # 4. ëª¨ë¸ ì •ë³´ í‘œì‹œ (ìš°ì¸¡ ìƒë‹¨)
    info_text = f"ëª¨ë¸: {model_info['model_type']}"
    frame = draw_korean_text(
        frame, info_text, (frame.shape[1] - 300, 30), font, (0, 0, 0)
    )

    # test_accuracyê°€ ìˆì„ ë•Œë§Œ í‘œì‹œ
    if (
        "training_stats" in model_info
        and "test_accuracy" in model_info["training_stats"]
    ):
        info_text2 = f"ì •í™•ë„: {model_info['training_stats']['test_accuracy']*100:.1f}%"
        frame = draw_korean_text(
            frame, info_text2, (frame.shape[1] - 300, 60), font, (0, 0, 0)
        )

    cv2.imshow("ìˆ˜ì–´ í€´ì¦ˆ", frame)
    key = cv2.waitKey(5) & 0xFF
    if key == ord("q"):
        break
    elif key == ord("n"):
        # 'n' í‚¤ë¡œ ì •ë‹µ ë¼ë²¨ ë„˜ê¸°ê¸°
        quiz_index = (quiz_index + 1) % len(QUIZ_LABELS)
        current_label = QUIZ_LABELS[quiz_index]
        feedback = ""
        feedback_timer = 0
        hold_counter = 0
        quiz_number += 1

cap.release()
cv2.destroyAllWindows()
holistic.close()

print(f"\nâœ… í€´ì¦ˆ ì¢…ë£Œ")
print(f"ğŸ“Š ì‚¬ìš©ëœ ëª¨ë¸: {model_info['model_type']}")
print(f"ğŸ¯ í€´ì¦ˆ ë¼ë²¨ ìˆ˜: {len(QUIZ_LABELS)}")

# test_accuracyê°€ ìˆì„ ë•Œë§Œ í‘œì‹œ
if "training_stats" in model_info and "test_accuracy" in model_info["training_stats"]:
    print(f"ğŸ“ˆ ëª¨ë¸ ì •í™•ë„: {model_info['training_stats']['test_accuracy']*100:.1f}%")
else:
    print("ğŸ“ˆ ëª¨ë¸ ì •í™•ë„: ì •ë³´ ì—†ìŒ")
