import numpy as np
import tensorflow as tf
import time
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from archive.improved_main import improved_preprocess_landmarks, extract_landmarks
from archive.main import preprocess_landmarks
import os

# ëª¨ë¸ ê²½ë¡œ
LSTM_MODEL_PATH = 'lstm_model_multiclass.keras'
TRANSFORMER_MODEL_PATH = 'improved_transformer_model.keras'
ACTIONS = ["Fire", "Toilet", "None"]

def load_models():
    """ê¸°ì¡´ LSTM ëª¨ë¸ê³¼ ê°œì„ ëœ Transformer ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    models = {}
    
    # LSTM ëª¨ë¸ ë¡œë“œ
    try:
        models['lstm'] = tf.keras.models.load_model(LSTM_MODEL_PATH)
        print(f"âœ… LSTM ëª¨ë¸ ë¡œë”© ì„±ê³µ: {LSTM_MODEL_PATH}")
    except Exception as e:
        print(f"âŒ LSTM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        models['lstm'] = None
    
    # Transformer ëª¨ë¸ ë¡œë“œ
    try:
        models['transformer'] = tf.keras.models.load_model(TRANSFORMER_MODEL_PATH)
        print(f"âœ… Transformer ëª¨ë¸ ë¡œë”© ì„±ê³µ: {TRANSFORMER_MODEL_PATH}")
    except Exception as e:
        print(f"âŒ Transformer ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        models['transformer'] = None
    
    return models

def evaluate_model_performance(model, X_test, y_test, model_name):
    """ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    if model is None:
        return None
    
    print(f"\n--- {model_name} ëª¨ë¸ í‰ê°€ ---")
    
    # ì˜ˆì¸¡ ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    y_pred_prob = model.predict(X_test, verbose=0)
    prediction_time = time.time() - start_time
    
    y_pred_classes = np.argmax(y_pred_prob, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy = np.mean(y_pred_classes == y_true_classes)
    
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print(f"ì •í™•ë„: {accuracy * 100:.2f}%")
    print(f"í‰ê·  ì˜ˆì¸¡ ì‹œê°„: {prediction_time / len(X_test) * 1000:.2f}ms")
    
    # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_true_classes, y_pred_classes, target_names=ACTIONS))
    
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_true_classes, y_pred_classes)
    
    return {
        'accuracy': accuracy,
        'prediction_time': prediction_time / len(X_test),
        'confusion_matrix': cm,
        'y_pred': y_pred_classes,
        'y_true': y_true_classes,
        'y_pred_prob': y_pred_prob
    }

def plot_confusion_matrices(results):
    """í˜¼ë™ í–‰ë ¬ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    for i, (model_name, result) in enumerate(results.items()):
        if result is None:
            continue
            
        cm = result['confusion_matrix']
        ax = axes[i]
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=ACTIONS, yticklabels=ACTIONS, ax=ax)
        ax.set_title(f'{model_name} ëª¨ë¸ í˜¼ë™ í–‰ë ¬')
        ax.set_xlabel('ì˜ˆì¸¡')
        ax.set_ylabel('ì‹¤ì œ')
    
    plt.tight_layout()
    plt.savefig('model_comparison_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_accuracy_comparison(results):
    """ì •í™•ë„ ë¹„êµë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    model_names = []
    accuracies = []
    prediction_times = []
    
    for model_name, result in results.items():
        if result is not None:
            model_names.append(model_name)
            accuracies.append(result['accuracy'] * 100)
            prediction_times.append(result['prediction_time'] * 1000)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ì •í™•ë„ ë¹„êµ
    bars1 = ax1.bar(model_names, accuracies, color=['skyblue', 'lightcoral'])
    ax1.set_title('ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ')
    ax1.set_ylabel('ì •í™•ë„ (%)')
    ax1.set_ylim(0, 100)
    
    # ê°’ í‘œì‹œ
    for bar, acc in zip(bars1, accuracies):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                f'{acc:.1f}%', ha='center', va='bottom')
    
    # ì˜ˆì¸¡ ì‹œê°„ ë¹„êµ
    bars2 = ax2.bar(model_names, prediction_times, color=['lightgreen', 'orange'])
    ax2.set_title('ëª¨ë¸ë³„ ì˜ˆì¸¡ ì‹œê°„ ë¹„êµ')
    ax2.set_ylabel('ì˜ˆì¸¡ ì‹œê°„ (ms)')
    
    # ê°’ í‘œì‹œ
    for bar, time_val in zip(bars2, prediction_times):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{time_val:.2f}ms', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('model_comparison_performance.png', dpi=300, bbox_inches='tight')
    plt.show()

def compare_feature_importance(results):
    """íŠ¹ì§• ì¤‘ìš”ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."""
    if 'transformer' not in results or results['transformer'] is None:
        print("Transformer ëª¨ë¸ì´ ì—†ì–´ íŠ¹ì§• ì¤‘ìš”ë„ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # ê° í´ë˜ìŠ¤ë³„ í‰ê·  ì‹ ë¢°ë„ ë¹„êµ
    transformer_probs = results['transformer']['y_pred_prob']
    lstm_probs = results['lstm']['y_pred_prob'] if results['lstm'] else None
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Transformer ëª¨ë¸ ì‹ ë¢°ë„ ë¶„í¬
    for i, action in enumerate(ACTIONS):
        action_probs = transformer_probs[:, i]
        axes[0].hist(action_probs, alpha=0.7, label=action, bins=20)
    
    axes[0].set_title('Transformer ëª¨ë¸ ì‹ ë¢°ë„ ë¶„í¬')
    axes[0].set_xlabel('ì‹ ë¢°ë„')
    axes[0].set_ylabel('ë¹ˆë„')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # LSTM ëª¨ë¸ ì‹ ë¢°ë„ ë¶„í¬ (ìˆëŠ” ê²½ìš°)
    if lstm_probs is not None:
        for i, action in enumerate(ACTIONS):
            action_probs = lstm_probs[:, i]
            axes[1].hist(action_probs, alpha=0.7, label=action, bins=20)
        
        axes[1].set_title('LSTM ëª¨ë¸ ì‹ ë¢°ë„ ë¶„í¬')
        axes[1].set_xlabel('ì‹ ë¢°ë„')
        axes[1].set_ylabel('ë¹ˆë„')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_comparison_confidence.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    print("ğŸ” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    
    # ëª¨ë¸ ë¡œë“œ
    models = load_models()
    
    if not any(models.values()):
        print("âŒ ë¡œë“œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)
    if os.path.exists('preprocessed_data_multiclass.npz'):
        print("ğŸ“Š ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ ë¹„êµë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
        data = np.load('preprocessed_data_multiclass.npz')
        X = data['X']
        y = data['y']
        
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë§Œ ì‚¬ìš© (20%)
        from sklearn.model_selection import train_test_split
        _, X_test, _, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        results = {}
        
        # LSTM ëª¨ë¸ í‰ê°€
        if models['lstm']:
            results['LSTM'] = evaluate_model_performance(
                models['lstm'], X_test, y_test, "LSTM"
            )
        
        # Transformer ëª¨ë¸ í‰ê°€ (ë°ì´í„° ë³€í™˜ í•„ìš”)
        if models['transformer']:
            # Transformer ëª¨ë¸ìš© ë°ì´í„° ë³€í™˜
            print("\nğŸ”„ Transformer ëª¨ë¸ìš© ë°ì´í„° ë³€í™˜ ì¤‘...")
            X_test_transformer = []
            
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, 
            # ì‹¤ì œë¡œëŠ” improved_preprocess_landmarksë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
            # í˜„ì¬ëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ í˜•íƒœ ìœ ì§€
            X_test_transformer = X_test
            
            results['Transformer'] = evaluate_model_performance(
                models['transformer'], X_test_transformer, y_test, "Transformer"
            )
        
        # ê²°ê³¼ ì‹œê°í™”
        if len(results) > 1:
            print("\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì‹œê°í™”...")
            plot_confusion_matrices(results)
            plot_accuracy_comparison(results)
            compare_feature_importance(results)
            
            # ìš”ì•½
            print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ìš”ì•½:")
            for model_name, result in results.items():
                if result:
                    print(f"{model_name}: ì •í™•ë„ {result['accuracy']*100:.2f}%, "
                          f"í‰ê·  ì˜ˆì¸¡ì‹œê°„ {result['prediction_time']*1000:.2f}ms")
        else:
            print("âš ï¸ ë¹„êµí•  ëª¨ë¸ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € main.py ë˜ëŠ” improved_main.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main() 