import os
import cv2
import numpy as np
import mediapipe as mp
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    LSTM,
    Dense,
    Dropout,
    Input,
    Bidirectional,
    Conv1D,
    MaxPooling1D,
    GlobalAveragePooling1D,
    LayerNormalization,
    MultiHeadAttention,
    Add,
)
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from scipy.interpolate import interp1d
import sys

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

# ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
VIDEO_ROOT1 = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/ìˆ˜ì–´ ë°ì´í„°ì…‹/0001~3000(ì˜ìƒ)"
VIDEO_ROOT2 = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/ìˆ˜ì–´ ë°ì´í„°ì…‹/3001~6000(ì˜ìƒ)"
VIDEO_ROOT3 = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/ìˆ˜ì–´ ë°ì´í„°ì…‹/6001~8280(ì˜ìƒ)"
VIDEO_ROOT4 = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/ìˆ˜ì–´ ë°ì´í„°ì…‹/8381~9000(ì˜ìƒ)"
VIDEO_ROOT5 = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/ìˆ˜ì–´ ë°ì´í„°ì…‹/9001~9600(ì˜ìƒ)"

TARGET_SEQ_LENGTH = 30
AUGMENTATIONS_PER_VIDEO = 20  # ì¦ê°• íšŸìˆ˜ ì¦ê°€
DATA_CACHE_PATH = "fixed_preprocessed_data.npz"
MODEL_SAVE_PATH = "fixed_transformer_model.keras"
ACTIONS = ["í™”ì¬", "í™”ì¥ì‹¤", "í™”ìš”ì¼", "í™”ì•½", "í™”ìƒ", "None"]

label_dict = {
    "KETI_SL_0000000419.MOV": "í™”ì¬",
    "KETI_SL_0000000838.MTS": "í™”ì¬",
    "KETI_SL_0000001255.MTS": "í™”ì¬",
    "KETI_SL_0000001674.MTS": "í™”ì¬",
    "KETI_SL_0000002032.MOV": "í™”ì¬",
    "KETI_SL_0000002451.MP4": "í™”ì¬",
    "KETI_SL_0000002932.MOV": "í™”ì¬",
    "KETI_SL_0000003351.MTS": "í™”ì¬",
    "KETI_SL_0000003760.MOV": "í™”ì¬",
    "KETI_SL_0000004178.MTS": "í™”ì¬",
    "KETI_SL_0000004607.MOV": "í™”ì¬",
    "KETI_SL_0000005026.MTS": "í™”ì¬",
    "KETI_SL_0000005445.MOV": "í™”ì¬",
    "KETI_SL_0000005862.MTS": "í™”ì¬",
    "KETI_SL_0000006284.MOV": "í™”ì¬",
    "KETI_SL_0000006703.MTS": "í™”ì¬",
    "KETI_SL_0000007123.MOV": "í™”ì¬",
    "KETI_SL_0000007542.MTS": "í™”ì¬",
    "KETI_SL_0000007961.MOV": "í™”ì¬",
    "KETI_SL_0000008380.MTS": "í™”ì¬",
    "KETI_SL_0000000418.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000000837.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000001254.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000001673.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000002031.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000002450.MP4": "í™”ì¥ì‹¤",
    "KETI_SL_0000002931.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000003350.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000003759.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000004177.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000004606.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000005025.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000005444.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000005861.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000006283.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000006702.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000007122.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000007541.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000007960.MOV": "í™”ì¥ì‹¤",
    "KETI_SL_0000008379.MTS": "í™”ì¥ì‹¤",
    "KETI_SL_0000000417.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000000836.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000001253.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000001672.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000002030.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000002449.MP4": "í™”ìš”ì¼",
    "KETI_SL_0000002930.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000003349.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000003758.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000004176.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000004605.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000005024.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000005443.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000005860.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000006282.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000006701.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000007121.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000007540.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000007959.MOV": "í™”ìš”ì¼",
    "KETI_SL_0000008378.MTS": "í™”ìš”ì¼",
    "KETI_SL_0000000416.MOV": "í™”ì•½",
    "KETI_SL_0000000835.MTS": "í™”ì•½",
    "KETI_SL_0000001252.MTS": "í™”ì•½",
    "KETI_SL_0000001671.MTS": "í™”ì•½",
    "KETI_SL_0000002029.MOV": "í™”ì•½",
    "KETI_SL_0000002448.MP4": "í™”ì•½",
    "KETI_SL_0000002929.MOV": "í™”ì•½",
    "KETI_SL_0000003348.MTS": "í™”ì•½",
    "KETI_SL_0000003757.MOV": "í™”ì•½",
    "KETI_SL_0000004175.MTS": "í™”ì•½",
    "KETI_SL_0000004604.MOV": "í™”ì•½",
    "KETI_SL_0000005023.MTS": "í™”ì•½",
    "KETI_SL_0000005442.MOV": "í™”ì•½",
    "KETI_SL_0000005859.MTS": "í™”ì•½",
    "KETI_SL_0000006281.MOV": "í™”ì•½",
    "KETI_SL_0000006700.MTS": "í™”ì•½",
    "KETI_SL_0000007120.MOV": "í™”ì•½",
    "KETI_SL_0000007539.MTS": "í™”ì•½",
    "KETI_SL_0000007958.MOV": "í™”ì•½",
    "KETI_SL_0000008377.MTS": "í™”ì•½",
    "KETI_SL_0000000415.MOV": "í™”ìƒ",
    "KETI_SL_0000000834.MTS": "í™”ìƒ",
    "KETI_SL_0000001251.MTS": "í™”ìƒ",
    "KETI_SL_0000001670.MTS": "í™”ìƒ",
    "KETI_SL_0000002028.MOV": "í™”ìƒ",
    "KETI_SL_0000002447.MP4": "í™”ìƒ",
    "KETI_SL_0000002928.MOV": "í™”ìƒ",
    "KETI_SL_0000003347.MTS": "í™”ìƒ",
    "KETI_SL_0000003756.MOV": "í™”ìƒ",
    "KETI_SL_0000004174.MTS": "í™”ìƒ",
    "KETI_SL_0000004603.MOV": "í™”ìƒ",
    "KETI_SL_0000005022.MTS": "í™”ìƒ",
    "KETI_SL_0000005441.MOV": "í™”ìƒ",
    "KETI_SL_0000005858.MTS": "í™”ìƒ",
    "KETI_SL_0000006280.MOV": "í™”ìƒ",
    "KETI_SL_0000006699.MTS": "í™”ìƒ",
    "KETI_SL_0000007119.MOV": "í™”ìƒ",
    "KETI_SL_0000007538.MTS": "í™”ìƒ",
    "KETI_SL_0000007957.MOV": "í™”ìƒ",
    "KETI_SL_0000008376.MTS": "í™”ìƒ",
}


def get_video_root_and_path(filename):
    """íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•´ ì˜¬ë°”ë¥¸ VIDEO_ROOT ê²½ë¡œì™€ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        num_str = filename.split("_")[-1].split(".")[0]
        num = int(num_str)
    except Exception:
        return None

    if 1 <= num <= 3000:
        root = VIDEO_ROOT1
    elif 3001 <= num <= 6000:
        root = VIDEO_ROOT2
    elif 6001 <= num <= 8280:
        root = VIDEO_ROOT3
    elif 8381 <= num <= 9000:
        root = VIDEO_ROOT4
    elif 9001 <= num <= 9600:
        root = VIDEO_ROOT5
    else:
        return None

    base_name = "_".join(filename.split("_")[:-1]) + f"_{num_str}"
    for ext in [".MOV", ".MTS", ".AVI"]:
        candidate = os.path.join(root, base_name + ext)
        if os.path.exists(candidate):
            return candidate
    return None


def normalize_sequence_length(sequence, target_length=30):
    """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    current_length = len(sequence)

    if current_length == target_length:
        return sequence

    x_old = np.linspace(0, 1, current_length)
    x_new = np.linspace(0, 1, target_length)

    normalized_sequence = []
    for i in range(sequence.shape[1]):
        f = interp1d(
            x_old,
            sequence[:, i],
            kind="linear",
            bounds_error=False,
            fill_value="extrapolate",
        )
        normalized_sequence.append(f(x_new))

    return np.array(normalized_sequence).T


def extract_dynamic_features(sequence):
    """ì†ë„ì™€ ê°€ì†ë„ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    velocity = np.diff(sequence, axis=0, prepend=sequence[0:1])
    acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])
    dynamic_features = np.concatenate([sequence, velocity, acceleration], axis=1)
    return dynamic_features


def convert_to_relative_coordinates(landmarks_list):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ì–´ê¹¨ ì¤‘ì‹¬ ìƒëŒ€ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    relative_landmarks = []

    for frame in landmarks_list:
        if not frame["pose"]:
            relative_landmarks.append(frame)
            continue

        pose_landmarks = frame["pose"].landmark

        left_shoulder = pose_landmarks[11]
        right_shoulder = pose_landmarks[12]
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2

        shoulder_width = abs(right_shoulder.x - left_shoulder.x)
        if shoulder_width == 0:
            shoulder_width = 1.0

        new_frame = {}

        if frame["pose"]:
            relative_pose = []
            for landmark in pose_landmarks:
                rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                relative_pose.append([rel_x, rel_y, rel_z])
            new_frame["pose"] = relative_pose

        for hand_key in ["left_hand", "right_hand"]:
            if frame[hand_key]:
                relative_hand = []
                for landmark in frame[hand_key].landmark:
                    rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                    rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                    rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                    relative_hand.append([rel_x, rel_y, rel_z])
                new_frame[hand_key] = relative_hand
            else:
                new_frame[hand_key] = None

        relative_landmarks.append(new_frame)

    return relative_landmarks


def improved_preprocess_landmarks(landmarks_list):
    """ê°œì„ ëœ ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ í•¨ìˆ˜."""
    if not landmarks_list:
        return np.zeros((TARGET_SEQ_LENGTH, 675))

    relative_landmarks = convert_to_relative_coordinates(landmarks_list)

    processed_frames = []
    for frame in relative_landmarks:
        combined = []
        for key in ["pose", "left_hand", "right_hand"]:
            if frame[key]:
                if isinstance(frame[key], list):
                    combined.extend(frame[key])
                else:
                    combined.extend([[l.x, l.y, l.z] for l in frame[key].landmark])
            else:
                num_points = {"pose": 33, "left_hand": 21, "right_hand": 21}[key]
                combined.extend([[0, 0, 0]] * num_points)

        if combined:
            processed_frames.append(np.array(combined).flatten())
        else:
            processed_frames.append(np.zeros(75 * 3))

    if not processed_frames:
        return np.zeros((TARGET_SEQ_LENGTH, 675))

    sequence = np.array(processed_frames)

    if len(sequence) > 0:
        try:
            sequence = normalize_sequence_length(sequence, TARGET_SEQ_LENGTH)
            sequence = extract_dynamic_features(sequence)

            # ì •ê·œí™” ê°œì„ : ë” ê°•í•œ ì •ê·œí™”
            sequence = (sequence - np.mean(sequence)) / (np.std(sequence) + 1e-8)

            return sequence
        except Exception as e:
            print(f"âš ï¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return np.zeros((TARGET_SEQ_LENGTH, 675))

    return np.zeros((TARGET_SEQ_LENGTH, 675))


def create_simple_model(input_shape, num_classes):
    """ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    inputs = Input(shape=input_shape)

    # 1D CNN
    x = Conv1D(64, kernel_size=3, activation="relu", padding="same")(inputs)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    x = Conv1D(128, kernel_size=3, activation="relu", padding="same")(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    # LSTM
    x = Bidirectional(LSTM(64, return_sequences=True))(x)
    x = Dropout(0.3)(x)
    x = Bidirectional(LSTM(32))(x)
    x = Dropout(0.3)(x)

    # Dense layers
    x = Dense(64, activation="relu")(x)
    x = Dropout(0.3)(x)
    x = Dense(32, activation="relu")(x)
    x = Dropout(0.3)(x)

    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


def augment_sequence_improved(
    sequence, noise_level=0.05, scale_range=0.2, rotation_range=0.1
):
    """ê°œì„ ëœ ì‹œí€€ìŠ¤ ì¦ê°•."""
    augmented = sequence.copy()

    # ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.normal(0, noise_level, augmented.shape)
    augmented += noise

    # ìŠ¤ì¼€ì¼ë§
    scale_factor = np.random.uniform(1 - scale_range, 1 + scale_range)
    augmented *= scale_factor

    # ì‹œê°„ì¶•ì—ì„œì˜ íšŒì „ (ì‹œí”„íŠ¸)
    shift = np.random.randint(-3, 4)
    if shift > 0:
        augmented = np.roll(augmented, shift, axis=0)
    elif shift < 0:
        augmented = np.roll(augmented, shift, axis=0)

    return augmented


def extract_landmarks(video_path):
    """ë¹„ë””ì˜¤ì—ì„œ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    cap = cv2.VideoCapture(video_path)
    landmarks_list = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = holistic.process(rgb_frame)

        frame_data = {
            "pose": results.pose_landmarks,
            "left_hand": results.left_hand_landmarks,
            "right_hand": results.right_hand_landmarks,
        }
        landmarks_list.append(frame_data)

    cap.release()
    return landmarks_list


def get_action_index(label):
    """ACTIONSì— ì—†ëŠ” ë¼ë²¨ì€ -1 ë°˜í™˜"""
    try:
        return ACTIONS.index(label)
    except ValueError:
        print(f"âš ï¸ ACTIONSì— ì—†ëŠ” ë¼ë²¨: {label}")
        return -1


# None í´ë˜ìŠ¤ëª… ìë™ ì¶”ì¶œ
NONE_CLASS = ACTIONS[-1]


if __name__ == "__main__":
    print("ğŸ”§ í•™ìŠµ ë°ì´í„° ë¬¸ì œ í•´ê²° ë° ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")

    # ë°ì´í„° ì¶”ì¶œ
    X = []
    y = []

    for filename, label in tqdm(label_dict.items(), desc="ë°ì´í„° ì¶”ì¶œ"):
        if label not in ACTIONS:
            print(f"âš ï¸ ACTIONSì— ì—†ëŠ” ë¼ë²¨: {label}, íŒŒì¼: {filename} -> ê±´ë„ˆëœ€")
            continue
        actual_path = get_video_root_and_path(filename)
        if actual_path is None or not os.path.exists(actual_path):
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {actual_path}")
            continue

        landmarks = extract_landmarks(actual_path)
        if not landmarks:
            print(f"âš ï¸ ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {actual_path}")
            continue

        processed_sequence = improved_preprocess_landmarks(landmarks)

        if processed_sequence.shape != (TARGET_SEQ_LENGTH, 675):
            print(f"âš ï¸ ì‹œí€€ìŠ¤ í˜•íƒœ ì˜¤ë¥˜: {processed_sequence.shape}")
            continue

        # ì›ë³¸ ë°ì´í„° ì¶”ê°€
        X.append(processed_sequence)
        y.append(get_action_index(label))

        # ë” ë§ì€ ì¦ê°• ë°ì´í„° ì¶”ê°€
        for _ in range(AUGMENTATIONS_PER_VIDEO):
            try:
                augmented = augment_sequence_improved(processed_sequence)
                if augmented.shape == (TARGET_SEQ_LENGTH, 675):
                    X.append(augmented)
                    y.append(get_action_index(label))
            except Exception as e:
                print(f"âš ï¸ ì¦ê°• ì¤‘ ì˜¤ë¥˜: {e}")
                continue

    # None í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± (ë” ë‹¤ì–‘í•˜ê²Œ)
    print(f"\nâœ¨ '{NONE_CLASS}' í´ë˜ìŠ¤ ë°ì´í„° ëŒ€í­ ê°•í™” ì¤‘...")
    none_samples = []

    # ì „ëµ 1: ë” ë§ì€ ë¹„ë””ì˜¤ì—ì„œ, ë” ë‹¤ì–‘í•œ í”„ë ˆì„ì„ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
    source_videos = list(label_dict.keys())

    for filename in source_videos:
        file_id = filename.split(".")[0]
        actual_path = get_video_root_and_path(filename)

        if actual_path and os.path.exists(actual_path):
            landmarks = extract_landmarks(actual_path)
            if landmarks and len(landmarks) > 10:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì˜ìƒë§Œ ì‚¬ìš©
                # ì˜ìƒì˜ ì‹œì‘, 1/4, 1/2, 3/4, ë ì§€ì ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
                frame_indices = [
                    0,
                    len(landmarks) // 4,
                    len(landmarks) // 2,
                    3 * len(landmarks) // 4,
                    -1,
                ]

                for idx in frame_indices:
                    static_landmarks = [landmarks[idx]] * TARGET_SEQ_LENGTH
                    static_sequence = improved_preprocess_landmarks(static_landmarks)

                    if static_sequence.shape != (TARGET_SEQ_LENGTH, 675):
                        continue

                    # ì „ëµ 1-1: ì •ì  ì‹œí€€ìŠ¤ ìì²´ë¥¼ ì¶”ê°€
                    none_samples.append(static_sequence)

                    # ì „ëµ 1-2: ë¯¸ì„¸í•œ ì›€ì§ì„ ì¶”ê°€ (ë…¸ì´ì¦ˆ)
                    for _ in range(3):
                        augmented = augment_sequence_improved(
                            static_sequence, noise_level=0.01
                        )
                        if augmented.shape == (TARGET_SEQ_LENGTH, 675):
                            none_samples.append(augmented)

                # ì „ëµ 2: ëŠë¦° ì „í™˜ ë°ì´í„° ìƒì„± (ë‘ í”„ë ˆì„ ë³´ê°„)
                start_frame_lm = landmarks[0]
                middle_frame_lm = landmarks[len(landmarks) // 2]

                transition_landmarks = []
                for i in range(TARGET_SEQ_LENGTH):
                    alpha = i / (TARGET_SEQ_LENGTH - 1)
                    interp_frame = {}
                    for key in ["pose", "left_hand", "right_hand"]:
                        if start_frame_lm.get(key) and middle_frame_lm.get(key):
                            interp_lm = []
                            start_lms = start_frame_lm[key].landmark
                            mid_lms = middle_frame_lm[key].landmark
                            for j in range(len(start_lms)):
                                new_x = (
                                    start_lms[j].x * (1 - alpha) + mid_lms[j].x * alpha
                                )
                                new_y = (
                                    start_lms[j].y * (1 - alpha) + mid_lms[j].y * alpha
                                )
                                new_z = (
                                    start_lms[j].z * (1 - alpha) + mid_lms[j].z * alpha
                                )
                                # MediaPipe Landmark-like object
                                interp_lm.append(
                                    type(
                                        "obj",
                                        (object,),
                                        {"x": new_x, "y": new_y, "z": new_z},
                                    )
                                )
                            # MediaPipe LandmarkList-like object
                            interp_frame[key] = type(
                                "obj", (object,), {"landmark": interp_lm}
                            )
                        else:
                            interp_frame[key] = None
                    transition_landmarks.append(interp_frame)

                transition_sequence = improved_preprocess_landmarks(
                    transition_landmarks
                )
                if transition_sequence.shape == (TARGET_SEQ_LENGTH, 675):
                    none_samples.append(transition_sequence)

    # ì „ëµ 3: ì™„ì „í•œ ì •ì§€(zero) ë°ì´í„° ì¶”ê°€
    for _ in range(len(source_videos) * 5):  # ë‹¤ë¥¸ í´ë˜ìŠ¤ì™€ ìˆ˜ëŸ‰ ë§ì¶”ê¸°
        none_samples.append(np.zeros((TARGET_SEQ_LENGTH, 675)))

    # None í´ë˜ìŠ¤ ë°ì´í„° ì¶”ê°€
    none_label_index = get_action_index(NONE_CLASS)
    for sample in none_samples:
        X.append(sample)
        y.append(none_label_index)

    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° í†µê³„:")
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}")

    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸
    unique, counts = np.unique(y, return_counts=True)
    for class_idx, count in zip(unique, counts):
        if 0 <= class_idx < len(ACTIONS):
            print(f"í´ë˜ìŠ¤ {class_idx} ({ACTIONS[class_idx]}): {count}ê°œ")
        else:
            print(f"í´ë˜ìŠ¤ {class_idx} (Unknown): {count}ê°œ")

    X_padded = np.array(X)
    y_one_hot = to_categorical(y, num_classes=len(ACTIONS))

    # ë°ì´í„° ì €ì¥
    print(f"ğŸ’¾ ìˆ˜ì •ëœ ë°ì´í„° ì €ì¥: {DATA_CACHE_PATH}")
    np.savez(DATA_CACHE_PATH, X=X_padded, y=y_one_hot)

    # ëª¨ë¸ í•™ìŠµ
    print("\nğŸ‹ï¸â€â™€ï¸ ê°„ë‹¨í•œ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    X_train, X_test, y_train, y_test = train_test_split(
        X_padded, y_one_hot, test_size=0.2, random_state=42, stratify=y_one_hot
    )

    model = create_simple_model(
        input_shape=(X_padded.shape[1], X_padded.shape[2]), num_classes=len(ACTIONS)
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    print("\n--- ëª¨ë¸ êµ¬ì¡° ---")
    model.summary()

    # í•™ìŠµ
    checkpoint_dir = "checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, "model-epoch-{epoch:02d}.keras")

    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path, save_best_only=False, verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=8, min_lr=1e-6),
    ]

    history = model.fit(
        X_train,
        y_train,
        epochs=200,
        batch_size=8,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1,
    )

    print(f"ğŸ§  ìˆ˜ì •ëœ ëª¨ë¸ ì €ì¥: {MODEL_SAVE_PATH}")
    model.save(MODEL_SAVE_PATH)

    # í‰ê°€
    print("\n--- ëª¨ë¸ í‰ê°€ ---")
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"ğŸš€ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")

    # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
    y_pred_prob = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred_prob, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)

    print("\n--- í´ë˜ìŠ¤ë³„ ì •í™•ë„ ---")
    for i in range(len(ACTIONS)):
        class_mask = y_true_classes == i
        if np.sum(class_mask) > 0:
            class_accuracy = np.mean(
                y_pred_classes[class_mask] == y_true_classes[class_mask]
            )
            print(f"{ACTIONS[i]}: {class_accuracy:.4f}")

    holistic.close()
