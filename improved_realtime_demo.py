import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from collections import deque
from PIL import ImageFont, ImageDraw, Image
from scipy.interpolate import interp1d

# --- ì„¤ì •ê°’ ---
TARGET_SEQ_LENGTH = 30  # ì •ê·œí™”ëœ ì‹œí€€ìŠ¤ ê¸¸ì´
MODEL_SAVE_PATH = 'improved_transformer_model.keras'
ACTIONS = ["Fire", "Toilet", "None"]

# --- í•œê¸€ í°íŠ¸ ì„¤ì • ---
FONT_PATH = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"
try:
    font = ImageFont.truetype(FONT_PATH, 30)
except IOError:
    print(f"âŒ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FONT_PATH}")
    print("ë‹¤ë¥¸ ê²½ë¡œì˜ í•œê¸€ í°íŠ¸ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
    font = ImageFont.load_default()

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()
mp_drawing = mp.solutions.drawing_utils

# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
try:
    model = tf.keras.models.load_model(MODEL_SAVE_PATH)
    print(f"âœ… ê°œì„ ëœ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {MODEL_SAVE_PATH}")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    print("ë¨¼ì € improved_main.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ê°œì„ ëœ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•´ì£¼ì„¸ìš”.")
    exit()

# ëª¨ë¸ êµ¬ì¡° í™•ì¸
print(model.summary())

# ì…ë ¥ shape í™•ì¸
print("ëª¨ë¸ ì…ë ¥ shape:", model.input_shape)

def draw_korean_text(img, text, pos, font, color=(0, 255, 0)):
    """Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ OpenCV ì´ë¯¸ì§€ì— í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

def normalize_sequence_length(sequence, target_length=30):
    """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤ (ë‹¤ìš´ìƒ˜í”Œë§/ì—…ìƒ˜í”Œë§)."""
    current_length = len(sequence)
    
    if current_length == target_length:
        return sequence
    
    # ì‹œê°„ ì¶•ì„ ë”°ë¼ ë³´ê°„
    x_old = np.linspace(0, 1, current_length)
    x_new = np.linspace(0, 1, target_length)
    
    normalized_sequence = []
    for i in range(sequence.shape[1]):  # ê° íŠ¹ì§• ì°¨ì›ì— ëŒ€í•´
        f = interp1d(x_old, sequence[:, i], kind='linear', bounds_error=False, fill_value='extrapolate')
        normalized_sequence.append(f(x_new))
    
    return np.array(normalized_sequence).T

def extract_dynamic_features(sequence):
    """ì†ë„ì™€ ê°€ì†ë„ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ì†ë„ (ì´ì „ í”„ë ˆì„ ëŒ€ë¹„ ë³€í™”ëŸ‰)
    velocity = np.diff(sequence, axis=0, prepend=sequence[0:1])
    
    # ê°€ì†ë„ (ì†ë„ì˜ ë³€í™”ìœ¨)
    acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])
    
    # ì›ë³¸ + ì†ë„ + ê°€ì†ë„ ê²°í•©
    dynamic_features = np.concatenate([sequence, velocity, acceleration], axis=1)
    
    return dynamic_features

def convert_to_relative_coordinates(landmarks_list):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ì–´ê¹¨ ì¤‘ì‹¬ ìƒëŒ€ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    relative_landmarks = []
    
    for frame in landmarks_list:
        if not frame["pose"]:
            # í¬ì¦ˆ ëœë“œë§ˆí¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            relative_landmarks.append(frame)
            continue
        
        pose_landmarks = frame["pose"].landmark
        
        # ì–´ê¹¨ ì¤‘ì‹¬ì  ê³„ì‚° (ì™¼ìª½ ì–´ê¹¨ + ì˜¤ë¥¸ìª½ ì–´ê¹¨) / 2
        left_shoulder = pose_landmarks[11]  # ì™¼ìª½ ì–´ê¹¨
        right_shoulder = pose_landmarks[12]  # ì˜¤ë¥¸ìª½ ì–´ê¹¨
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2
        
        # ì–´ê¹¨ ë„ˆë¹„ ê³„ì‚° (ì •ê·œí™”ì— ì‚¬ìš©)
        shoulder_width = abs(right_shoulder.x - left_shoulder.x)
        if shoulder_width == 0:
            shoulder_width = 1.0  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # ìƒˆë¡œìš´ í”„ë ˆì„ ë°ì´í„° ìƒì„±
        new_frame = {}
        
        # í¬ì¦ˆ ëœë“œë§ˆí¬ ë³€í™˜
        if frame["pose"]:
            relative_pose = []
            for landmark in pose_landmarks:
                rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                relative_pose.append([rel_x, rel_y, rel_z])
            new_frame["pose"] = relative_pose
        
        # ì† ëœë“œë§ˆí¬ ë³€í™˜
        for hand_key in ["left_hand", "right_hand"]:
            if frame[hand_key]:
                relative_hand = []
                for landmark in frame[hand_key].landmark:
                    rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                    rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                    rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                    relative_hand.append([rel_x, rel_y, rel_z])
                new_frame[hand_key] = relative_hand
            else:
                new_frame[hand_key] = None
        
        relative_landmarks.append(new_frame)
    
    return relative_landmarks

def improved_preprocess_landmarks(landmarks_list):
    """ê°œì„ ëœ ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ í•¨ìˆ˜."""
    if not landmarks_list:
        # ë¹ˆ ëœë“œë§ˆí¬ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê¸°ë³¸ ì‹œí€€ìŠ¤ ë°˜í™˜
        return np.zeros((TARGET_SEQ_LENGTH, 675))  # 225*3 (ì›ë³¸+ì†ë„+ê°€ì†ë„)
    
    # 1. ìƒëŒ€ ì¢Œí‘œ ë³€í™˜
    relative_landmarks = convert_to_relative_coordinates(landmarks_list)
    
    # 2. ëœë“œë§ˆí¬ ê²°í•©
    processed_frames = []
    for frame in relative_landmarks:
        combined = []
        for key in ["pose", "left_hand", "right_hand"]:
            if frame[key]:
                if isinstance(frame[key], list):
                    combined.extend(frame[key])
                else:
                    combined.extend([[l.x, l.y, l.z] for l in frame[key].landmark])
            else:
                num_points = {"pose": 33, "left_hand": 21, "right_hand": 21}[key]
                combined.extend([[0,0,0]] * num_points)
        
        if combined:
            processed_frames.append(np.array(combined).flatten())
        else:
            # ê¸°ë³¸ í¬ê¸°ë¡œ 0 ì±„ìš°ê¸°
            processed_frames.append(np.zeros(75 * 3))
    
    if not processed_frames:
        # ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‹œí€€ìŠ¤ ë°˜í™˜
        return np.zeros((TARGET_SEQ_LENGTH, 675))
    
    sequence = np.array(processed_frames)
    
    # 3. ì‹œí€€ìŠ¤ ê¸¸ì´ ì •ê·œí™”
    if len(sequence) > 0:
        try:
            sequence = normalize_sequence_length(sequence, TARGET_SEQ_LENGTH)
            
            # 4. ë™ì  íŠ¹ì§• ì¶”ê°€
            sequence = extract_dynamic_features(sequence)
            
            # 5. ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            sequence = (sequence - np.min(sequence)) / (np.max(sequence) - np.min(sequence) + 1e-8)
            
            return sequence
        except Exception as e:
            print(f"âš ï¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‹œí€€ìŠ¤ ë°˜í™˜
            return np.zeros((TARGET_SEQ_LENGTH, 675))
    
    return np.zeros((TARGET_SEQ_LENGTH, 675))

# ì›¹ìº  ì‹¤í–‰
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

sequence = deque(maxlen=TARGET_SEQ_LENGTH)
current_prediction = ""
confidence = 0.0
pred_probs = np.zeros(len(ACTIONS))
pred_class_index = -1
prediction_counter = 0  # ì˜ˆì¸¡ íšŸìˆ˜ ì¶”ì 

print("ğŸš€ ê°œì„ ëœ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì‹œì‘!")
print("ğŸ“ ì‚¬ìš©ë²•: 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # ëœë“œë§ˆí¬ ì¶”ì¶œ
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(rgb_frame)

    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
    if results.face_landmarks:
        mp_drawing.draw_landmarks(frame, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,
                                  mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),
                                  mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

    # ëœë“œë§ˆí¬ ë°ì´í„° ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ì— ì¶”ê°€
    frame_data = {
        "pose": results.pose_landmarks,
        "left_hand": results.left_hand_landmarks,
        "right_hand": results.right_hand_landmarks,
        "face": results.face_landmarks
    }
    sequence.append(frame_data)

    # ì‹œí€€ìŠ¤ê°€ ì¶©ë¶„íˆ ìŒ“ì˜€ì„ ë•Œ ì˜ˆì¸¡ (ë§¤ 5í”„ë ˆì„ë§ˆë‹¤)
    if len(sequence) >= TARGET_SEQ_LENGTH and prediction_counter % 5 == 0:
        try:
            print(f"ğŸ” ì˜ˆì¸¡ ì‹œë„ #{prediction_counter}: ì‹œí€€ìŠ¤ ê¸¸ì´ {len(sequence)}")
            
            # ì‹œí€€ìŠ¤ ë‚´ìš© í™•ì¸ (ë””ë²„ê¹…ìš©)
            if prediction_counter <= 35:  # ì²˜ìŒ ëª‡ ë²ˆë§Œ í™•ì¸
                pose_count = sum(1 for frame in sequence if frame["pose"] is not None)
                left_hand_count = sum(1 for frame in sequence if frame["left_hand"] is not None)
                right_hand_count = sum(1 for frame in sequence if frame["right_hand"] is not None)
                print(f"ğŸ“Š ëœë“œë§ˆí¬ í†µê³„: í¬ì¦ˆ={pose_count}, ì™¼ì†={left_hand_count}, ì˜¤ë¥¸ì†={right_hand_count}")
            
            processed_sequence = improved_preprocess_landmarks(list(sequence))
            print(f"ğŸ“Š ì „ì²˜ë¦¬ëœ ì‹œí€€ìŠ¤ í˜•íƒœ: {processed_sequence.shape}")
            
            # ì‹œí€€ìŠ¤ ë°ì´í„° ë³€í™” í™•ì¸ (ë””ë²„ê¹…ìš©)
            if prediction_counter <= 35:
                seq_mean = np.mean(processed_sequence)
                seq_std = np.std(processed_sequence)
                print(f"ğŸ“ˆ ì‹œí€€ìŠ¤ í†µê³„: í‰ê· ={seq_mean:.6f}, í‘œì¤€í¸ì°¨={seq_std:.6f}")
            
            # ì‹œí€€ìŠ¤ í˜•íƒœ í™•ì¸
            if processed_sequence.shape == (TARGET_SEQ_LENGTH, 675):
                input_data = np.expand_dims(processed_sequence, axis=0)
                print(f"ğŸ¯ ëª¨ë¸ ì…ë ¥ í˜•íƒœ: {input_data.shape}")
                
                # ì…ë ¥ shape í™•ì¸
                print("ì‹¤ì œ ì…ë ¥ shape:", input_data.shape)
                
                # processed_sequence ì¼ë¶€ ê°’ ì¶œë ¥
                print("processed_sequence[0, :10]:", processed_sequence[0, :10])
                
                # model.predict() í˜¸ì¶œ ì „í›„ë¡œ pred_probs ì¶œë ¥
                print("ì˜ˆì¸¡ ì „ pred_probs:", pred_probs)
                pred_probs = model.predict(input_data, verbose=0)[0]
                print("ì˜ˆì¸¡ í›„ pred_probs:", pred_probs)
                
                pred_class_index = np.argmax(pred_probs)
                
                current_prediction = ACTIONS[pred_class_index]
                confidence = pred_probs[pred_class_index]
                
                print(f"âœ… ì˜ˆì¸¡ #{prediction_counter}: {current_prediction} (ì‹ ë¢°ë„: {confidence:.3f})")
                print(f"ğŸ“ˆ í™•ë¥  ë¶„í¬: Fire={pred_probs[0]:.3f}, Toilet={pred_probs[1]:.3f}, None={pred_probs[2]:.3f}")
            else:
                print(f"âš ï¸ ì‹œí€€ìŠ¤ í˜•íƒœ ì˜¤ë¥˜: {processed_sequence.shape}, ì˜ˆìƒ: ({TARGET_SEQ_LENGTH}, 675)")
                
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    prediction_counter += 1

    # --- ê²°ê³¼ ì‹œê°í™” ---
    
    # 1. ì˜ˆì¸¡ ê²°ê³¼ í…ìŠ¤íŠ¸
    display_label = {"Fire": "í™”ì¬", "Toilet": "í™”ì¥ì‹¤", "None": "ì—†ìŒ"}.get(current_prediction, "")
    if current_prediction == 'None' and confidence < 0.8:
        display_text = "..."
    else:
        display_text = f"ì˜ˆì¸¡: {display_label} (ì‹ ë¢°ë„: {confidence:.2f})"
        
    frame = draw_korean_text(frame, display_text, (20, 30), font, (0, 255, 0))

    # 2. í™•ë¥  ë§‰ëŒ€ê·¸ë˜í”„
    bar_start_x = frame.shape[1] - 300

    for i, prob in enumerate(pred_probs):
        action_korean = {"Fire": "í™”ì¬", "Toilet": "í™”ì¥ì‹¤", "None": "ì—†ìŒ"}.get(ACTIONS[i])
        y_pos = 50 + i * 40

        # ë§‰ëŒ€ê·¸ë˜í”„ ë°°ê²½
        cv2.rectangle(frame, (bar_start_x, y_pos), (bar_start_x + 250, y_pos + 30), (200, 200, 200), -1)
        
        # í™•ë¥  ë§‰ëŒ€
        bar_width = int(prob * 250)
        bar_color = (100, 100, 100) # ê¸°ë³¸ íšŒìƒ‰
        if i == pred_class_index:
            bar_color = (0, 255, 0) # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ëŠ” ë…¹ìƒ‰ìœ¼ë¡œ ê°•ì¡°

        cv2.rectangle(frame, (bar_start_x, y_pos), (bar_start_x + bar_width, y_pos + 30), bar_color, -1)
        
        # í…ìŠ¤íŠ¸
        text_on_bar = f"{action_korean}: {prob*100:.1f}%"
        frame = draw_korean_text(frame, text_on_bar, (bar_start_x + 5, y_pos), font, (0, 0, 0))

    # 3. ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
    debug_text = f"ì‹œí€€ìŠ¤ ê¸¸ì´: {len(sequence)}/{TARGET_SEQ_LENGTH}, ì˜ˆì¸¡ íšŸìˆ˜: {prediction_counter}"
    frame = draw_korean_text(frame, debug_text, (20, frame.shape[0] - 90), font, (255, 255, 255))

    # 4. ê°œì„  ì‚¬í•­ í‘œì‹œ
    info_text = "ê°œì„ ëœ ëª¨ë¸: Transformer + ë™ì  íŠ¹ì§•"
    frame = draw_korean_text(frame, info_text, (20, frame.shape[0] - 60), font, (255, 255, 0))

    # í™”ë©´ì— ì¶œë ¥
    cv2.imshow('ê°œì„ ëœ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ (Transformer)', frame)

    if cv2.waitKey(5) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
holistic.close()
print("ğŸ‰ ê°œì„ ëœ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì¢…ë£Œ") 