import os
import cv2
import numpy as np
import mediapipe as mp
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    LSTM, Dense, Dropout, Input, Bidirectional, 
    Conv1D, MaxPooling1D, GlobalAveragePooling1D,
    LayerNormalization, MultiHeadAttention, Add
)
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from scipy.interpolate import interp1d
import sys

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

# ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
VIDEO_ROOT = "/Volumes/Sub_Storage/ìˆ˜ì–´ ë°ì´í„°ì…‹/0001~3000(ì˜ìƒ)"
TARGET_SEQ_LENGTH = 30  # ì •ê·œí™”ëœ ì‹œí€€ìŠ¤ ê¸¸ì´ (ë¦¬í¬íŠ¸ ê¶Œì¥ì‚¬í•­)
AUGMENTATIONS_PER_VIDEO = 9
DATA_CACHE_PATH = 'improved_preprocessed_data.npz'
MODEL_SAVE_PATH = 'improved_transformer_model.keras'
ACTIONS = ["Fire", "Toilet", "None"]

label_dict = {
    # Fire
    "KETI_SL_0000000419.MOV": "Fire",
    "KETI_SL_0000000838.MTS": "Fire",
    "KETI_SL_0000001255.MTS": "Fire",
    "KETI_SL_0000001674.MTS": "Fire",
    "KETI_SL_0000002032.MOV": "Fire",
    "KETI_SL_0000002451.MP4": "Fire",
    "KETI_SL_0000002932.MOV": "Fire",
    # Toilet
    "KETI_SL_0000000418.MOV": "Toilet",
    "KETI_SL_0000000837.MTS": "Toilet",
    "KETI_SL_0000001254.MTS": "Toilet",
    "KETI_SL_0000001673.MTS": "Toilet",
    "KETI_SL_0000002031.MOV": "Toilet",
    "KETI_SL_0000002450.MP4": "Toilet",
    "KETI_SL_0000002931.MOV": "Toilet"
}

def normalize_sequence_length(sequence, target_length=30):
    """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤ (ë‹¤ìš´ìƒ˜í”Œë§/ì—…ìƒ˜í”Œë§)."""
    current_length = len(sequence)
    
    if current_length == target_length:
        return sequence
    
    # ì‹œê°„ ì¶•ì„ ë”°ë¼ ë³´ê°„
    x_old = np.linspace(0, 1, current_length)
    x_new = np.linspace(0, 1, target_length)
    
    normalized_sequence = []
    for i in range(sequence.shape[1]):  # ê° íŠ¹ì§• ì°¨ì›ì— ëŒ€í•´
        f = interp1d(x_old, sequence[:, i], kind='linear', bounds_error=False, fill_value='extrapolate')
        normalized_sequence.append(f(x_new))
    
    return np.array(normalized_sequence).T

def extract_dynamic_features(sequence):
    """ì†ë„ì™€ ê°€ì†ë„ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ì†ë„ (ì´ì „ í”„ë ˆì„ ëŒ€ë¹„ ë³€í™”ëŸ‰)
    velocity = np.diff(sequence, axis=0, prepend=sequence[0:1])
    
    # ê°€ì†ë„ (ì†ë„ì˜ ë³€í™”ìœ¨)
    acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])
    
    # ì›ë³¸ + ì†ë„ + ê°€ì†ë„ ê²°í•©
    dynamic_features = np.concatenate([sequence, velocity, acceleration], axis=1)
    
    return dynamic_features

def convert_to_relative_coordinates(landmarks_list):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ì–´ê¹¨ ì¤‘ì‹¬ ìƒëŒ€ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    relative_landmarks = []
    
    for frame in landmarks_list:
        if not frame["pose"]:
            # í¬ì¦ˆ ëœë“œë§ˆí¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            relative_landmarks.append(frame)
            continue
        
        pose_landmarks = frame["pose"].landmark
        
        # ì–´ê¹¨ ì¤‘ì‹¬ì  ê³„ì‚° (ì™¼ìª½ ì–´ê¹¨ + ì˜¤ë¥¸ìª½ ì–´ê¹¨) / 2
        left_shoulder = pose_landmarks[11]  # ì™¼ìª½ ì–´ê¹¨
        right_shoulder = pose_landmarks[12]  # ì˜¤ë¥¸ìª½ ì–´ê¹¨
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2
        
        # ì–´ê¹¨ ë„ˆë¹„ ê³„ì‚° (ì •ê·œí™”ì— ì‚¬ìš©)
        shoulder_width = abs(right_shoulder.x - left_shoulder.x)
        if shoulder_width == 0:
            shoulder_width = 1.0  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # ìƒˆë¡œìš´ í”„ë ˆì„ ë°ì´í„° ìƒì„±
        new_frame = {}
        
        # í¬ì¦ˆ ëœë“œë§ˆí¬ ë³€í™˜
        if frame["pose"]:
            relative_pose = []
            for landmark in pose_landmarks:
                rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                relative_pose.append([rel_x, rel_y, rel_z])
            new_frame["pose"] = relative_pose
        
        # ì† ëœë“œë§ˆí¬ ë³€í™˜
        for hand_key in ["left_hand", "right_hand"]:
            if frame[hand_key]:
                relative_hand = []
                for landmark in frame[hand_key].landmark:
                    rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                    rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                    rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                    relative_hand.append([rel_x, rel_y, rel_z])
                new_frame[hand_key] = relative_hand
            else:
                new_frame[hand_key] = None
        
        relative_landmarks.append(new_frame)
    
    return relative_landmarks

def improved_preprocess_landmarks(landmarks_list):
    """ê°œì„ ëœ ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ í•¨ìˆ˜."""
    if not landmarks_list:
        # ë¹ˆ ëœë“œë§ˆí¬ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê¸°ë³¸ ì‹œí€€ìŠ¤ ë°˜í™˜
        return np.zeros((TARGET_SEQ_LENGTH, 675))  # 225*3 (ì›ë³¸+ì†ë„+ê°€ì†ë„)
    
    # 1. ìƒëŒ€ ì¢Œí‘œ ë³€í™˜
    relative_landmarks = convert_to_relative_coordinates(landmarks_list)
    
    # 2. ëœë“œë§ˆí¬ ê²°í•©
    processed_frames = []
    for frame in relative_landmarks:
        combined = []
        for key in ["pose", "left_hand", "right_hand"]:
            if frame[key]:
                if isinstance(frame[key], list):
                    combined.extend(frame[key])
                else:
                    combined.extend([[l.x, l.y, l.z] for l in frame[key].landmark])
            else:
                num_points = {"pose": 33, "left_hand": 21, "right_hand": 21}[key]
                combined.extend([[0,0,0]] * num_points)
        
        if combined:
            processed_frames.append(np.array(combined).flatten())
        else:
            # ê¸°ë³¸ í¬ê¸°ë¡œ 0 ì±„ìš°ê¸°
            processed_frames.append(np.zeros(75 * 3))
    
    if not processed_frames:
        # ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‹œí€€ìŠ¤ ë°˜í™˜
        return np.zeros((TARGET_SEQ_LENGTH, 675))
    
    sequence = np.array(processed_frames)
    
    # 3. ì‹œí€€ìŠ¤ ê¸¸ì´ ì •ê·œí™”
    if len(sequence) > 0:
        try:
            sequence = normalize_sequence_length(sequence, TARGET_SEQ_LENGTH)
            
            # 4. ë™ì  íŠ¹ì§• ì¶”ê°€
            sequence = extract_dynamic_features(sequence)
            
            # 5. ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            sequence = (sequence - np.min(sequence)) / (np.max(sequence) - np.min(sequence) + 1e-8)
            
            return sequence
        except Exception as e:
            print(f"âš ï¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‹œí€€ìŠ¤ ë°˜í™˜
            return np.zeros((TARGET_SEQ_LENGTH, 675))
    
    return np.zeros((TARGET_SEQ_LENGTH, 675))

def create_transformer_model(input_shape, num_classes):
    """Transformer ê¸°ë°˜ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    inputs = Input(shape=input_shape)
    
    # 1D CNNìœ¼ë¡œ ê³µê°„ íŒ¨í„´ ì¶”ì¶œ
    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)
    x = MaxPooling1D(pool_size=2)(x)
    
    # Transformer Encoder Layer
    def transformer_encoder_block(x, num_heads=8, ff_dim=256, dropout=0.1):
        # Multi-Head Self-Attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=128, dropout=dropout
        )(x, x)
        x = Add()([x, attention_output])
        x = LayerNormalization(epsilon=1e-6)(x)
        
        # Feed Forward Network
        ffn_output = Dense(ff_dim, activation='relu')(x)
        ffn_output = Dense(128)(ffn_output)
        ffn_output = Dropout(dropout)(ffn_output)
        x = Add()([x, ffn_output])
        x = LayerNormalization(epsilon=1e-6)(x)
        
        return x
    
    # Transformer ë¸”ë¡ ì ìš©
    x = transformer_encoder_block(x)
    x = transformer_encoder_block(x)
    
    # BiLSTMìœ¼ë¡œ ì‹œê°„ì  íŒ¨í„´ ë¶„ì„
    x = Bidirectional(LSTM(64, return_sequences=True))(x)
    x = Bidirectional(LSTM(32))(x)
    
    # ë¶„ë¥˜ ë ˆì´ì–´
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

def create_hybrid_model(input_shape, num_classes):
    """CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    model = Sequential([
        # 1D CNNìœ¼ë¡œ ê³µê°„ íŒ¨í„´ ì¶”ì¶œ
        Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),
        MaxPooling1D(pool_size=2),
        Conv1D(128, kernel_size=3, activation='relu', padding='same'),
        MaxPooling1D(pool_size=2),
        
        # BiLSTMìœ¼ë¡œ ì‹œê°„ì  íŒ¨í„´ ë¶„ì„
        Bidirectional(LSTM(64, return_sequences=True)),
        Dropout(0.3),
        Bidirectional(LSTM(32)),
        Dropout(0.3),
        
        # ë¶„ë¥˜ ë ˆì´ì–´
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    return model

def augment_sequence_improved(sequence, noise_level=0.01, scale_range=0.1, rotation_range=0.05):
    """ê°œì„ ëœ ì‹œí€€ìŠ¤ ì¦ê°• í•¨ìˆ˜."""
    augmented_sequence = sequence.copy()
    
    # 1. ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.normal(0, noise_level, augmented_sequence.shape)
    augmented_sequence += noise
    
    # 2. í¬ê¸° ì¡°ì ˆ
    scale_factor = 1.0 + np.random.uniform(-scale_range, scale_range)
    augmented_sequence *= scale_factor
    
    # 3. ì‹œê°„ì  ë³€í˜• (í”„ë ˆì„ ìˆœì„œ ì•½ê°„ ë³€ê²½)
    if len(augmented_sequence) > 3:
        # ëœë¤í•˜ê²Œ ëª‡ ê°œ í”„ë ˆì„ì„ êµí™˜
        for _ in range(2):
            i, j = np.random.choice(len(augmented_sequence), 2, replace=False)
            augmented_sequence[i], augmented_sequence[j] = augmented_sequence[j].copy(), augmented_sequence[i].copy()
    
    return augmented_sequence

def extract_landmarks(video_path):
    """ë¹„ë””ì˜¤ì—ì„œ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    cap = cv2.VideoCapture(video_path)
    landmarks_list = []
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = holistic.process(rgb_frame)
        
        frame_data = {
            "pose": results.pose_landmarks,
            "left_hand": results.left_hand_landmarks,
            "right_hand": results.right_hand_landmarks,
        }
        landmarks_list.append(frame_data)
    
    cap.release()
    return landmarks_list

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    # --- 1. ë°ì´í„° ë¡œë”© ë˜ëŠ” ì¶”ì¶œ ---
    if os.path.exists(DATA_CACHE_PATH):
        print(f"ğŸ’¾ ìºì‹œì—ì„œ ê°œì„ ëœ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë”©: {DATA_CACHE_PATH}")
        cached_data = np.load(DATA_CACHE_PATH)
        X_padded = cached_data['X']
        y_one_hot = cached_data['y']
    else:
        print("âœ¨ ê°œì„ ëœ ë°ì´í„° ìºì‹œ ì—†ìŒ. ë¹„ë””ì˜¤ì—ì„œ ëœë“œë§ˆí¬ ì¶”ì¶œ ë° ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        X = []
        y = []

        for filename, label in tqdm(label_dict.items(), desc="ë°ì´í„° ì¶”ì¶œ"):
            file_id = filename.split(".")[0]
            actual_path = os.path.join(VIDEO_ROOT, f"{file_id}.avi")
            
            if not os.path.exists(actual_path):
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {actual_path}")
                continue
            
            landmarks = extract_landmarks(actual_path)
            if not landmarks:
                print(f"âš ï¸ ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {actual_path}")
                continue
                
            processed_sequence = improved_preprocess_landmarks(landmarks)
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ í™•ì¸
            if processed_sequence.shape != (TARGET_SEQ_LENGTH, 675):
                print(f"âš ï¸ ì‹œí€€ìŠ¤ í˜•íƒœ ì˜¤ë¥˜: {processed_sequence.shape}, ì˜ˆìƒ: ({TARGET_SEQ_LENGTH}, 675)")
                continue
            
            # ì›ë³¸ ë°ì´í„° ì¶”ê°€
            X.append(processed_sequence)
            y.append(ACTIONS.index(label))

            # ì¦ê°• ë°ì´í„° ì¶”ê°€
            for _ in range(AUGMENTATIONS_PER_VIDEO):
                try:
                    augmented = augment_sequence_improved(processed_sequence)
                    # ì¦ê°• í›„ì—ë„ í˜•íƒœ í™•ì¸
                    if augmented.shape == (TARGET_SEQ_LENGTH, 675):
                        X.append(augmented)
                        y.append(ACTIONS.index(label))
                except Exception as e:
                    print(f"âš ï¸ ì¦ê°• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

        # 'None' í´ë˜ìŠ¤ ë°ì´í„° ìƒì„±
        print("\nâœ¨ 'None' í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        base_video_path = os.path.join(VIDEO_ROOT, f"{list(label_dict.keys())[0].split('.')[0]}.avi")
        if os.path.exists(base_video_path):
            landmarks = extract_landmarks(base_video_path)
            if landmarks:
                first_frame_landmarks = improved_preprocess_landmarks([landmarks[0]])
                if first_frame_landmarks.shape == (TARGET_SEQ_LENGTH, 675):
                    still_sequence = np.tile(first_frame_landmarks, (TARGET_SEQ_LENGTH, 1))
                    
                    none_label_index = ACTIONS.index("None")
                    for _ in range(10 * len(ACTIONS)):
                        try:
                            augmented = augment_sequence_improved(still_sequence)
                            if augmented.shape == (TARGET_SEQ_LENGTH, 675):
                                X.append(augmented)
                                y.append(none_label_index)
                        except Exception as e:
                            print(f"âš ï¸ None í´ë˜ìŠ¤ ì¦ê°• ì¤‘ ì˜¤ë¥˜: {e}")
                            continue

        # ìµœì¢… ë°ì´í„° í˜•íƒœ í™•ì¸
        if not X:
            print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ë°ì´í„° ê²½ë¡œì™€ íŒŒì¼ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
        
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ì‹œí€€ìŠ¤ ìˆ˜: {len(X)}")
        print(f"ğŸ“Š ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ í˜•íƒœ: {X[0].shape}")
        
        # ëª¨ë“  ì‹œí€€ìŠ¤ê°€ ê°™ì€ í˜•íƒœì¸ì§€ í™•ì¸
        shapes = [seq.shape for seq in X]
        if len(set(shapes)) > 1:
            print(f"âš ï¸ ì‹œí€€ìŠ¤ í˜•íƒœê°€ ì¼ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {set(shapes)}")
            # ê°€ì¥ ì¼ë°˜ì ì¸ í˜•íƒœë¡œ í•„í„°ë§
            most_common_shape = max(set(shapes), key=shapes.count)
            X = [seq for seq in X if seq.shape == most_common_shape]
            print(f"ğŸ“Š í•„í„°ë§ í›„ ì‹œí€€ìŠ¤ ìˆ˜: {len(X)}")

        X_padded = np.array(X)
        y_one_hot = to_categorical(y, num_classes=len(ACTIONS))
        
        print(f"ğŸ’¾ ê°œì„ ëœ ë°ì´í„° ìºì‹œ ì €ì¥: {DATA_CACHE_PATH}")
        np.savez(DATA_CACHE_PATH, X=X_padded, y=y_one_hot)

    print(f"âœ… ê°œì„ ëœ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_padded.shape[0]}ê°œ ìƒ˜í”Œ, ì‹œí€€ìŠ¤ ê¸¸ì´: {X_padded.shape[1]}")

    # --- 2. ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ë¡œë”© ---
    if X_padded.shape[0] < 2:
        print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X_padded, y_one_hot, test_size=0.2, random_state=42, stratify=y_one_hot
        )

        if os.path.exists(MODEL_SAVE_PATH):
            print(f"ğŸ§  ì €ì¥ëœ ê°œì„  ëª¨ë¸ ë¡œë”©: {MODEL_SAVE_PATH}")
            model = tf.keras.models.load_model(MODEL_SAVE_PATH)
        else:
            print("ğŸ‹ï¸â€â™€ï¸ ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ. ê°œì„ ëœ Transformer ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ëª¨ë¸ ì„ íƒ (Transformer ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ)
            use_transformer = True  # True: Transformer, False: í•˜ì´ë¸Œë¦¬ë“œ
            
            if use_transformer:
                model = create_transformer_model(
                    input_shape=(X_padded.shape[1], X_padded.shape[2]), 
                    num_classes=len(ACTIONS)
                )
            else:
                model = create_hybrid_model(
                    input_shape=(X_padded.shape[1], X_padded.shape[2]), 
                    num_classes=len(ACTIONS)
                )

            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )

            print("\n--- ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
            model.summary()

            # Early stoppingê³¼ ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
            callbacks = [
                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)
            ]

            history = model.fit(
                X_train, y_train, 
                epochs=100, 
                batch_size=16, 
                validation_data=(X_test, y_test),
                callbacks=callbacks
            )
            
            print(f"ğŸ§  í•™ìŠµëœ ê°œì„  ëª¨ë¸ ì €ì¥: {MODEL_SAVE_PATH}")
            model.save(MODEL_SAVE_PATH)

        # --- 3. ëª¨ë¸ í‰ê°€ ---
        print("\n--- ê°œì„ ëœ ëª¨ë¸ í‰ê°€ ---")
        loss, accuracy = model.evaluate(X_test, y_test)
        print(f"ğŸš€ ê°œì„ ëœ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")

        # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
        print("\n--- Test Sample Predictions ---")
        y_pred_prob = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred_prob, axis=1)
        y_true_classes = np.argmax(y_test, axis=1)

        correct_predictions = 0
        for i, (pred_class, true_class) in enumerate(zip(y_pred_classes, y_true_classes)):
            pred_label = ACTIONS[pred_class]
            actual_label = ACTIONS[true_class]
            result = "âœ…" if pred_class == true_class else "âŒ"
            confidence = y_pred_prob[i][pred_class]
            print(f"Sample {i+1}: Prediction={pred_label} (Confidence: {confidence:.2f}), Actual={actual_label} {result}")
            if pred_class == true_class:
                correct_predictions += 1
        
        print(f"\nğŸ“Š ì •í™•ë„ ìš”ì•½: {correct_predictions}/{len(y_test)} ({correct_predictions/len(y_test)*100:.2f}%)")

    holistic.close() 