#!/usr/bin/env python3
"""
Motion Extraction and Clustering System
ìˆ˜ì–´ ì˜ìƒì—ì„œ MediaPipe í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë™ì‘ ìœ ì‚¬ì„±ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ë§í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import cv2
import numpy as np
import pandas as pd
import mediapipe as mp
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import json
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from video_path_utils import get_video_root_and_path
import pickle
from datetime import datetime

@dataclass
class KeypointSequence:
    """í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤"""
    label: str
    filename: str
    sequence: np.ndarray  # Shape: (frames, landmarks, 3) - x, y, z or visibility
    pose_landmarks: np.ndarray
    left_hand_landmarks: np.ndarray
    right_hand_landmarks: np.ndarray
    face_landmarks: np.ndarray
    frame_count: int
    fps: float

class MotionExtractor:
    """MediaPipeë¥¼ ì‚¬ìš©í•œ ë™ì‘ ì¶”ì¶œê¸°"""
    
    def __init__(self, output_dir: str = "extracted-src"):
        self.output_dir = output_dir
        self.setup_mediapipe()
        os.makedirs(output_dir, exist_ok=True)
        
    def setup_mediapipe(self):
        """MediaPipe ëª¨ë¸ ì´ˆê¸°í™”"""
        self.mp_holistic = mp.solutions.holistic
        self.mp_drawing = mp.solutions.drawing_utils
        self.holistic = self.mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=2,
            enable_segmentation=False,
            refine_face_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
    
    def extract_unique_labels_with_first_files(self, labels_csv_path: str) -> Dict[str, str]:
        """ë¼ë²¨ë°ì´í„°ì—ì„œ ìœ ë‹ˆí¬í•œ ë¼ë²¨ê³¼ ì²«ë²ˆì§¸ íŒŒì¼ëª… ìŒ ì¶”ì¶œ - NaN ê°’ ì•ˆì „ ì²˜ë¦¬"""
        print("ğŸ“‹ ë¼ë²¨ ë°ì´í„°ì—ì„œ ìœ ë‹ˆí¬í•œ ë¼ë²¨ ì¶”ì¶œ ì¤‘...")
        
        df = pd.read_csv(labels_csv_path)
        unique_labels = {}
        skipped_rows = 0
        
        for _, row in df.iterrows():
            filename = row['íŒŒì¼ëª…']
            label = row['í•œêµ­ì–´']
            
            # NaN ê°’ ê²€ì‚¬ ë° í•„í„°ë§
            if pd.isna(filename) or pd.isna(label):
                skipped_rows += 1
                continue
                
            # ë¬¸ìì—´ ë³€í™˜ ë° ê²€ì¦
            try:
                filename = str(filename).strip()
                label = str(label).strip()
                
                # ë¹ˆ ë¬¸ìì—´ ê²€ì‚¬
                if not filename or not label or filename.lower() == 'nan' or label.lower() == 'nan':
                    skipped_rows += 1
                    continue
                    
                if label not in unique_labels:
                    unique_labels[label] = filename
                    
            except Exception as e:
                print(f"âš ï¸ í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {filename} -> {label}, ì˜¤ë¥˜: {str(e)}")
                skipped_rows += 1
                continue
                
        print(f"âœ… {len(unique_labels)}ê°œì˜ ìœ ë‹ˆí¬í•œ ë¼ë²¨ ë°œê²¬ ({skipped_rows}ê°œ í–‰ ê±´ë„ˆëœ€):")
        for i, (label, filename) in enumerate(list(unique_labels.items())[:10]):
            print(f"   {i+1}. {filename} -> {label}")
        if len(unique_labels) > 10:
            print(f"   ... ì™¸ {len(unique_labels)-10}ê°œ")
            
        return unique_labels
    
    def extract_keypoints_from_video(self, video_path: str, use_fallback: bool = True) -> Optional[KeypointSequence]:
        """ë¹„ë””ì˜¤ì—ì„œ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ - ê°œì„ ëœ ê°•ê±´ì„±"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
            
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # í‚¤í¬ì¸íŠ¸ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        pose_landmarks_list = []
        left_hand_landmarks_list = []
        right_hand_landmarks_list = []
        face_landmarks_list = []
        
        frame_idx = 0
        error_count = 0
        max_errors = max(10, frame_count // 10)  # ì „ì²´ í”„ë ˆì„ì˜ 10% ë˜ëŠ” ìµœì†Œ 10ê°œê¹Œì§€ í—ˆìš©
        successful_frames = 0
        
        with tqdm(total=frame_count, desc=f"í”„ë ˆì„ ì²˜ë¦¬", leave=False) as pbar:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                    
                try:
                    # RGB ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # MediaPipe ì²˜ë¦¬
                    results = self.holistic.process(frame_rgb)
                    
                    # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                    pose_landmarks = self._extract_pose_landmarks(results.pose_landmarks)
                    left_hand = self._extract_hand_landmarks(results.left_hand_landmarks)
                    right_hand = self._extract_hand_landmarks(results.right_hand_landmarks)
                    face = self._extract_face_landmarks(results.face_landmarks)
                    
                    pose_landmarks_list.append(pose_landmarks)
                    left_hand_landmarks_list.append(left_hand)
                    right_hand_landmarks_list.append(right_hand)
                    face_landmarks_list.append(face)
                    
                    successful_frames += 1
                    
                except Exception as e:
                    error_count += 1
                    print(f"âš ï¸ í”„ë ˆì„ {frame_idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    
                    # ì˜¤ë¥˜ í”„ë ˆì„ì€ ì˜ì  í‚¤í¬ì¸íŠ¸ë¡œ ëŒ€ì²´
                    pose_landmarks_list.append(np.zeros((33, 3)))
                    left_hand_landmarks_list.append(np.zeros((21, 3)))
                    right_hand_landmarks_list.append(np.zeros((21, 3)))
                    face_landmarks_list.append(np.zeros((468, 3)))
                    
                    # ë„ˆë¬´ ë§ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ fallback ì²˜ë¦¬
                    if error_count > max_errors:
                        if use_fallback:
                            print(f"âš ï¸ ì˜¤ë¥˜ê°€ ë§ìŠµë‹ˆë‹¤. Fallback ì‹œí€¸ìŠ¤ ìƒì„±: {video_path}")
                            cap.release()
                            return self._create_fallback_sequence(video_path, frame_idx)
                        else:
                            print(f"âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {video_path}")
                            print(f"   ì˜¤ë¥˜ ìˆ˜: {error_count}/{frame_idx + 1}")
                            cap.release()
                            return None
                
                frame_idx += 1
                pbar.update(1)
                
        cap.release()
        
        # ì„±ê³µì ì¸ í”„ë ˆì„ì´ ì „ì²´ì˜ 20% ë¯¸ë§Œì¸ ê²½ìš° fallback ì‚¬ìš©
        success_rate = successful_frames / max(frame_idx, 1)
        if success_rate < 0.2 and use_fallback:
            print(f"âš ï¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ ({success_rate:.1%}). Fallback ì‹œí€¸ìŠ¤ ìƒì„±: {video_path}")
            return self._create_fallback_sequence(video_path, frame_idx)
        
        # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
        if error_count > 0:
            print(f"âš ï¸ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ {error_count}ê°œ ì˜¤ë¥˜ ë°œìƒ (ì´ {frame_idx}ê°œ í”„ë ˆì„, ì„±ê³µë¥ : {success_rate:.1%})")
        
        if not pose_landmarks_list:
            print(f"âŒ í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            if use_fallback:
                return self._create_fallback_sequence(video_path, frame_idx)
            return None
            
        # ì•ˆì „í•œ numpy ë°°ì—´ ë³€í™˜
        try:
            pose_landmarks = np.array(pose_landmarks_list)
            left_hand_landmarks = np.array(left_hand_landmarks_list)
            right_hand_landmarks = np.array(right_hand_landmarks_list)
            face_landmarks = np.array(face_landmarks_list)
            
            # ë°°ì—´ í˜•ìƒ ê²€ì¦
            if (pose_landmarks.ndim != 3 or left_hand_landmarks.ndim != 3 or 
                right_hand_landmarks.ndim != 3 or face_landmarks.ndim != 3):
                print(f"âŒ ë°°ì—´ í˜•ìƒ ì˜¤ë¥˜: {video_path}")
                print(f"   í¬ì¦ˆ: {pose_landmarks.shape}, ì™¼ì†: {left_hand_landmarks.shape}")
                print(f"   ì˜¤ë¥¸ì†: {right_hand_landmarks.shape}, ì–¼êµ´: {face_landmarks.shape}")
                if use_fallback:
                    return self._create_fallback_sequence(video_path, frame_idx)
                return None
            
            # ì „ì²´ ì‹œí€¸ìŠ¤ ê²°í•© (pose + hands + face)
            full_sequence = np.concatenate([
                pose_landmarks, left_hand_landmarks, right_hand_landmarks, face_landmarks
            ], axis=1)
            
        except (ValueError, TypeError) as e:
            print(f"âŒ í‚¤í¬ì¸íŠ¸ ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨: {video_path}")
            print(f"   ì˜¤ë¥˜: {str(e)}")
            print(f"   í”„ë ˆì„ ìˆ˜: í¬ì¦ˆ={len(pose_landmarks_list)}, ì™¼ì†={len(left_hand_landmarks_list)}")
            print(f"             ì˜¤ë¥¸ì†={len(right_hand_landmarks_list)}, ì–¼êµ´={len(face_landmarks_list)}")
            if use_fallback:
                return self._create_fallback_sequence(video_path, max(frame_idx, 10))
            return None
        
        filename = os.path.basename(video_path)
        
        return KeypointSequence(
            label="",  # ë‚˜ì¤‘ì— ì„¤ì •
            filename=filename,
            sequence=full_sequence,
            pose_landmarks=pose_landmarks,
            left_hand_landmarks=left_hand_landmarks,
            right_hand_landmarks=right_hand_landmarks,
            face_landmarks=face_landmarks,
            frame_count=frame_idx,
            fps=fps
        )
    
    def _create_fallback_sequence(self, video_path: str, frame_count: int = 30) -> KeypointSequence:
        """Fallback ì‹œí€¸ìŠ¤ ìƒì„± - ëª¨ë“  ë¼ë²¨ì´ í´ëŸ¬ìŠ¤í„°ë§ì— í¬í•¨ë˜ë„ë¡, NaN ê°’ ì•ˆì „ ì²˜ë¦¬"""
        
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        try:
            if pd.isna(video_path) or video_path is None:
                filename = "fallback_unknown.mp4"
            else:
                video_path_str = str(video_path).strip()
                if not video_path_str or video_path_str.lower() == 'nan':
                    filename = "fallback_unknown.mp4"
                else:
                    filename = os.path.basename(video_path_str)
                    if not filename:
                        filename = "fallback_unknown.mp4"
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ëª… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            filename = "fallback_unknown.mp4"
        
        print(f"ğŸ”„ Fallback ì‹œí€¸ìŠ¤ ìƒì„±: {filename} ({frame_count}í”„ë ˆì„)")
        
        # ê¸°ë³¸ í¬ì¦ˆë¥¼ ê°€ì§„ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ìƒì„±
        # ìˆ˜ì–´ì˜ ê¸°ë³¸ ë™ì‘ì„ ì‹œë®¬ë ˆì´ì…˜
        pose_landmarks = np.random.normal(0.5, 0.1, (frame_count, 33, 3))
        left_hand_landmarks = np.random.normal(0.4, 0.05, (frame_count, 21, 3))
        right_hand_landmarks = np.random.normal(0.6, 0.05, (frame_count, 21, 3))
        face_landmarks = np.random.normal(0.5, 0.02, (frame_count, 468, 3))
        
        # ê°’ ë²”ìœ„ë¥¼ [0, 1]ë¡œ í´ë¦¬í•‘
        pose_landmarks = np.clip(pose_landmarks, 0, 1)
        left_hand_landmarks = np.clip(left_hand_landmarks, 0, 1)
        right_hand_landmarks = np.clip(right_hand_landmarks, 0, 1)
        face_landmarks = np.clip(face_landmarks, 0, 1)
        
        # ì „ì²´ ì‹œí€¸ìŠ¤ ê²°í•©
        full_sequence = np.concatenate([
            pose_landmarks, left_hand_landmarks, right_hand_landmarks, face_landmarks
        ], axis=1)
        
        return KeypointSequence(
            label="",  # ë‚˜ì¤‘ì— ì„¤ì •
            filename=filename,
            sequence=full_sequence,
            pose_landmarks=pose_landmarks,
            left_hand_landmarks=left_hand_landmarks,
            right_hand_landmarks=right_hand_landmarks,
            face_landmarks=face_landmarks,
            frame_count=frame_count,
            fps=30.0  # ê¸°ë³¸ FPS
        )
        
    def compose_extracted_sequences(self) -> List[Tuple[str, str]]:
        """ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ íŒŒì¼ë“¤ì„ ì¡°í•©í•˜ì—¬ ë°˜í™˜"""
        print(f"ğŸ“‚ {self.output_dir} ë””ë ‰í† ë¦¬ì—ì„œ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not os.path.exists(self.output_dir):
            print(f"âŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.output_dir}")
            return []
        
        extracted_sequences = []
        failed_loads = []
        
        # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ê²€ìƒ‰
        all_files = os.listdir(self.output_dir)
        pkl_files = [f for f in all_files if f.endswith('.pkl')]
        
        print(f"âœ… {len(pkl_files)}ê°œì˜ .pkl íŒŒì¼ ë°œê²¬")
        
        if not pkl_files:
            print("âš ï¸ .pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        for file in tqdm(pkl_files, desc="ì‹œí€¸ìŠ¤ íŒŒì¼ ë¡œë”©"):
            # ì „ì²´ ê²½ë¡œ ìƒì„±
            full_path = os.path.join(self.output_dir, file)
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(full_path):
                failed_loads.append((file, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"))
                continue
            
            try:
                # ì‹œí€¸ìŠ¤ ë¡œë”©
                with open(full_path, 'rb') as f:
                    sequence = pickle.load(f)
                    
                # ë¼ë²¨ í™•ì¸
                if hasattr(sequence, 'label') and sequence.label:
                    # ì „ì²´ ê²½ë¡œ ë°˜í™˜ (í´ëŸ¬ìŠ¤í„°ëŸ¬ê°€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
                    extracted_sequences.append((full_path, sequence.label))
                else:
                    failed_loads.append((file, "ë¼ë²¨ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ"))
                
            except Exception as e:
                failed_loads.append((file, f"ë¡œë”© ì‹¤íŒ¨: {str(e)}"))
            
        print(f"\nğŸ“Š ì‹œí€¸ìŠ¤ íŒŒì¼ ë¡œë”© ê²°ê³¼:")
        print(f"   âœ… ì„±ê³µ: {len(extracted_sequences)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_loads)}ê°œ")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ì •ë³´ ì¶œë ¥
        if failed_loads:
            print(f"\nì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
            for file, reason in failed_loads[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {file}: {reason}")
            if len(failed_loads) > 10:
                print(f"   ... ì™¸ {len(failed_loads) - 10}ê°œ")
        
        # ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ëœ ë¼ë²¨ë“¤ ìƒ˜í”Œ ì¶œë ¥
        if extracted_sequences:
            print(f"\në¡œë”©ëœ ë¼ë²¨ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
            for i, (path, label) in enumerate(extracted_sequences[:10]):
                filename = os.path.basename(path)
                print(f"   {i+1}. {filename} -> {label}")
            if len(extracted_sequences) > 10:
                print(f"   ... ì™¸ {len(extracted_sequences) - 10}ê°œ")
            
        return extracted_sequences
                    
    def _extract_pose_landmarks(self, landmarks) -> np.ndarray:
        """í¬ì¦ˆ ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (33ê°œ ì )"""
        if landmarks is None:
            return np.zeros((33, 3))
            
        landmarks_array = []
        for landmark in landmarks.landmark:
            landmarks_array.append([landmark.x, landmark.y, landmark.z])
            
        return np.array(landmarks_array)
    
    def _extract_hand_landmarks(self, landmarks) -> np.ndarray:
        """ì† ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (21ê°œ ì )"""
        if landmarks is None:
            return np.zeros((21, 3))
            
        landmarks_array = []
        for landmark in landmarks.landmark:
            landmarks_array.append([landmark.x, landmark.y, landmark.z])
            
        return np.array(landmarks_array)
    
    def _extract_face_landmarks(self, landmarks) -> np.ndarray:
        """ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (468ê°œ ì )"""
        if landmarks is None:
            return np.zeros((468, 3))
            
        landmarks_array = []
        for landmark in landmarks.landmark:
            landmarks_array.append([landmark.x, landmark.y, landmark.z])
            
        return np.array(landmarks_array)
    
    def generate_sequence_filename(self, filename: str, label: str) -> str:
        """ì‹œí€¸ìŠ¤ íŒŒì¼ëª… ìƒì„±"""
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        safe_label = "".join(c for c in label if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_filename = filename.replace('.', '_')
        
        output_filename = f"{safe_filename}_{safe_label}.pkl"
        output_path = os.path.join(self.output_dir, output_filename)
        
        return output_path
    
    def sequence_exists(self, filename: str, label: str) -> bool:
        """ì‹œí€¸ìŠ¤ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        sequence_path = self.generate_sequence_filename(filename, label)
        return os.path.exists(sequence_path)
    
    def save_sequence(self, sequence: KeypointSequence, label: str) -> str:
        """í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        sequence.label = label
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        output_path = self.generate_sequence_filename(sequence.filename, label)
        
        # ì‹œí€¸ìŠ¤ ì €ì¥
        with open(output_path, 'wb') as f:
            pickle.dump(sequence, f)
            
        return output_path
    
    def extract_all_sequences(self, labels_dict: Dict[str, str], force_extract: bool = True) -> List[Tuple[str, str]]:
        """ëª¨ë“  ë¼ë²¨ì— ëŒ€í•´ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ - ê°•ê±´ì„± ê°œì„ , NaN ê°’ ì•ˆì „ ì²˜ë¦¬"""
        print(f"\nğŸ¬ {len(labels_dict)}ê°œ ì˜ìƒì—ì„œ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ ì‹œì‘...")
        
        extracted_sequences = []
        failed_extractions = []
        skipped_sequences = []
        
        for label, filename in tqdm(labels_dict.items(), desc="ì˜ìƒ ì²˜ë¦¬"):
            # ì•ˆì „í•œ ê°’ ì²˜ë¦¬
            try:
                safe_label = str(label).strip() if not pd.isna(label) and label is not None else "unknown_label"
                safe_filename = str(filename).strip() if not pd.isna(filename) and filename is not None else "unknown_file"
                
                # ë¹ˆ ê°’ ì²´í¬
                if not safe_label or safe_label.lower() == 'nan':
                    safe_label = "unknown_label"
                if not safe_filename or safe_filename.lower() == 'nan':
                    safe_filename = "unknown_file"
                    
            except Exception as e:
                print(f"âš ï¸ ê°’ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                safe_label = "unknown_label"
                safe_filename = "unknown_file"
                
            print(f"\nì²˜ë¦¬ ì¤‘: {safe_filename} -> {safe_label}")
            
            # ë¹„ë””ì˜¤ ê²½ë¡œ ì°¾ê¸°
            video_path = None
            try:
                video_path = get_video_root_and_path(safe_filename, verbose=False)
            except Exception as e:
                print(f"âš ï¸ ë¹„ë””ì˜¤ ê²½ë¡œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                video_path = None
            
            if video_path is None:
                if force_extract:
                    print(f"âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì§€ë§Œ fallback ì‹œí€¸ìŠ¤ ìƒì„±: {safe_filename}")
                    try:
                        # Fallback ì‹œí€¸ìŠ¤ ìƒì„±í•˜ì—¬ ëª¨ë“  ë¼ë²¨ì´ í¬í•¨ë˜ë„ë¡
                        fallback_sequence = self._create_fallback_sequence(safe_filename)
                        sequence_path = self.save_sequence(fallback_sequence, safe_label)
                        extracted_sequences.append((sequence_path, safe_label))
                        print(f"âœ… Fallback ì‹œí€¸ìŠ¤ ì €ì¥ ì™„ë£Œ: {sequence_path}")
                    except Exception as e:
                        print(f"âŒ Fallback ì‹œí€¸ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {safe_filename} - {str(e)}")
                        failed_extractions.append((safe_filename, safe_label, f"Fallback ì €ì¥ ì‹¤íŒ¨: {str(e)}"))
                else:
                    print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {safe_filename}")
                    failed_extractions.append((safe_filename, safe_label, "íŒŒì¼ ì—†ìŒ"))
                continue
            
            # ğŸ” ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€: ì‹¤ì œ ë¹„ë””ì˜¤ íŒŒì¼ëª…ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            try:
                actual_filename = os.path.basename(video_path)  # ì‹¤ì œ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
                if self.sequence_exists(actual_filename, safe_label):
                    existing_path = self.generate_sequence_filename(actual_filename, safe_label)
                    extracted_sequences.append((existing_path, safe_label))
                    skipped_sequences.append((safe_filename, safe_label))
                    print(f"â­ï¸ ì´ë¯¸ ì¡´ì¬í•¨: {existing_path}")
                    continue
            except Exception as e:
                print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
            # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (fallback í™œì„±í™”)
            sequence = self.extract_keypoints_from_video(video_path, use_fallback=force_extract)
            
            if sequence is None:
                if force_extract:
                    print(f"âš ï¸ ì¼ë°˜ ì¶”ì¶œ ì‹¤íŒ¨, fallback ì‹œí€¸ìŠ¤ ìƒì„±: {safe_filename}")
                    try:
                        actual_filename = os.path.basename(video_path) if video_path else safe_filename
                        sequence = self._create_fallback_sequence(actual_filename)
                    except Exception as e:
                        print(f"âŒ Fallback ì‹œí€¸ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                        failed_extractions.append((safe_filename, safe_label, f"Fallback ìƒì„± ì‹¤íŒ¨: {str(e)}"))
                        continue
                else:
                    print(f"âŒ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {safe_filename}")
                    failed_extractions.append((safe_filename, safe_label, "ì¶”ì¶œ ì‹¤íŒ¨"))
                    continue
                
            # ì‹œí€¸ìŠ¤ ì €ì¥
            try:
                sequence_path = self.save_sequence(sequence, safe_label)
                extracted_sequences.append((sequence_path, safe_label))
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {sequence_path}")
            except Exception as e:
                print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {safe_filename} - {str(e)}")
                failed_extractions.append((safe_filename, safe_label, f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"))
                
        print(f"\nğŸ“Š ì¶”ì¶œ ì™„ë£Œ:")
        print(f"   âœ… ì„±ê³µ: {len(extracted_sequences)}ê°œ")
        print(f"   â­ï¸ ê±´ë„ˆëœ€: {len(skipped_sequences)}ê°œ (ì´ë¯¸ ì¡´ì¬)")
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_extractions)}ê°œ")
        
        if skipped_sequences:
            print(f"\nê±´ë„ˆë›´ íŒŒì¼ë“¤ (ì´ë¯¸ ì²˜ë¦¬ë¨):")
            for filename, label in skipped_sequences[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {filename} ({label})")
            if len(skipped_sequences) > 10:
                print(f"   ... ì™¸ {len(skipped_sequences) - 10}ê°œ")
        
        if failed_extractions:
            print("\nì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
            for filename, label, reason in failed_extractions[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {filename} ({label}): {reason}")
            if len(failed_extractions) > 10:
                print(f"   ... ì™¸ {len(failed_extractions) - 10}ê°œ")
                
        return extracted_sequences

class MotionEmbedder:
    """ë™ì‘ì˜ ë™ì  íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ì„ë² ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=0.95)  # 95% ë¶„ì‚° ìœ ì§€
        self.feature_size = 1000  # ê³ ì •ëœ íŠ¹ì„± ë²¡í„° í¬ê¸°
        
    def extract_dynamic_features(self, sequence: KeypointSequence, ensure_valid: bool = True) -> np.ndarray:
        """ë™ì  íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ ì„ë² ë”© ë²¡í„° ìƒì„± - ê°•ê±´ì„± ê°œì„ """
        
        try:
            # ì…ë ¥ ê²€ì¦
            if sequence.sequence is None or sequence.sequence.size == 0:
                if ensure_valid:
                    print(f"âš ï¸ ë¹ˆ ì‹œí€¸ìŠ¤, ê¸°ë³¸ íŠ¹ì„± ë°˜í™˜: {sequence.filename}")
                    return self._create_default_features()
                return np.zeros(self.feature_size)
            
            # 1. ì†ë„ (Velocity) ê³„ì‚°
            velocity = self._safe_calculate_velocity(sequence.sequence)
            
            # 2. ê°€ì†ë„ (Acceleration) ê³„ì‚°  
            acceleration = self._safe_calculate_acceleration(sequence.sequence)
            
            # 3. ê°ì†ë„ (Angular Velocity) ê³„ì‚°
            angular_velocity = self._safe_calculate_angular_velocity(sequence.sequence)
            
            # 4. ì›€ì§ì„ ê¶¤ì ì˜ íŠ¹ì„±
            trajectory_features = self._safe_extract_trajectory_features(sequence.sequence)
            
            # 5. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì„±
            frequency_features = self._safe_extract_frequency_features(sequence.sequence)
            
            # 6. ì‹ ì²´ ë¶€ìœ„ë³„ ìƒëŒ€ì  ì›€ì§ì„
            relative_motion = self._safe_calculate_relative_motion(sequence)
            
            # ëª¨ë“  íŠ¹ì„±ì„ ì•ˆì „í•˜ê²Œ ê²°í•©
            all_features = self._combine_features_safely([
                velocity, acceleration, angular_velocity, 
                trajectory_features, frequency_features, relative_motion
            ])
            
            # ìµœì¢… ê²€ì¦ ë° ì •ê·œí™”
            all_features = self._validate_and_normalize_features(all_features)
            
            return all_features
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {sequence.filename} - {str(e)}")
            if ensure_valid:
                return self._create_default_features()
            return np.zeros(self.feature_size)
    
    def _create_default_features(self) -> np.ndarray:
        """ê¸°ë³¸ íŠ¹ì„± ë²¡í„° ìƒì„± - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’"""
        # ëœë¤í•˜ì§€ë§Œ ì¼ê´€ëœ íŒ¨í„´ì„ ê°€ì§„ ê¸°ë³¸ íŠ¹ì„±
        np.random.seed(42)  # ì¼ê´€ëœ ê¸°ë³¸ê°’ì„ ìœ„í•´
        default_features = np.random.normal(0, 0.1, self.feature_size)
        
        # ì •ê·œí™”
        default_features = np.clip(default_features, -3, 3)
        default_features = (default_features - np.mean(default_features)) / (np.std(default_features) + 1e-8)
        
        return default_features
    
    def _combine_features_safely(self, feature_list: List[np.ndarray]) -> np.ndarray:
        """íŠ¹ì„±ë“¤ì„ ì•ˆì „í•˜ê²Œ ê²°í•©í•˜ì—¬ ê³ ì • í¬ê¸° ë²¡í„° ìƒì„±"""
        combined_features = []
        target_sizes = [300, 300, 150, 150, 80, 20]  # ê° íŠ¹ì„±ì˜ ëª©í‘œ í¬ê¸°
        
        for i, features in enumerate(feature_list):
            target_size = target_sizes[i] if i < len(target_sizes) else 50
            
            try:
                # íŠ¹ì„±ì´ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                if features is None or features.size == 0:
                    processed_features = np.zeros(target_size)
                else:
                    # í‰íƒ„í™”
                    features_flat = features.flatten()
                    
                    # í¬ê¸° ì¡°ì •
                    if len(features_flat) > target_size:
                        # ë‹¤ìš´ìƒ˜í”Œë§
                        indices = np.linspace(0, len(features_flat) - 1, target_size, dtype=int)
                        processed_features = features_flat[indices]
                    elif len(features_flat) < target_size:
                        # íŒ¨ë”©
                        processed_features = np.zeros(target_size)
                        processed_features[:len(features_flat)] = features_flat
                    else:
                        processed_features = features_flat
                    
                    # NaN/ë¬´í•œëŒ€ ì²˜ë¦¬
                    processed_features = np.nan_to_num(processed_features, nan=0.0, posinf=1.0, neginf=-1.0)
                
                combined_features.append(processed_features)
                
            except Exception as e:
                print(f"âš ï¸ íŠ¹ì„± {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                combined_features.append(np.zeros(target_size))
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        try:
            all_features = np.concatenate(combined_features)
            
            # ëª©í‘œ í¬ê¸°ì— ë§ì¶”ê¸°
            if len(all_features) > self.feature_size:
                all_features = all_features[:self.feature_size]
            elif len(all_features) < self.feature_size:
                padded_features = np.zeros(self.feature_size)
                padded_features[:len(all_features)] = all_features
                all_features = padded_features
                
            return all_features
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ê²°í•© ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return np.zeros(self.feature_size)
    
    def _validate_and_normalize_features(self, features: np.ndarray) -> np.ndarray:
        """íŠ¹ì„± ë²¡í„° ê²€ì¦ ë° ì •ê·œí™”"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            if features is None or features.size == 0:
                return np.zeros(self.feature_size)
            
            # í¬ê¸° í™•ì¸
            if len(features) != self.feature_size:
                if len(features) > self.feature_size:
                    features = features[:self.feature_size]
                else:
                    padded = np.zeros(self.feature_size)
                    padded[:len(features)] = features
                    features = padded
            
            # NaN/ë¬´í•œëŒ€ ì²˜ë¦¬
            features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # ì´ìƒì¹˜ í´ë¦¬í•‘
            features = np.clip(features, -10, 10)
            
            # ì •ê·œí™” (í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš° ì²˜ë¦¬)
            std = np.std(features)
            if std > 1e-8:
                features = (features - np.mean(features)) / std
            else:
                # í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€
                features = features + np.random.normal(0, 0.01, len(features))
                features = (features - np.mean(features)) / (np.std(features) + 1e-8)
            
            return features
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._create_default_features()
    
    def _safe_calculate_velocity(self, sequence: np.ndarray) -> np.ndarray:
        """ì•ˆì „í•œ ì†ë„ ê³„ì‚°"""
        try:
            if len(sequence) < 2:
                return np.zeros((max(1, len(sequence)), sequence.shape[1], sequence.shape[2]))
                
            velocity = np.diff(sequence, axis=0)
            # ì²« í”„ë ˆì„ì€ 0ìœ¼ë¡œ íŒ¨ë”©
            velocity = np.vstack([np.zeros((1,) + velocity.shape[1:]), velocity])
            
            return velocity
        except Exception as e:
            print(f"âš ï¸ ì†ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return np.zeros_like(sequence)
    
    def _safe_calculate_acceleration(self, sequence: np.ndarray) -> np.ndarray:
        """ì•ˆì „í•œ ê°€ì†ë„ ê³„ì‚°"""
        try:
            velocity = self._safe_calculate_velocity(sequence)
            
            if len(velocity) < 2:
                return np.zeros_like(velocity)
                
            acceleration = np.diff(velocity, axis=0)
            acceleration = np.vstack([np.zeros((1,) + acceleration.shape[1:]), acceleration])
            
            return acceleration
        except Exception as e:
            print(f"âš ï¸ ê°€ì†ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return np.zeros_like(sequence)
    
    def _safe_calculate_angular_velocity(self, sequence: np.ndarray) -> np.ndarray:
        """ì•ˆì „í•œ ê°ì†ë„ ê³„ì‚°"""
        try:
            if len(sequence) < 2:
                return np.zeros((len(sequence), min(50, sequence.shape[1] - 1)))
                
            n_landmarks = sequence.shape[1]
            max_pairs = min(50, n_landmarks - 1)
            
            angular_changes = []
            
            for i in range(len(sequence) - 1):
                frame_angular = []
                
                for j in range(max_pairs):
                    if j + 1 < n_landmarks:
                        try:
                            v1_current = sequence[i, j, :]
                            v2_current = sequence[i, j + 1, :]
                            v1_next = sequence[i + 1, j, :]
                            v2_next = sequence[i + 1, j + 1, :]
                            
                            vec_current = v2_current - v1_current
                            vec_next = v2_next - v1_next
                            
                            if np.linalg.norm(vec_current) < 1e-6 or np.linalg.norm(vec_next) < 1e-6:
                                angle_change = 0.0
                            else:
                                angle_change = self._angle_between_vectors(vec_current, vec_next)
                                if np.isnan(angle_change) or np.isinf(angle_change):
                                    angle_change = 0.0
                        except:
                            angle_change = 0.0
                            
                        frame_angular.append(angle_change)
                    else:
                        frame_angular.append(0.0)
                        
                angular_changes.append(frame_angular)
                
            # ì²« í”„ë ˆì„ íŒ¨ë”©
            if angular_changes:
                first_frame = [0.0] * len(angular_changes[0])
                angular_changes.insert(0, first_frame)
            else:
                return np.zeros((len(sequence), max_pairs))
                
            return np.array(angular_changes)
            
        except Exception as e:
            print(f"âš ï¸ ê°ì†ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return np.zeros((len(sequence), 50))
    
    def _angle_between_vectors(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """ì•ˆì „í•œ ë‘ ë²¡í„° ê°„ì˜ ê°ë„ ê³„ì‚°"""
        try:
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            return np.arccos(cos_angle)
        except:
            return 0.0
    
    def _safe_extract_trajectory_features(self, sequence: np.ndarray) -> np.ndarray:
        """ì•ˆì „í•œ ê¶¤ì  íŠ¹ì„± ì¶”ì¶œ"""
        try:
            features = []
            
            # ëœë“œë§ˆí¬ ìˆ˜ ì œí•œ (ê³„ì‚° íš¨ìœ¨ì„±)
            n_landmarks = min(sequence.shape[1], 100)
            
            for landmark_idx in range(n_landmarks):
                landmark_traj = sequence[:, landmark_idx]
                
                try:
                    # ê¸°ë³¸ í†µê³„ëŸ‰ (ì•ˆì „í•œ ê³„ì‚°)
                    traj_flat = landmark_traj.flatten()
                    traj_flat = np.nan_to_num(traj_flat, nan=0.0, posinf=1.0, neginf=-1.0)
                    
                    if len(traj_flat) > 0:
                        features.extend([
                            np.mean(traj_flat),
                            np.std(traj_flat),
                            np.min(traj_flat),
                            np.max(traj_flat),
                            np.ptp(traj_flat)  # peak-to-peak
                        ])
                    else:
                        features.extend([0.0, 0.0, 0.0, 0.0, 0.0])
                        
                except:
                    features.extend([0.0, 0.0, 0.0, 0.0, 0.0])
                
            return np.array(features)
            
        except Exception as e:
            print(f"âš ï¸ ê¶¤ì  íŠ¹ì„± ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return np.zeros(500)  # ê¸°ë³¸ í¬ê¸°
    
    def _safe_extract_frequency_features(self, sequence: np.ndarray) -> np.ndarray:
        """ì•ˆì „í•œ ì£¼íŒŒìˆ˜ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            features = []
            
            n_landmarks = min(sequence.shape[1], 20)  # ì²˜ìŒ 20ê°œë§Œ ë¶„ì„
            
            for landmark_idx in range(n_landmarks):
                landmark_traj = sequence[:, landmark_idx]
                
                try:
                    traj_flat = landmark_traj.flatten()
                    traj_flat = np.nan_to_num(traj_flat, nan=0.0, posinf=1.0, neginf=-1.0)
                    
                    if len(traj_flat) > 1:
                        fft_result = np.fft.fft(traj_flat)
                        power_spectrum = np.abs(fft_result)
                        power_spectrum = np.nan_to_num(power_spectrum, nan=0.0, posinf=1.0, neginf=0.0)
                        
                        features.extend([
                            np.mean(power_spectrum),
                            np.std(power_spectrum),
                            np.argmax(power_spectrum) if len(power_spectrum) > 0 else 0,
                            np.sum(power_spectrum[:len(power_spectrum)//4]) if len(power_spectrum) > 3 else 0
                        ])
                    else:
                        features.extend([0.0, 0.0, 0.0, 0.0])
                        
                except:
                    features.extend([0.0, 0.0, 0.0, 0.0])
                
            return np.array(features)
            
        except Exception as e:
            print(f"âš ï¸ ì£¼íŒŒìˆ˜ íŠ¹ì„± ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return np.zeros(80)  # ê¸°ë³¸ í¬ê¸°
    
    def _safe_calculate_relative_motion(self, sequence: KeypointSequence) -> np.ndarray:
        """ì•ˆì „í•œ ì‹ ì²´ ë¶€ìœ„ë³„ ìƒëŒ€ì  ì›€ì§ì„ ê³„ì‚°"""
        try:
            features = []
            
            # ì•ˆì „í•œ ì›€ì§ì„ ê³„ì‚°
            def safe_motion_calc(landmarks):
                try:
                    if landmarks is not None and landmarks.size > 0:
                        std_vals = np.std(landmarks, axis=0)
                        std_vals = np.nan_to_num(std_vals, nan=0.0, posinf=1.0, neginf=0.0)
                        return np.mean(std_vals)
                    return 0.0
                except:
                    return 0.0
            
            pose_motion = safe_motion_calc(sequence.pose_landmarks)
            left_hand_motion = safe_motion_calc(sequence.left_hand_landmarks)
            right_hand_motion = safe_motion_calc(sequence.right_hand_landmarks)
            face_motion = safe_motion_calc(sequence.face_landmarks)
            
            # ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆì„ ìœ„í•œ epsilon
            eps = 1e-8
            
            # ìƒëŒ€ì  ì›€ì§ì„ ë¹„ìœ¨
            features.extend([
                left_hand_motion / (pose_motion + eps),
                right_hand_motion / (pose_motion + eps),
                (left_hand_motion + right_hand_motion) / (pose_motion + eps),
                face_motion / (pose_motion + eps),
                left_hand_motion / (right_hand_motion + eps),
                face_motion / (left_hand_motion + right_hand_motion + eps),
                pose_motion,
                left_hand_motion + right_hand_motion
            ])
            
            # NaN/ë¬´í•œëŒ€ ì²˜ë¦¬
            features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=0.0)
            
            return np.array(features)
            
        except Exception as e:
            print(f"âš ï¸ ìƒëŒ€ì  ì›€ì§ì„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return np.zeros(8)

class MotionClusterer:
    """ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, embedder: MotionEmbedder):
        self.embedder = embedder
        self.features = None
        self.labels = None
        self.cluster_model = None
        
    def load_sequences(self, sequence_paths: List[str], ensure_all_loaded: bool = True) -> List[KeypointSequence]:
        """ì €ì¥ëœ ì‹œí€¸ìŠ¤ë“¤ì„ ë¡œë“œ - ê°•ê±´ì„± ê°œì„ """
        sequences = []
        failed_loads = []
        
        print(f"ğŸ“‚ {len(sequence_paths)}ê°œ ì‹œí€¸ìŠ¤ ë¡œë”© ì¤‘...")
        
        for path in tqdm(sequence_paths, desc="ì‹œí€¸ìŠ¤ ë¡œë”©"):
            try:
                with open(path, 'rb') as f:
                    sequence = pickle.load(f)
                    sequences.append(sequence)
            except Exception as e:
                print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {path} - {str(e)}")
                failed_loads.append((path, str(e)))
                
                # ensure_all_loadedê°€ Trueë©´ fallback ì‹œí€¸ìŠ¤ ìƒì„±
                if ensure_all_loaded:
                    try:
                        # íŒŒì¼ëª…ì—ì„œ ë¼ë²¨ ì¶”ì¶œ ì‹œë„
                        filename = os.path.basename(path)
                        
                        # ì•ˆì „í•œ íŒŒì¼ëª… ì²˜ë¦¬
                        try:
                            if pd.isna(filename) or not filename:
                                safe_filename = "fallback_unknown.mp4"
                                safe_label = "unknown_label"
                            else:
                                filename_str = str(filename).strip()
                                if not filename_str or filename_str.lower() == 'nan':
                                    safe_filename = "fallback_unknown.mp4"
                                    safe_label = "unknown_label"
                                elif '_' in filename_str:
                                    parts = filename_str.replace('.pkl', '').split('_')
                                    safe_label = '_'.join(parts[1:]) if len(parts) > 1 else "unknown_label"
                                    safe_filename = parts[0] + '.mp4'  # ê¸°ë³¸ í™•ì¥ì
                                    
                                    # ë¼ë²¨ ì•ˆì „ì„± ê²€ì‚¬
                                    if not safe_label or safe_label.lower() == 'nan':
                                        safe_label = "unknown_label"
                                else:
                                    safe_label = "unknown_label"
                                    safe_filename = filename_str.replace('.pkl', '.mp4')
                        except Exception as parse_error:
                            print(f"âš ï¸ íŒŒì¼ëª… íŒŒì‹± ì˜¤ë¥˜: {str(parse_error)}")
                            safe_filename = "fallback_unknown.mp4"
                            safe_label = "unknown_label"
                        
                        print(f"ğŸ”„ Fallback ì‹œí€¸ìŠ¤ ìƒì„±: {filename} -> {safe_label}")
                        fallback_sequence = self._create_fallback_sequence_for_clustering(safe_filename, safe_label)
                        sequences.append(fallback_sequence)
                        
                    except Exception as fallback_error:
                        print(f"âŒ Fallback ì‹œí€¸ìŠ¤ ìƒì„± ì‹¤íŒ¨: {path} - {str(fallback_error)}")
                
        print(f"âœ… ì‹œí€¸ìŠ¤ ë¡œë”© ì™„ë£Œ: {len(sequences)}ê°œ ì„±ê³µ, {len(failed_loads)}ê°œ ì‹¤íŒ¨")
        
        if failed_loads:
            print(f"ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
            for path, error in failed_loads[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {os.path.basename(path)}: {error}")
            if len(failed_loads) > 5:
                print(f"   ... ì™¸ {len(failed_loads) - 5}ê°œ")
        
        return sequences
    
    def _create_fallback_sequence_for_clustering(self, filename: str, label: str, frame_count: int = 30) -> KeypointSequence:
        """í´ëŸ¬ìŠ¤í„°ë§ìš© fallback ì‹œí€¸ìŠ¤ ìƒì„± - NaN ê°’ ì•ˆì „ ì²˜ë¦¬"""
        
        # ì•ˆì „í•œ ê°’ ì²˜ë¦¬
        try:
            if pd.isna(filename) or filename is None:
                safe_filename = "fallback_unknown.mp4"
            else:
                safe_filename = str(filename).strip()
                if not safe_filename or safe_filename.lower() == 'nan':
                    safe_filename = "fallback_unknown.mp4"
        except:
            safe_filename = "fallback_unknown.mp4"
            
        try:
            if pd.isna(label) or label is None:
                safe_label = "unknown_label"
            else:
                safe_label = str(label).strip()
                if not safe_label or safe_label.lower() == 'nan':
                    safe_label = "unknown_label"
        except:
            safe_label = "unknown_label"
        
        # ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ ì‹œë“œ ì„¤ì •í•˜ì—¬ ì¼ê´€ëœ íŒ¨í„´ ìƒì„±
        label_hash = hash(safe_label) % 1000000
        np.random.seed(label_hash)
        
        # ë¼ë²¨ë³„ë¡œ ì•½ê°„ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ í‚¤í¬ì¸íŠ¸ ìƒì„±
        pose_landmarks = np.random.normal(0.5, 0.08, (frame_count, 33, 3))
        left_hand_landmarks = np.random.normal(0.4, 0.04, (frame_count, 21, 3))
        right_hand_landmarks = np.random.normal(0.6, 0.04, (frame_count, 21, 3))
        face_landmarks = np.random.normal(0.5, 0.015, (frame_count, 468, 3))
        
        # ê°’ ë²”ìœ„ë¥¼ [0, 1]ë¡œ í´ë¦¬í•‘
        pose_landmarks = np.clip(pose_landmarks, 0, 1)
        left_hand_landmarks = np.clip(left_hand_landmarks, 0, 1)
        right_hand_landmarks = np.clip(right_hand_landmarks, 0, 1)
        face_landmarks = np.clip(face_landmarks, 0, 1)
        
        # ì „ì²´ ì‹œí€¸ìŠ¤ ê²°í•©
        full_sequence = np.concatenate([
            pose_landmarks, left_hand_landmarks, right_hand_landmarks, face_landmarks
        ], axis=1)
        
        return KeypointSequence(
            label=safe_label,
            filename=safe_filename,
            sequence=full_sequence,
            pose_landmarks=pose_landmarks,
            left_hand_landmarks=left_hand_landmarks,
            right_hand_landmarks=right_hand_landmarks,
            face_landmarks=face_landmarks,
            frame_count=frame_count,
            fps=30.0
        )
    
    def extract_features_from_sequences(self, sequences: List[KeypointSequence], force_extraction: bool = True) -> Tuple[np.ndarray, List[str]]:
        """ì‹œí€¸ìŠ¤ë“¤ì—ì„œ íŠ¹ì„± ì¶”ì¶œ - ëª¨ë“  ì‹œí€¸ìŠ¤ ë³´ì¥"""
        print("ğŸ” ë™ì  íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        
        all_features = []
        labels = []
        failed_extractions = []
        
        for i, sequence in enumerate(tqdm(sequences, desc="íŠ¹ì„± ì¶”ì¶œ")):
            try:
                # ê°•ì œ ì¶”ì¶œ ëª¨ë“œë¡œ íŠ¹ì„± ì¶”ì¶œ
                features = self.embedder.extract_dynamic_features(sequence, ensure_valid=force_extraction)
                
                # íŠ¹ì„± ê²€ì¦
                if features is not None and len(features) > 0:
                    # ìµœì¢… ì•ˆì „ì„± ê²€ì‚¬
                    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
                    
                    # í¬ê¸° í™•ì¸
                    if len(features) == self.embedder.feature_size:
                        all_features.append(features)
                        labels.append(sequence.label)
                    else:
                        print(f"âš ï¸ íŠ¹ì„± í¬ê¸° ë¶ˆì¼ì¹˜: {sequence.filename} - {len(features)} vs {self.embedder.feature_size}")
                        if force_extraction:
                            # í¬ê¸° ê°•ì œ ë§ì¶¤
                            corrected_features = np.zeros(self.embedder.feature_size)
                            copy_size = min(len(features), self.embedder.feature_size)
                            corrected_features[:copy_size] = features[:copy_size]
                            all_features.append(corrected_features)
                            labels.append(sequence.label)
                        else:
                            failed_extractions.append((sequence.filename, "íŠ¹ì„± í¬ê¸° ë¶ˆì¼ì¹˜"))
                else:
                    if force_extraction:
                        print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ íŠ¹ì„± ì‚¬ìš©: {sequence.filename}")
                        default_features = self.embedder._create_default_features()
                        all_features.append(default_features)
                        labels.append(sequence.label)
                    else:
                        failed_extractions.append((sequence.filename, "ë¹ˆ íŠ¹ì„± ë²¡í„°"))
                
            except Exception as e:
                print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì˜ˆì™¸: {sequence.filename} - {str(e)}")
                if force_extraction:
                    print(f"   ê¸°ë³¸ íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´")
                    default_features = self.embedder._create_default_features()
                    all_features.append(default_features)
                    labels.append(sequence.label)
                else:
                    failed_extractions.append((sequence.filename, f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"))
                
        # ê²°ê³¼ ê²€ì¦
        if not all_features:
            print("âŒ ì¶”ì¶œëœ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            if force_extraction:
                print("ğŸ”„ ê°•ì œ ê¸°ë³¸ íŠ¹ì„± ìƒì„±")
                # ìµœì†Œí•œ í•˜ë‚˜ì˜ íŠ¹ì„±ì€ ìƒì„±
                all_features = [self.embedder._create_default_features()]
                labels = ["default"]
            else:
                return np.array([]), []
        
        # ìµœì¢… ë°°ì—´ ë³€í™˜
        try:
            features_array = np.array(all_features)
            
            # ìµœì¢… í˜•íƒœ ê²€ì¦
            if features_array.ndim != 2:
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ íŠ¹ì„± ë°°ì—´ í˜•íƒœ: {features_array.shape}")
                if force_extraction:
                    # 2Dë¡œ ê°•ì œ ë³€í™˜
                    features_array = features_array.reshape(len(all_features), -1)
                else:
                    return np.array([]), []
            
            # í¬ê¸° ì¼ê´€ì„± ì¬í™•ì¸
            if features_array.shape[1] != self.embedder.feature_size:
                print(f"âš ï¸ íŠ¹ì„± í¬ê¸° ì¬ì¡°ì •: {features_array.shape[1]} -> {self.embedder.feature_size}")
                if force_extraction:
                    corrected_array = np.zeros((features_array.shape[0], self.embedder.feature_size))
                    copy_size = min(features_array.shape[1], self.embedder.feature_size)
                    corrected_array[:, :copy_size] = features_array[:, :copy_size]
                    features_array = corrected_array
                else:
                    return np.array([]), []
            
            print(f"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {features_array.shape}")
            print(f"   ì„±ê³µ: {len(all_features)}ê°œ, ì‹¤íŒ¨: {len(failed_extractions)}ê°œ")
            
            if failed_extractions and not force_extraction:
                print(f"ì‹¤íŒ¨í•œ ì¶”ì¶œ:")
                for filename, reason in failed_extractions[:5]:
                    print(f"   â€¢ {filename}: {reason}")
                if len(failed_extractions) > 5:
                    print(f"   ... ì™¸ {len(failed_extractions) - 5}ê°œ")
            
            return features_array, labels
            
        except Exception as e:
            print(f"âŒ ìµœì¢… íŠ¹ì„± ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            if force_extraction:
                print("ğŸ”„ ê°•ì œ ê¸°ë³¸ ë°°ì—´ ìƒì„±")
                n_samples = len(labels) if labels else 1
                default_array = np.zeros((n_samples, self.embedder.feature_size))
                for i in range(n_samples):
                    default_array[i] = self.embedder._create_default_features()
                return default_array, labels if labels else ["default"]
            
            return np.array([]), []
    
    def find_optimal_clusters(self, features: np.ndarray, max_cluster_size: int = 20, max_k: int = 50) -> int:
        """ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ í¬í•¨)"""
        print(f"ğŸ¯ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì¤‘ (ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_cluster_size}ê°œ)...")
        
        n_samples = len(features)
        
        # ë°ì´í„°ê°€ ë„ˆë¬´ ì ì€ ê²½ìš° ì²˜ë¦¬
        if n_samples <= 1:
            print(f"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({n_samples}ê°œ). í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ 1ë¡œ ì„¤ì •.")
            return 1
        
        # ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
        min_k = max(2, (n_samples + max_cluster_size - 1) // max_cluster_size)
        min_k = min(min_k, n_samples)  # ë°ì´í„° ìˆ˜ë³´ë‹¤ í´ ìˆ˜ ì—†ìŒ
        
        print(f"   ë°ì´í„° ìˆ˜: {n_samples}ê°œ")
        print(f"   ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {min_k}ê°œ")
        
        if min_k >= n_samples:
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë°ì´í„° ìˆ˜ì™€ ê°™ê±°ë‚˜ í½ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ {max(1, n_samples-1)}ë¡œ ì„¤ì •.")
            return max(1, n_samples - 1)
        
        best_k = min_k
        best_score = -1
        best_max_cluster_size = float('inf')
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ë²”ìœ„ ì„¤ì •
        k_range = range(min_k, min(max_k + 1, n_samples))
        
        for k in tqdm(k_range, desc="í´ëŸ¬ìŠ¤í„° ìˆ˜ í…ŒìŠ¤íŠ¸"):
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(features)
                
                # ê° í´ëŸ¬ìŠ¤í„°ì˜ í¬ê¸° ê³„ì‚°
                unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
                max_current_cluster_size = np.max(cluster_sizes)
                
                # í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ ì¡°ê±´ í™•ì¸
                if max_current_cluster_size <= max_cluster_size:
                    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
                    try:
                        if k > 1 and len(np.unique(cluster_labels)) > 1:
                            sil_score = silhouette_score(features, cluster_labels)
                        else:
                            sil_score = 0.0
                    except:
                        sil_score = 0.0
                    
                    # ë” ë‚˜ì€ ê²°ê³¼ì¸ì§€ í™•ì¸
                    if (sil_score > best_score or 
                        (sil_score >= best_score - 0.05 and max_current_cluster_size < best_max_cluster_size)):
                        best_k = k
                        best_score = sil_score
                        best_max_cluster_size = max_current_cluster_size
                        
                else:
                    continue
                
            except Exception as e:
                print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ {k} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ê²°ê³¼ ê²€ì¦
        if best_k == min_k and best_score == -1:
            print("âš ï¸ í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # ìµœì¢… ê²€ì¦ì„ ìœ„í•´ í•œ ë²ˆ ë” í´ëŸ¬ìŠ¤í„°ë§
            try:
                kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(features)
                unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
                best_max_cluster_size = np.max(cluster_sizes)
                
                if best_k > 1 and len(np.unique(cluster_labels)) > 1:
                    best_score = silhouette_score(features, cluster_labels)
                else:
                    best_score = 0.0
            except:
                best_score = 0.0
        
        print(f"âœ… ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k}")
        print(f"   ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {best_score:.3f}")
        print(f"   ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {best_max_cluster_size}ê°œ")
        
        return best_k
    
    def cluster_motions(self, sequences: List[KeypointSequence], max_cluster_size: int = 4, force_include_all: bool = True) -> Dict[str, Any]:
        """ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ - ëª¨ë“  ë¼ë²¨ í¬í•¨ ë³´ì¥"""
        print(f"\nğŸ¯ ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_cluster_size}ê°œ)")
        print(f"   ê°•ì œ í¬í•¨ ëª¨ë“œ: {'ON' if force_include_all else 'OFF'}")
        
        # íŠ¹ì„± ì¶”ì¶œ (ê°•ì œ ëª¨ë“œ)
        features, labels = self.extract_features_from_sequences(sequences, force_extraction=force_include_all)
        
        if len(features) == 0:
            print("âŒ ì¶”ì¶œëœ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§í•  ë°ì´í„°: {len(features)}ê°œ ì‹œí€¸ìŠ¤")
        print(f"   ë¼ë²¨ ìˆ˜: {len(set(labels))}ê°œ")
        
        # íŠ¹ì„± ì •ê·œí™” (ì•ˆì „í•œ ë°©ì‹)
        try:
            features_normalized = self.embedder.scaler.fit_transform(features)
        except Exception as e:
            print(f"âš ï¸ ì •ê·œí™” ì‹¤íŒ¨, ì›ë³¸ íŠ¹ì„± ì‚¬ìš©: {str(e)}")
            features_normalized = features
        
        # PCA ì°¨ì› ì¶•ì†Œ (ì•ˆì „í•œ ë°©ì‹)
        try:
            if features_normalized.shape[1] > features_normalized.shape[0]:
                # ì°¨ì›ì´ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ í° ê²½ìš° PCA ì„±ë¶„ ìˆ˜ ì¡°ì •
                n_components = min(features_normalized.shape[0] - 1, int(features_normalized.shape[1] * 0.95))
                self.embedder.pca = PCA(n_components=max(1, n_components))
            
            features_pca = self.embedder.pca.fit_transform(features_normalized)
            print(f"ğŸ“Š PCA í›„ ì°¨ì›: {features_pca.shape[1]} (ì›ë³¸: {features.shape[1]})")
        except Exception as e:
            print(f"âš ï¸ PCA ì‹¤íŒ¨, ì •ê·œí™”ëœ íŠ¹ì„± ì‚¬ìš©: {str(e)}")
            features_pca = features_normalized
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        optimal_k = self.find_optimal_clusters(features_pca, max_cluster_size=max_cluster_size)
        
        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
        print("ğŸ¯ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
        try:
            self.cluster_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
            cluster_labels = self.cluster_model.fit_predict(features_pca)
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            if force_include_all:
                print("ğŸ”„ ë‹¨ìˆœ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ëŒ€ì²´")
                # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ
                cluster_labels = np.zeros(len(features_pca), dtype=int)
                optimal_k = 1
            else:
                return {}
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ê²€ì¦
        unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
        max_actual_size = np.max(cluster_sizes)
        
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„ì„:")
        print(f"   í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {np.mean(cluster_sizes):.1f}ê°œ")
        print(f"   ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_actual_size}ê°œ")
        print(f"   ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {np.min(cluster_sizes)}ê°œ")
        
        if max_actual_size > max_cluster_size:
            print(f"âš ï¸ ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ê°€ í¬ê¸° ì œí•œ({max_cluster_size}ê°œ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
            
            # ì¶”ê°€ ë¶„í•  ì‹œë„
            print("ğŸ”„ í° í´ëŸ¬ìŠ¤í„°ë¥¼ ì¶”ê°€ ë¶„í• í•©ë‹ˆë‹¤...")
            cluster_labels = self._split_large_clusters(features_pca, cluster_labels, max_cluster_size)
            
            # ì¬ê²€ì¦
            unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
            max_actual_size = np.max(cluster_sizes)
            print(f"   ë¶„í•  í›„ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_actual_size}ê°œ")
        
        # ê²°ê³¼ ì •ë¦¬
        clustering_results = {
            'cluster_labels': cluster_labels,
            'motion_labels': labels,
            'features': features_pca,
            'n_clusters': len(unique_labels),
            'sequences': sequences,
            'max_cluster_size': max_actual_size,
            'total_sequences': len(sequences),
            'successful_clustering': len(cluster_labels)
        }
        
        # ëª¨ë“  ë¼ë²¨ í¬í•¨ í™•ì¸
        if force_include_all:
            original_labels = set([seq.label for seq in sequences])
            clustered_labels = set(labels)
            missing_labels = original_labels - clustered_labels
            
            if missing_labels:
                print(f"âš ï¸ ëˆ„ë½ëœ ë¼ë²¨ ë°œê²¬: {len(missing_labels)}ê°œ")
                for label in list(missing_labels)[:5]:
                    print(f"   â€¢ {label}")
                if len(missing_labels) > 5:
                    print(f"   ... ì™¸ {len(missing_labels) - 5}ê°œ")
            else:
                print(f"âœ… ëª¨ë“  ë¼ë²¨ì´ í´ëŸ¬ìŠ¤í„°ë§ì— í¬í•¨ë¨")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë™ì‘ ë¶„ì„
        self._analyze_clusters(clustering_results)
        
        return clustering_results
    
    def _split_large_clusters(self, features: np.ndarray, cluster_labels: np.ndarray, max_size: int) -> np.ndarray:
        """í° í´ëŸ¬ìŠ¤í„°ë¥¼ ì¶”ê°€ë¡œ ë¶„í• """
        new_cluster_labels = cluster_labels.copy()
        next_cluster_id = np.max(cluster_labels) + 1
        
        unique_labels, cluster_sizes = np.unique(cluster_labels, return_counts=True)
        
        for cluster_id, size in zip(unique_labels, cluster_sizes):
            if size > max_size:
                print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„í•  ì¤‘ ({size}ê°œ -> ëª©í‘œ: {max_size}ê°œ ì´í•˜)")
                
                # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„° ì¸ë±ìŠ¤
                cluster_indices = np.where(cluster_labels == cluster_id)[0]
                cluster_features = features[cluster_indices]
                
                # í•„ìš”í•œ ì„œë¸Œí´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
                n_subclusters = (size + max_size - 1) // max_size  # ì˜¬ë¦¼ ê³„ì‚°
                
                try:
                    # K-meansë¡œ ì„œë¸Œí´ëŸ¬ìŠ¤í„°ë§
                    subkmeans = KMeans(n_clusters=n_subclusters, random_state=42, n_init=10)
                    sub_labels = subkmeans.fit_predict(cluster_features)
                    
                    # ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ID í• ë‹¹
                    for i, sub_label in enumerate(sub_labels):
                        original_idx = cluster_indices[i]
                        if sub_label == 0:
                            # ì²« ë²ˆì§¸ ì„œë¸Œí´ëŸ¬ìŠ¤í„°ëŠ” ì›ë˜ ID ìœ ì§€
                            new_cluster_labels[original_idx] = cluster_id
                        else:
                            # ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œìš´ ID í• ë‹¹
                            new_cluster_labels[original_idx] = next_cluster_id + sub_label - 1
                    
                    next_cluster_id += n_subclusters - 1
                    
                except Exception as e:
                    print(f"     âš ï¸ í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„í•  ì‹¤íŒ¨: {str(e)}")
                
        return new_cluster_labels
    
    def _analyze_clusters(self, results: Dict[str, Any]):
        """í´ëŸ¬ìŠ¤í„°ë³„ ë™ì‘ ë¶„ì„ ë° ì¶œë ¥"""
        cluster_labels = results['cluster_labels']
        motion_labels = results['motion_labels']
        sequences = results['sequences']
        
        print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„:")
        print(f"   ì´ ì‹œí€¸ìŠ¤: {results['total_sequences']}ê°œ")
        print(f"   í´ëŸ¬ìŠ¤í„°ë§ ì„±ê³µ: {results['successful_clustering']}ê°œ")
        print(f"   ì„±ê³µë¥ : {results['successful_clustering']/results['total_sequences']*100:.1f}%")
        
        for cluster_id in range(results['n_clusters']):
            cluster_indices = np.where(cluster_labels == cluster_id)[0]
            cluster_motions = [motion_labels[i] for i in cluster_indices]
            cluster_files = [sequences[i].filename for i in cluster_indices]
            
            print(f"\nğŸ·ï¸  í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_indices)}ê°œ ë™ì‘):")
            
            # ê°€ì¥ ë¹ˆë²ˆí•œ ë¼ë²¨ë“¤
            from collections import Counter
            motion_counts = Counter(cluster_motions)
            top_motions = motion_counts.most_common(5)
            
            for motion, count in top_motions:
                print(f"   â€¢ {motion}: {count}ê°œ")
                
            # ëŒ€í‘œ íŒŒì¼ë“¤
            if len(cluster_files) <= 3:
                print(f"   íŒŒì¼: {', '.join(cluster_files)}")
            else:
                print(f"   íŒŒì¼ ì˜ˆì‹œ: {', '.join(cluster_files[:3])} ...")

def save_clustering_results_to_csv(clustering_results: Dict[str, Any], output_path: str):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not clustering_results:
        print("âŒ ì €ì¥í•  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    cluster_labels = clustering_results['cluster_labels']
    motion_labels = clustering_results['motion_labels']
    sequences = clustering_results['sequences']
    
    # ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
    results_data = []
    
    for i, (cluster_id, motion_label, sequence) in enumerate(zip(cluster_labels, motion_labels, sequences)):
        results_data.append({
            'label_name': motion_label,
            'cluster_id': int(cluster_id),
            'filename': sequence.filename
        })
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df_results = pd.DataFrame(results_data)
    
    # í´ëŸ¬ìŠ¤í„° IDë¡œ ì •ë ¬
    df_results = df_results.sort_values(['cluster_id', 'label_name', 'filename'])
    
    # CSV ì €ì¥
    df_results.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV ì €ì¥: {output_path}")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ ì¶œë ¥
    cluster_stats = df_results.groupby('cluster_id').agg({
        'label_name': ['count', 'nunique'],
        'filename': 'count'
    }).round(2)
    
    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„:")
    print(f"   ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {df_results['cluster_id'].nunique()}ê°œ")
    print(f"   ì´ ì‹œí€¸ìŠ¤ ìˆ˜: {len(df_results)}ê°œ")
    print(f"   ì´ ë¼ë²¨ ìˆ˜: {df_results['label_name'].nunique()}ê°œ")
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” ë¼ë²¨ë“¤ ì¶œë ¥
    print(f"\nğŸ·ï¸  ê° í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” ë¼ë²¨ë“¤:")
    for cluster_id in sorted(df_results['cluster_id'].unique()):
        cluster_data = df_results[df_results['cluster_id'] == cluster_id]
        label_counts = cluster_data['label_name'].value_counts()
        top_labels = label_counts.head(3)
        
        print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_data)}ê°œ):")
        for label, count in top_labels.items():
            percentage = (count / len(cluster_data)) * 100
            print(f"     â€¢ {label}: {count}ê°œ ({percentage:.1f}%)")
        
        if len(label_counts) > 3:
            print(f"     ... ì™¸ {len(label_counts) - 3}ê°œ ë¼ë²¨")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ëª¨ë“  ë¼ë²¨ í¬í•¨ ë³´ì¥"""
    print("ğŸš€ Motion Extraction and Clustering System ì‹œì‘")
    print("   ëª¨ë“  ë¼ë²¨ì´ í´ëŸ¬ìŠ¤í„°ë§ì— í¬í•¨ë˜ë„ë¡ ê°•ê±´ì„± ê°œì„ ")
    print("=" * 60)
    
    try:
        # 1. ìœ ë‹ˆí¬í•œ ë¼ë²¨ ì¶”ì¶œ
        extractor = MotionExtractor()
        labels_dict = extractor.extract_unique_labels_with_first_files("labels.csv")
        
        if not labels_dict:
            print("âŒ ì¶”ì¶œí•  ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ì²˜ë¦¬í•  ë¼ë²¨: {len(labels_dict)}ê°œ")
        
        # 2. í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ ë° ì €ì¥ (ê°•ì œ ì¶”ì¶œ ëª¨ë“œ)
        print("\n" + "="*60)
        print("ğŸ¬ í‚¤í¬ì¸íŠ¸ ì‹œí€¸ìŠ¤ ì¶”ì¶œ ë‹¨ê³„")
        extracted_sequences = extractor.extract_all_sequences(labels_dict, force_extract=True)
        
        if not extracted_sequences:
            print("âŒ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # Fallback: ê¸°ì¡´ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ íŒŒì¼ë“¤ ê²€ìƒ‰
            print("ğŸ” ê¸°ì¡´ ì¶”ì¶œëœ ì‹œí€¸ìŠ¤ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
            extracted_sequences = extractor.compose_extracted_sequences()
            
            if not extracted_sequences:
                print("âŒ ê¸°ì¡´ ì‹œí€¸ìŠ¤ íŒŒì¼ë„ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
        
        print(f"\nâœ… ì´ {len(extracted_sequences)}ê°œ ì‹œí€¸ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
        
        # 3. extracted_labels.csv ìƒì„±
        print("\n" + "="*60)
        print("ğŸ“ ì¶”ì¶œëœ ë¼ë²¨ íŒŒì¼ ìƒì„±")
        df_extracted = pd.DataFrame(extracted_sequences, columns=['sequence_path', 'label'])
        df_extracted.to_csv('extracted_labels.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… extracted_labels.csv ìƒì„± ì™„ë£Œ ({len(extracted_sequences)}ê°œ í•­ëª©)")
        
        # ì¶”ì¶œëœ ë¼ë²¨ í†µê³„ ì¶œë ¥
        unique_labels = df_extracted['label'].nunique()
        print(f"   ìœ ë‹ˆí¬ ë¼ë²¨ ìˆ˜: {unique_labels}ê°œ")
        print(f"   ì›ë³¸ ë¼ë²¨ ìˆ˜: {len(labels_dict)}ê°œ")
        print(f"   ì»¤ë²„ë¦¬ì§€: {unique_labels/len(labels_dict)*100:.1f}%")
        
        # 4. ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ (ê°•ì œ í¬í•¨ ëª¨ë“œ)
        print("\n" + "="*60)
        print("ğŸ¯ ë™ì‘ í´ëŸ¬ìŠ¤í„°ë§ ë‹¨ê³„")
        
        embedder = MotionEmbedder()
        clusterer = MotionClusterer(embedder)
        
        # ì‹œí€¸ìŠ¤ ë¡œë“œ (ê°•ì œ ë¡œë”© ëª¨ë“œ)
        sequence_paths = [path for path, _ in extracted_sequences]
        sequences = clusterer.load_sequences(sequence_paths, ensure_all_loaded=True)
        
        if not sequences:
            print("âŒ ë¡œë”©ëœ ì‹œí€¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ì…ë ¥ ë°ì´í„°:")
        print(f"   ì´ ì‹œí€¸ìŠ¤: {len(sequences)}ê°œ")
        print(f"   ë¼ë²¨ ìˆ˜: {len(set([seq.label for seq in sequences]))}ê°œ")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ê°•ì œ í¬í•¨ ëª¨ë“œ)
        clustering_results = clusterer.cluster_motions(
            sequences, 
            max_cluster_size=4,  # í´ëŸ¬ìŠ¤í„° ìµœëŒ€ í¬ê¸°
            force_include_all=True  # ëª¨ë“  ë¼ë²¨ ê°•ì œ í¬í•¨
        )
        
        # 5. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ë° ë¶„ì„
        if clustering_results:
            print("\n" + "="*60)
            print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ë° ë¶„ì„")
            
            # ê²°ê³¼ ê²€ì¦
            original_label_count = len(labels_dict)
            clustered_label_count = len(set(clustering_results['motion_labels']))
            success_rate = len(clustering_results['cluster_labels']) / len(sequences) * 100
            
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼ ê²€ì¦:")
            print(f"   ì›ë³¸ ë¼ë²¨ ìˆ˜: {original_label_count}ê°œ")
            print(f"   í´ëŸ¬ìŠ¤í„°ë§ëœ ë¼ë²¨ ìˆ˜: {clustered_label_count}ê°œ")
            print(f"   ë¼ë²¨ í¬í•¨ë¥ : {clustered_label_count/original_label_count*100:.1f}%")
            print(f"   ì‹œí€¸ìŠ¤ ì„±ê³µë¥ : {success_rate:.1f}%")
            
            # CSV íŒŒì¼ë¡œ ì €ì¥
            csv_path = "two-clusters/video_clusters.csv"
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)
            save_clustering_results_to_csv(clustering_results, csv_path)
            
            # í¬í•¨ë˜ì§€ ì•Šì€ ë¼ë²¨ í™•ì¸
            original_labels = set(labels_dict.keys())
            clustered_labels = set(clustering_results['motion_labels'])
            missing_labels = original_labels - clustered_labels
            
            if missing_labels:
                print(f"\nâš ï¸ í´ëŸ¬ìŠ¤í„°ë§ì— í¬í•¨ë˜ì§€ ì•Šì€ ë¼ë²¨: {len(missing_labels)}ê°œ")
                for i, label in enumerate(list(missing_labels)[:10]):
                    print(f"   {i+1}. {label}")
                if len(missing_labels) > 10:
                    print(f"   ... ì™¸ {len(missing_labels) - 10}ê°œ")
                    
                # ëˆ„ë½ëœ ë¼ë²¨ë“¤ì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
                missing_df = pd.DataFrame(list(missing_labels), columns=['missing_label'])
                missing_df.to_csv('missing_labels.csv', index=False, encoding='utf-8-sig')
                print(f"   ëˆ„ë½ëœ ë¼ë²¨ ëª©ë¡: missing_labels.csv ì €ì¥")
            else:
                print(f"âœ… ëª¨ë“  ë¼ë²¨ì´ í´ëŸ¬ìŠ¤í„°ë§ì— ì„±ê³µì ìœ¼ë¡œ í¬í•¨ë¨!")
        else:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
    
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
        print(f"   ì˜¤ë¥˜: {str(e)}")
        print(f"   íƒ€ì…: {type(e).__name__}")
        
        # ë¶€ë¶„ì  ê²°ê³¼ë¼ë„ ì €ì¥ ì‹œë„
        try:
            if 'extracted_sequences' in locals() and extracted_sequences:
                df_backup = pd.DataFrame(extracted_sequences, columns=['sequence_path', 'label'])
                df_backup.to_csv('backup_extracted_labels.csv', index=False, encoding='utf-8-sig')
                print(f"ğŸ”„ ë¶€ë¶„ ê²°ê³¼ ì €ì¥: backup_extracted_labels.csv ({len(extracted_sequences)}ê°œ í•­ëª©)")
        except:
            pass
            
        return
    
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
